{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARG_WV_v03.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/ARG_WV_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gT0OTdDEUUp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./drive/MyDrive/*.model /content/"
      ],
      "metadata": {
        "id": "DXiGMFqAEYWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re, os\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import csv\n",
        "from statistics import mean"
      ],
      "metadata": {
        "id": "3MKvG1TrEbFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm /content/*.txt\n",
        "#!mkdir /content/comp_covid/\n",
        "#!mv /content/drive/MyDrive/*.model /content/comp_covid/\n",
        "#!mv /content/comp_covid/*.model /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "n1Gb8PbJEq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "l3GO9fFsG9_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data\n",
        "model_c05 = Word2Vec.load(\"/content/drive/MyDrive/covid_lem_s5_e5.model\")\n",
        "model_c10 = Word2Vec.load(\"/content/drive/MyDrive/covid_lem_s10_e5.model\")\n",
        "model_c20 = Word2Vec.load(\"/content/drive/MyDrive/covid_lem_s20_e5.model\")\n",
        "\n",
        "model_ep05 = Word2Vec.load(\"/content/drive/MyDrive/ep_en_s5_e5.model\")\n",
        "model_ep10 = Word2Vec.load(\"/content/drive/MyDrive/ep_en_s10_e5.model\")\n",
        "model_ep20 = Word2Vec.load(\"/content/drive/MyDrive/ep_en_s20_e5.model\")\n",
        "\n",
        "word_vectors_covid5 = model_c05.wv\n",
        "word_vectors_covid10 = model_c10.wv\n",
        "word_vectors_covid20 = model_c20.wv\n",
        "\n",
        "word_vectors_ep5 = model_ep05.wv\n",
        "word_vectors_ep10 = model_ep10.wv\n",
        "word_vectors_ep20 = model_ep20.wv\n"
      ],
      "metadata": {
        "id": "x2NtdijAG_hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class"
      ],
      "metadata": {
        "id": "BuXi4RXwWeTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api # Allows us to download some free training data\n"
      ],
      "metadata": {
        "id": "odiRZmTJWo6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/53afe57ca6aa4b2cb614/?dl=1\n",
        "!mv index.html?dl=1 ep_en_s5_e5.model\n",
        "model_ep05 = Word2Vec.load(\"ep_en_s5_e5.model\")\n",
        "word_vectors_ep5 = model_ep05.wv"
      ],
      "metadata": {
        "id": "kdFzdb1jVIA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINITION compaire spans (5, 10, 20) COVID\n",
        "def compaire(list_lex, list_name,wordnum):\n",
        "  import sys, re, os\n",
        "  import scipy.stats as ss\n",
        "  import matplotlib.pyplot as plt\n",
        "  import math\n",
        "  import csv\n",
        "  from statistics import mean\n",
        "  for lex in list_lex:\n",
        "    res5 = word_vectors_covid5.most_similar(lex, topn=wordnum)\n",
        "    fte_5 = [a_tuple[0] for a_tuple in res5]\n",
        "    weights_5= [a_tuple[1] for a_tuple in res5]\n",
        "    weight_5 = mean([a_tuple[1] for a_tuple in res5])\n",
        "    res10 = word_vectors_covid10.most_similar(lex, topn=wordnum)\n",
        "    fte_10 = [a_tuple[0] for a_tuple in res10]\n",
        "    weights_10 = [a_tuple[1] for a_tuple in res10]\n",
        "    weight_10 = mean([a_tuple[1] for a_tuple in res10])\n",
        "    res20 = word_vectors_covid20.most_similar(lex, topn=wordnum)\n",
        "    fte_20 = [a_tuple[0] for a_tuple in res20]\n",
        "    weights_20 = [a_tuple[1] for a_tuple in res20]\n",
        "    weight_20 = mean([a_tuple[1] for a_tuple in res20])\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_5:\n",
        "      if word in fte_10: k += 1\n",
        "    common_5_10 = k\n",
        "\n",
        "    common5_10 = \"\"\n",
        "    for resX in res5:\n",
        "      for resY in res10:\n",
        "        if(resX[0] == resY[0]):\n",
        "          cos_comp5_10 = (resX[1]-resY[1])/min(resX[1],resY[1])\n",
        "          common5_10=common5_10+resX[0] + '\\t' + str(cos_comp5_10) +'\\n'\n",
        "    FN = '/content/comp_covid/' + list_name + '_' + lex + '_weight_5_10.txt'\n",
        "    FileOut = open(FN, 'w')\n",
        "    FileOut.write(common5_10)\n",
        "    FileOut.flush()\n",
        "    FileOut.close()\n",
        "\n",
        "\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_5:\n",
        "      if word in fte_20: k += 1\n",
        "    common_5_20 = k\n",
        "        \n",
        "    common5_20 = \"\"\n",
        "    for resX in res5:\n",
        "      for resY in res20:\n",
        "        if(resX[0] == resY[0]):\n",
        "          cos_comp5_20 = (resX[1]-resY[1])/min(resX[1],resY[1])\n",
        "          common5_20= common5_20+ resX[0] + '\\t' + str(cos_comp5_20) +'\\n'\n",
        "    FN = '/content/comp_covid/' +list_name + '_' +lex + '_weight_5_20.txt'\n",
        "    FileOut = open(FN, 'w')\n",
        "    FileOut.write(common5_20)\n",
        "    FileOut.flush()\n",
        "    FileOut.close()\n",
        "\n",
        "    common10_20 = \"\"\n",
        "    for resX in res10:\n",
        "      for resY in res20:\n",
        "        if(resX[0] == resY[0]):\n",
        "          cos_comp10_20 = (resX[1]-resY[1])/min(resX[1],resY[1])\n",
        "          common10_20= common10_20+ resX[0] + '\\t' + str(cos_comp10_20) +'\\n'\n",
        "    FN = '/content/comp_covid/' + list_name + '_'+lex + '_weight_10_20.txt'\n",
        "    FileOut = open(FN, 'w')\n",
        "    FileOut.write(common10_20)\n",
        "    FileOut.flush()\n",
        "    FileOut.close()\n",
        "\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_20:\n",
        "      if word in fte_10: k += 1\n",
        "    common_10_20 = k\n",
        "\n",
        "\n",
        "    FN = '/content/comp_covid/' + list_name + '_' + lex + '5_10_20.txt'\n",
        "    FileOut = open(FN, 'w')\n",
        "    FileOut.write(lex + str(5)+'\\t'+'interect_5_10'+'_'+str(common_5_10) +'\\t'+\n",
        "                  lex + str(10)+'\\t'+'interect_10_20'+ '_'+ str(common_10_20) +'\\t'+\n",
        "                  lex + str(20)+'\\t' +'interect_5_20'+'_'+ str(common_5_20))\n",
        "    FileOut.write('\\n')\n",
        "    FileOut.write('av_weight_5' +'\\t'+str(weight_5) + '\\t' +\n",
        "                  'av_weight_10' +'\\t'+str(weight_10) + '\\t' +\n",
        "                  'av_weight_20' +'\\t'+str(weight_20))\n",
        "    FileOut.write('\\n')\n",
        "    for i in range(0,wordnum):\n",
        "      #print(res5[i])\n",
        "      FileOut.write(res5[i][0]+'\\t'+str(res5[i][1]) +'\\t' + \n",
        "                    res10[i][0]+'\\t'+str(res10[i][1]) +'\\t' +\n",
        "                    res20[i][0]+'\\t'+str(res20[i][1]) +'\\t' +'\\n')\n",
        "    FileOut.flush()\n",
        "    FileOut.close()"
      ],
      "metadata": {
        "id": "dRXlOoVcEtnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXECUTE compaire spans (5, 10, 20) COVID\n",
        "!mkdir ./comp_covid/\n",
        "!rm ./comp_covid/*.txt\n",
        "CONN = [\"despite\",\"because\", \"since\", \"therefore\", \"thus\", \"hence\",\"although\", \"but\", \"nevertheless\", \"yet\", \"though\", \"furthemore\", \"indeed\"]\n",
        "MA = [\"prove\", \"judgement\", \"reason\", \"logic\", \"resulting\",\"conclusion\"]\n",
        "EV = [\"safe\", \"efficient\", \"dangerous\", \"risk\", \"critical\",\"help\", \"fortunately\", \"unfortunately\"]\n",
        "KN = [\"covid\", \"vaccination\", \"mortality\", \"decease\", \"pandemic\", \"infodemic\", \"virus\", \"prevention\", \"intensive\"]\n",
        "\n",
        "compaire(KN,\"KN\",100)\n",
        "!zip -r /content/comp_covid/key_notions.zip /content/comp_covid/*.txt\n",
        "!rm /content/comp_covid/*.txt\n",
        "\n",
        "compaire(CONN, \"CONN\",100)\n",
        "!zip -r /content/comp_covid/conn.zip /content/comp_covid/*.txt\n",
        "!rm /content/comp_covid/*.txt\n",
        "\n",
        "compaire(EV,\"EV\",100)\n",
        "!zip -r /content/comp_covid/eval_words.zip /content/comp_covid/*.txt\n",
        "!rm /content/comp_covid/*.txt\n",
        "\n",
        "compaire(MA,\"MA\",100)\n",
        "!zip -r /content/comp_covid/meta_arg.zip /content/comp_covid/*.txt\n",
        "!rm /content/comp_covid/*.txt\n"
      ],
      "metadata": {
        "id": "-SbOjAUqE4wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINITION compair spans (5, 10, 20) EP\n",
        "def compaire_EP(list_lex, list_name,wordnum):\n",
        "  import sys, re, os\n",
        "  import scipy.stats as ss\n",
        "  import matplotlib.pyplot as plt\n",
        "  import math\n",
        "  import csv\n",
        "  from statistics import mean\n",
        "  for lex in list_lex:\n",
        "    res5 = word_vectors_ep5.most_similar(lex, topn=wordnum)\n",
        "    fte_5 = [a_tuple[0] for a_tuple in res5]\n",
        "    weight_5 = mean([a_tuple[1] for a_tuple in res5])\n",
        "    res10 = word_vectors_ep10.most_similar(lex, topn=wordnum)\n",
        "    fte_10 = [a_tuple[0] for a_tuple in res10]\n",
        "    weight_10 = mean([a_tuple[1] for a_tuple in res10])\n",
        "    res20 = word_vectors_ep20.most_similar(lex, topn=wordnum)\n",
        "    fte_20 = [a_tuple[0] for a_tuple in res20]\n",
        "    weight_20 = mean([a_tuple[1] for a_tuple in res20])\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_5:\n",
        "      if word in fte_10: k += 1\n",
        "    common_5_10 = k\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_5:\n",
        "      if word in fte_20: k += 1\n",
        "    common_5_20 = k\n",
        "\n",
        "    k = 0\n",
        "    for word in fte_20:\n",
        "      if word in fte_10: k += 1\n",
        "    common_10_20 = k\n",
        "\n",
        "\n",
        "    FN = '/content/comp_ep/' +lex + '5_10_20.txt'\n",
        "    FileOut = open(FN, 'w')\n",
        "    FileOut.write(lex + str(5)+'\\t'+'interect_5_10'+' = '+str(common_5_10) +'\\t'+\n",
        "                  lex + str(10)+'\\t'+'interect_10_20'+' = '+str(common_10_20) +'\\t'+\n",
        "                  lex + str(20)+'\\t' +'interect_5_20'+' = '+str(common_5_20))\n",
        "    FileOut.write('\\n')\n",
        "    FileOut.write('av_weight_5' +'\\t'+str(weight_5) + '\\t' +\n",
        "                  'av_weight_10' +'\\t'+str(weight_10) + '\\t' +\n",
        "                  'av_weight_20' +'\\t'+str(weight_20))\n",
        "    FileOut.write('\\n')\n",
        "    for i in range(0,wordnum):\n",
        "      #print(res5[i])\n",
        "      FileOut.write(res5[i][0]+'\\t'+str(res5[i][1]) +'\\t' + \n",
        "                    res10[i][0]+'\\t'+str(res10[i][1]) +'\\t' +\n",
        "                    res20[i][0]+'\\t'+str(res20[i][1]) +'\\t' +'\\n')\n",
        "    \"\"\"\n",
        "      FileOut.write(str(Word[1]))\n",
        "      FileOut.write('\\n')\n",
        "    FileOut.write('\\n')\n",
        "    FileOut.write(lex + str(20))\n",
        "    FileOut.write('\\n')\n",
        "    for Word in res20:\n",
        "      #print(Word[1])\n",
        "      FileOut.write(Word[0]+'\\t')\n",
        "      FileOut.write(str(Word[1]))\n",
        "      FileOut.write('\\n')\n",
        "    \"\"\"\n",
        "    FileOut.flush()\n",
        "    FileOut.close()"
      ],
      "metadata": {
        "id": "m4KR_naYE_dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXECUTE compaire spans (5, 10, 20) EP\n",
        "!mkdir ./comp_ep\n",
        "!rm ./comp_ep/*.*\n",
        "conn_list = [\"despite\",\"because\", \"since\", \"therefore\", \"thus\", \"hence\",\"although\", \"but\", \"nevertheless\", \"yet\", \"though\", \"indeed\"]\n",
        "meta_arg = [\"prove\", \"judgement\", \"reason\", \"logic\", \"conclusion\"]\n",
        "eval_words = [\"safe\", \"efficient\", \"dangerous\", \"risk\", \"critical\",\"fortunately\", \"moral\", \"freedom\",\"immoral\",\"unfortunately\", \"human\", \"value\", \"democracy\", \"right\", \"principle\", \"liberty\", \"dignity\", \"oppression\", \"violation\"]\n",
        "key_notions = [\"union\", \"commission\", \"community\", \"member\", \"politics\", \"policy\", \"defence\", \"citizen\", \"election\"]\n",
        "compaire_EP(key_notions,\"KN\",500)\n",
        "!zip -r /content/comp_ep/key_notions.zip /content/comp_ep/*.txt\n",
        "!rm /content/comp_ep/*.txt\n",
        "\n",
        "compaire_EP(conn_list, \"CONN\",500)\n",
        "!zip -r /content/comp_ep/conn.zip /content/comp_ep/*.txt\n",
        "!rm /content/comp_ep/*.txt\n",
        "\n",
        "compaire_EP(eval_words,\"EV\",500)\n",
        "!zip -r /content/comp_ep/eval_words.zip /content/comp_ep/*.txt\n",
        "!rm /content/comp_ep/*.txt\n",
        "\n",
        "compaire_EP(meta_arg,\"MA\",500)\n",
        "!zip -r /content/comp_ep/meta_arg.zip /content/comp_ep/*.txt\n",
        "!rm /content/comp_ep/*.txt\n"
      ],
      "metadata": {
        "id": "x-rOKbFQFDsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLUSTERING\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.cluster\n",
        "import sklearn.metrics\n",
        "import re\n",
        "\n",
        "#NUMBER of clusters\n",
        "K=4\n",
        "\n",
        "\n",
        "#Extract word vector from a model\n",
        "\n",
        "#find \"synonyms\"\n",
        "lex = 'constitution'\n",
        "res5 = word_vectors_ep5.most_similar(lex, topn=200)\n",
        "#build the list of sysnonyms\n",
        "fte_5 = [a_tuple[0] for a_tuple in res5]\n",
        "\n",
        "model = model_ep05\n",
        "\n",
        "#list of words to cluster\n",
        "words = fte_5\n",
        "\n",
        "FIn = open('value_coll.txt', 'r')\n",
        "words = []\n",
        "for SWordNFrq in FIn:\n",
        "  try:\n",
        "    LWordNFrq = re.split('\\t', SWordNFrq)\n",
        "    SWord = LWordNFrq[0]\n",
        "    words.append(SWord)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "words = words[3:]\n",
        "\n",
        "NumOfWords = len(words)\n",
        "# construct the n-dimentional array for input data, each row is a word vector\n",
        "x = np.zeros((NumOfWords, model.vector_size))\n",
        "for i in range(0, NumOfWords):\n",
        "    x[i,]=model[words[i]] \n",
        "\n",
        "\n",
        "\n",
        "# train the k-means model\n",
        "classifier = MiniBatchKMeans(n_clusters=K, random_state=1, max_iter=100)\n",
        "classifier.fit(x)\n",
        "\n",
        "# check whether the words are clustered correctly\n",
        "\n",
        "# find the index and the distance of the closest points from x to each class centroid\n",
        "close = pairwise_distances_argmin_min(classifier.cluster_centers_, x, metric='euclidean')\n",
        "index_closest_points = close[0]\n",
        "distance_closest_points = close[1]\n",
        "\n",
        "#find the word nearest to the centroid for all clusters (apparently, it can be from another cluster)\n",
        "for i in range(0, K):\n",
        "    print(\"The closest word to the centroid of class {0} is {1}, the distance is {2}\".format(i, words[index_closest_points[i]], distance_closest_points[i]))\n",
        "\n",
        "clusters = classifier.predict(x)\n",
        "\n",
        "zip_iterator = zip(fte_5, classifier.predict(x))\n",
        "\n",
        "\n",
        "a_dic ={}\n",
        "for key, value in zip_iterator:\n",
        "  a_dic[key] = value\n",
        "  \n",
        "print(\"********************\")\n",
        "print(a_dic)\n",
        "\n",
        "FO = FileOut = open(lex+'.txt', 'w')\n",
        "\n",
        "for Word, cluster in sorted(a_dic.items(), key=lambda x: x[1], reverse=False):\n",
        "\n",
        "#save the clusters\n",
        "  FileOut.write(Word)\n",
        "  FileOut.write('\\t')\n",
        "  FileOut.write(str(cluster))\n",
        "  FileOut.write('\\n')\n",
        "\n",
        "#find the word nearest to the centroid for all clusters (apparently, it can be from another cluster)\n",
        "for i in range(0, K):\n",
        "    print(\"The closest word to the centroid of class {0} is {1}, the distance is {2}\".format(i, words[index_closest_points[i]], distance_closest_points[i]), file = FileOut)\n",
        "\n",
        "FileOut.flush()\n",
        "FileOut.close()\n",
        "\n",
        "cost =[]\n",
        "for i in range(1, 5):\n",
        "    KM = KMeans(n_clusters = i, max_iter = 100)\n",
        "    KM.fit(x)\n",
        "     \n",
        "    # calculates squared error  for the clustered points\n",
        "    cost.append(KM.inertia_)    \n",
        "# plot the cost against K values\n",
        "#plt.plot(range(1, 5), cost, color ='g', linewidth ='3')\n",
        "#plt.xlabel(\"Value of K\")\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
        "silhouette_avg = []\n",
        "for num_clusters in range_n_clusters:\n",
        " \n",
        " # initialise kmeans\n",
        " kmeans = KMeans(n_clusters=num_clusters, max_iter = 100)\n",
        " kmeans.fit(x)\n",
        " cluster_labels = kmeans.labels_\n",
        " \n",
        " # silhouette score\n",
        " ss = sklearn.metrics.silhouette_score(x, cluster_labels)\n",
        " silhouette_avg.append(ss)\n",
        "plt.plot(range_n_clusters,silhouette_avg,'bx-')\n",
        "plt.xlabel('Values of K') \n",
        "plt.ylabel('Silhouette score') \n",
        "plt.title('Silhouette analysis For Optimal k')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "60qFRq8qFLvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}