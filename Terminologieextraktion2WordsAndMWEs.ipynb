{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion2WordsAndMWEs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion2WordsAndMWEs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir693xy5khPU"
      },
      "source": [
        "# Terminology extraction experiments\n",
        "\n",
        "## Improving and running terminology extraction script on different corpora\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtWZ1FuvlfLk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXKKCk4ow8fQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f703abe-6213-44d4-a312-03617e7f02a9"
      },
      "source": [
        "# ArmenianWP\n",
        "# !wget https://heibox.uni-heidelberg.de/f/206d85a0270943e4b87b/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 WPhy_vert.txt\n",
        "\n",
        "# Constitution Republic of Armenia\n",
        "# !wget https://heibox.uni-heidelberg.de/f/bf54977b17604ec592cd/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 Const_RA_l.txt\n",
        "\n",
        "# Grundgesetz\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d6c5b31edc84422d9e14/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 GG_l.txt\n",
        "\n",
        "# Origin of the species\n",
        "# !wget https://heibox.uni-heidelberg.de/f/befc6dbe718b4a37ba74/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 OOOS_l.txt\n",
        "\n",
        "# Europarl DE\n",
        "# !wget https://heibox.uni-heidelberg.de/f/3ba6122e744e4b7f9c14/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 EP_DE_l.txt\n",
        "\n",
        "# German legal corpus\n",
        "!wget https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 BGH-utf8-lem.zip\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-22 13:50:06--  https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/6ea33bf2-3c70-43ef-97b2-db944a36ad34/output-utf8-lem.zip [following]\n",
            "--2021-10-22 13:50:07--  https://heibox.uni-heidelberg.de/seafhttp/files/6ea33bf2-3c70-43ef-97b2-db944a36ad34/output-utf8-lem.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 673144348 (642M) [application/zip]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 641.96M  16.6MB/s    in 43s     \n",
            "\n",
            "2021-10-22 13:50:51 (14.8 MB/s) - ‘index.html?dl=1’ saved [673144348/673144348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAvDk1BRQtsZ",
        "outputId": "21431cc6-83e4-4c40-bf60-991a4440ef48"
      },
      "source": [
        "!unzip BGH-utf8-lem.zip\n",
        "!rm BGH-utf8-lem.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BGH-utf8-lem.zip\n",
            "  inflating: output-utf8-lem.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_OKgqfNRmQO"
      },
      "source": [
        "!mv output-utf8-lem.txt BGHlem.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koQG-wAsR0rQ"
      },
      "source": [
        "!head --lines=100000 BGHlem.txt >BGHlem1k.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMF8TyfHjmAy",
        "outputId": "3bd67d0e-5dfb-4aa5-8c0d-a83f2c3188ad"
      },
      "source": [
        "# debugging and explaining the algorithm for Stage1:\n",
        "LLCandidates = []\n",
        "LWords = ['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n",
        "for klen in range(len(LWords)): # lengths of candidate lists\n",
        "    klength = klen+1 # true length: for 0 it is le = 1\n",
        "    print(f'klen:{klength};')\n",
        "    for i in range(len(LWords) - klen): # positions where candidates start\n",
        "        print(f'i:{i};')\n",
        "        LCandidate = LWords[i:i+klength]\n",
        "        LLCandidates.append(LCandidate)\n",
        "        print(LCandidate)\n",
        "\n",
        "        # print(LWords[i])\n",
        "\n",
        "for LEl in LLCandidates:\n",
        "    # for el in LEl:\n",
        "    print(LEl)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "klen:1;\n",
            "i:0;\n",
            "['Revisionsverfahren']\n",
            "i:1;\n",
            "['entstanden']\n",
            "i:2;\n",
            "['notwendig']\n",
            "i:3;\n",
            "['Auslage']\n",
            "klen:2;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden']\n",
            "i:1;\n",
            "['entstanden', 'notwendig']\n",
            "i:2;\n",
            "['notwendig', 'Auslage']\n",
            "klen:3;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig']\n",
            "i:1;\n",
            "['entstanden', 'notwendig', 'Auslage']\n",
            "klen:4;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n",
            "['Revisionsverfahren']\n",
            "['entstanden']\n",
            "['notwendig']\n",
            "['Auslage']\n",
            "['Revisionsverfahren', 'entstanden']\n",
            "['entstanden', 'notwendig']\n",
            "['notwendig', 'Auslage']\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig']\n",
            "['entstanden', 'notwendig', 'Auslage']\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeamIEr15Lev"
      },
      "source": [
        "#Terminologieextraktion\n",
        "import os, re, sys\n",
        "class clProcCorpus(object):\n",
        "    ''' we will read a text file and return a dictionary\n",
        "    this will be done on the line by line basis\n",
        "    The dictionary can be sorted later...\n",
        "    '''\n",
        "    # this is a class for processing a corpus\n",
        "\n",
        "    def __init__(self, FileIN):\n",
        "        self.DictFrq = {}\n",
        "        self.processCorpus(FileIN)\n",
        "\n",
        "    def processCorpus(self, FileIN):\n",
        "        # here we consider a larger MWE, which has been collected, e.g., \n",
        "        # Wahl Schluckebier Nachschlagewerk;\n",
        "        # graphical user interface\n",
        "        # in the default version only the longest string is preserved;\n",
        "        # we try to split it into meaningful smaller units and preserve them as well:\n",
        "        # Wahl Schluckebier\n",
        "        # graphical user ; user interface ; interface\n",
        "        # PoS restrictions apply: \n",
        "        # - Adj cannot be at the end; function words (Prepositions, articles, etc.) cannot be on either edge.\n",
        "        # so we keep the list of tuples: and extract smaller MWEs from the larger strings using PoS restrictions\n",
        "\n",
        "\n",
        "        INGram = 2 ## -- homework: n-gram size\n",
        "        # collecting the longest string\n",
        "        LLTerm = []\n",
        "        for Line in FileIN:\n",
        "            Line = Line.strip()\n",
        "            LLine = re.split('\\t', Line)\n",
        "            \n",
        "            try:\n",
        "                Word = LLine[0]\n",
        "                PoS = LLine[1]\n",
        "                Lemma = LLine[2]\n",
        "            except:\n",
        "                Word = \"\"\n",
        "                PoS = \"\"\n",
        "                Lemma = \"\"\n",
        "            \n",
        "            #Select the Tags for your langauge\n",
        "            #if re.match('N.*', PoS) or re.match('A.*', PoS): #Arm\n",
        "            #if re.match('N.*', PoS) or re.match('J.*', PoS): #EN\n",
        "            if re.match('N.*', PoS) or re.match('ADJ.*', PoS): #DE\n",
        "                LLTerm.append(LLine) # all the field as a list, to form the list of lists\n",
        "\n",
        "                #Terms as Words or Lemmas\n",
        "                # to compare with the gold standard do we need words, or do we need to generate words from lemas?\n",
        "                # LTerm.append(Lemma)\n",
        "                # here we keep all the fields, because sub-sections of the longer MWE would also be analysed using PoS codes\n",
        "\n",
        "            else: # end of the 'candidate collection'\n",
        "                ### changed to an algorithm based on the data format: list of lists\n",
        "\n",
        "                # stage 1: we generate candidate sub-n-grams, starting from the largest one\n",
        "                L3Candidates = [] # this is the list of all MWE candidates\n",
        "                IMaxMWE = len(LLTerm) # this is the longest MWE we can get from LLTerm\n",
        "                for klen in range(IMaxMWE): # lengths of candidate lists\n",
        "                    klength = klen+1 # true length: for 0 it is le = 1\n",
        "                    # print(f'klen:{klength};')\n",
        "                    for i in range(IMaxMWE - klen): # positions where candidates start\n",
        "                        # print(f'i:{i};')\n",
        "                        L2Candidate = LLTerm[i:i+klength]\n",
        "                        L3Candidates.append(L2Candidate)\n",
        "\n",
        "                # stage 2: filtering by part-of-speech configurations what is impossible\n",
        "                L3CandidatesFilt = []\n",
        "                for L2Candidate in L3Candidates: # for each candidate configuration\n",
        "                    # if Adj at the end -- remove\n",
        "                    FileDebug.write(str(L2Candidate) + '\\n') \n",
        "                FileDebug.write('-\\n')\n",
        "\n",
        "\n",
        "\n",
        "                ####\n",
        "                # -- homework goes here -- \n",
        "                # -- break longer MWE into MWEs of size INGram\n",
        "                '''\n",
        "                for i in range(len(LTerm) - INGram):\n",
        "                    # if i+i+INGram > len(LTerm): break\n",
        "                    try: LWordsMWE = LTerm[i:i+INGram]\n",
        "                    except: LWordsMWE = []\n",
        "                    if LWordsMWE:\n",
        "                        SWordsMWE = ' '.join(LWordsMWE)\n",
        "                        # testing effectiveness:\n",
        "                        # SWordsMWE += ' ~~'\n",
        "                        try:\n",
        "                            self.DictFrq[SWordsMWE] += 1\n",
        "                        except:\n",
        "                            self.DictFrq[SWordsMWE] = 1                        \n",
        "                '''\n",
        "                # avoiding adding repeated sequences into dictionary:\n",
        "\n",
        "                # -- end: homework\n",
        "                # if we still need longer MWEs: (if not , just put ''' ''' around this block up to self.DictFrq[STerm] = 1)\n",
        "                # if len(LTerm) != INGram:  \n",
        "\n",
        "                ## -- this is going to be replaced; correct field identified...             \n",
        "                ## STerm = ' '.join(LTerm)\n",
        "                LLTerm = []\n",
        "\n",
        "                try:\n",
        "                    # self.DictFrq[STerm] += 1\n",
        "                    self.DictFrq[str(LLTerm)] += 1\n",
        "                except:\n",
        "                    # self.DictFrq[STerm] = 1\n",
        "                    self.DictFrq[str(LLTerm)] = 1       \n",
        "                \n",
        "        return\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnxocDPBMHcL"
      },
      "source": [
        "FileDebug = open('BGH_debug.txt', 'w')\n",
        "# FileIN = open('BGHlem.txt', 'r')\n",
        "FileIN = open('BGHlem1k.txt', 'r')\n",
        "\n",
        "FileOut1w = open('BGH_term1w.txt', 'w')\n",
        "FileOutMWE = open('BGH_termMWE.txt', 'w')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY0xMY9SMSEE"
      },
      "source": [
        "# save the frequency dictionary into file, by decreasing frequencies\n",
        "# FileOutput.write( str( DictionaryFrq ) + '\\n' )\n",
        "\n",
        "OCorpus = clProcCorpus(FileIN)\n",
        "DictionaryFrq = OCorpus.DictFrq\n",
        "\n",
        "\n",
        "for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[1], reverse=True):\n",
        "    if re.search(' ', Word):\n",
        "        FileOutMWE.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    else:\n",
        "        FileOut1w.write(Word + '\\t' + str(Frq) + '\\n')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUPLnj8E5byZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}