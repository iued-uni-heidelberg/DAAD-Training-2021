{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion8AEvaluationKeyWordsV03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTfz1GjJHuXGK3+BaG7/lE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion8AEvaluationKeyWordsV03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_u_1JsuhpJV"
      },
      "source": [
        "# Terminology extraction with keywords, association measures, etc.\n",
        "\n",
        "Here we enrich extracted MWEs with keyness information (and other paremeters) and test Precision / Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI638SJm_FAJ"
      },
      "source": [
        "## Preparing gold standard annotation dictinaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWLBEC9UQGCS"
      },
      "source": [
        "# Stage 0: Some useful read/write and convert functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vFQdVPvMPWr"
      },
      "source": [
        "# import useful libraries\n",
        "import re, os, sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXrfaSo8QlWL",
        "outputId": "9b878dcf-f048-4130-8a82-5b23eabecfe0"
      },
      "source": [
        "# file for recording results of different configurations\n",
        "!rm AllTermExtractionResultsV01.txt\n",
        "!rm AllTermExtractionResultsV02.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'AllTermExtractionResultsV01.txt': No such file or directory\n",
            "rm: cannot remove 'AllTermExtractionResultsV02.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR2ZYBrxOhIt"
      },
      "source": [
        "FOutResults1 = open('AllTermExtractionResultsV01.txt', 'a')\n",
        "FOutResults2 = open('AllTermExtractionResultsV02.txt', 'a')\n",
        "FOutResults1.write('Run\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "FOutResults2.write('Run\\tW1P\\tW1R\\tW2P\\tW2R\\tW3P\\tW3R\\tW4P\\tW4R\\n') # only precision and recall figures\n",
        "FOutResults1.flush()\n",
        "FOutResults2.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNK9JjVN1_N7"
      },
      "source": [
        "# a useful function for recording / visualising current stage of dictionaries\n",
        "def printDictionary(DictionaryFrq, FOut, K = 1, Rev = True): # printing a dictionary: by values or alphabetically\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[K], reverse=Rev):\n",
        "        FOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    FOut.flush()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgCuEZ8vGeNW"
      },
      "source": [
        "# another useful function to just read and return a 2-field dictionary, eg., frequency or keyness\n",
        "def readDictionary(FIN, SkipComments = True, Caps=False):\n",
        "    DScoresLarge = {} # keywords - scores\n",
        "    for Line in FIN:\n",
        "        if SkipComments and re.match('#', Line): \n",
        "            continue\n",
        "        Line = Line.strip()\n",
        "        if Caps: \n",
        "            Line = Line.upper() # convert to upper case\n",
        "        LFieldsKW = re.split('\\t', Line)\n",
        "        SWord = LFieldsKW[0]\n",
        "        AKScore = float(LFieldsKW[1])\n",
        "        DScoresLarge[SWord] = AKScore   \n",
        "    return DScoresLarge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpzfGgOnKzZu"
      },
      "source": [
        "# another possibly useful function: convert dictionary values to ranks (frequency, keyness weights, etc.)\n",
        "# for understanding how far down the list the item has been found...\n",
        "def rankDict(DIN):\n",
        "    '''\n",
        "    reading a frequency dictionary from a file\n",
        "    '''\n",
        "    DTermRanks = {}\n",
        "    i = 0\n",
        "    IRank = 0\n",
        "    IPrevFrq = 0\n",
        "    SumRanks = 0\n",
        "    for SKey, Frq in DIN.items():\n",
        "        # if re.match('#', SKey): continue # skipping comments\n",
        "        i+=1\n",
        "        if IPrevFrq != Frq: IRank = i # rank is the number of the highest ranking element of the same frequency group\n",
        "        IPrevFrq = Frq\n",
        "        \n",
        "        DTermRanks[SKey] = IRank\n",
        "        SumRanks += IRank\n",
        "\n",
        "    AAveRank = SumRanks / i\n",
        "    print(f'MaxRank = {IRank}\\nAve Rank = {AAveRank}\\n')\n",
        "    return DTermRanks, AAveRank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQ8SJI5kvF2"
      },
      "source": [
        "# Main evaluation function\n",
        "# One-directional comparision of dictionaries\n",
        "# one-directional comparison of two dictionaries; arguments: DGoldStandard (smaller) DTest (larger), file: GS items found in DTest; GS items missing from DText...\n",
        "# usually testing: smaller vs. bigger dictionaries\n",
        "def countIntersectDictionaries(DGS, DTest, FOutputPrecFOUND, FOutputPrecMISSING, SortBy = 0, Rev = False):\n",
        "    '''\n",
        "    general function: intersect dictionaries, return new intersection dictionaries, record \"in\" and \"out\" expressions\n",
        "    \n",
        "    3b: intersecting All possible MWEs in GS list with the \"Extracted\" list\n",
        "    DA (smaller and going over each element) with D1W / DMWE lists \n",
        "    '''\n",
        "\n",
        "    print('Total len of Gold Standard: ' + str(len(DGS.items())))\n",
        "    IFound = 0\n",
        "    IMissing = 0\n",
        "    SumFoundRanks = 0\n",
        "    DFound = {} # intersection dictionary\n",
        "\n",
        "    for Word, Frq in sorted(DGS.items(),  key=lambda x: x[SortBy], reverse=Rev):\n",
        "        if Word in DTest:\n",
        "            IFound += 1\n",
        "            try: # normally will not fire: if this word already exists with some rank, calculate the average of a new and old rank\n",
        "                r0 = DFound[Word]\n",
        "                r1 = DTest[Word]\n",
        "                r = (r0+r1)/2\n",
        "                DFound[Word] = r\n",
        "                print('r?')\n",
        "            except: # normal route: find the rank of the word in the dictionary\n",
        "                DFound[Word] = DTest[Word]\n",
        "\n",
        "            SumFoundRanks += DTest[Word] # add rank, to calculate average\n",
        "            try: FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + str(DFound[Word]) + '\\n') # record/calculate average rank, etc.\n",
        "            except: \n",
        "                FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "                print(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "        else:\n",
        "            IMissing += 1\n",
        "            FOutputPrecMISSING.write(Word + '\\t' + str(Frq) + '\\n') # record/calculate average rank, etc.\n",
        "\n",
        "    print(f'Found: {IFound}')\n",
        "    print(f'Missing: {IMissing}')\n",
        "    try: ACoverage = IFound / len(DGS.items())\n",
        "    except: ACoverage = 0\n",
        "    print(f'Found2LenGS: {ACoverage}')\n",
        "    try: AAverageFoundRanks = SumFoundRanks / IFound\n",
        "    except: AAverageFoundRanks = 0\n",
        "    print(f'Ave Found Ranks: {AAverageFoundRanks} \\n')\n",
        "\n",
        "    FOutputPrecFOUND.flush()\n",
        "    FOutputPrecMISSING.flush()\n",
        "\n",
        "    return ACoverage, AAverageFoundRanks, DFound\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEIsH60_hdEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d1387a-0f17-4f46-d36b-405e7dfc901a"
      },
      "source": [
        "# Stage 1: Preparing Gold standard: Reading / extracting information from gold standard: creating a list of annotated terms\n",
        "# set 1 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Astghik.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Maia.txt\n",
        "\n",
        "# set 2 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Hayk.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Nino.txt\n",
        "\n",
        "# set 3 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
        "!mv index.html?dl=1 BGH3_s00Tamar.txt\n",
        "# !wget \n",
        "# !mv index.html?dl=1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-29 08:00:02--  https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/a0e66e03-86b9-487b-ad93-1e5363e001e3/BGH1_Astghik.txt [following]\n",
            "--2021-10-29 08:00:03--  https://heibox.uni-heidelberg.de/seafhttp/files/a0e66e03-86b9-487b-ad93-1e5363e001e3/BGH1_Astghik.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 369792 (361K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 361.12K   638KB/s    in 0.6s    \n",
            "\n",
            "2021-10-29 08:00:04 (638 KB/s) - ‘index.html?dl=1’ saved [369792/369792]\n",
            "\n",
            "--2021-10-29 08:00:04--  https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/7fd17694-06bf-449c-ae87-752dc9907cd4/BGH1_Maia.txt [following]\n",
            "--2021-10-29 08:00:04--  https://heibox.uni-heidelberg.de/seafhttp/files/7fd17694-06bf-449c-ae87-752dc9907cd4/BGH1_Maia.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 389237 (380K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 380.11K   764KB/s    in 0.5s    \n",
            "\n",
            "2021-10-29 08:00:05 (764 KB/s) - ‘index.html?dl=1’ saved [389237/389237]\n",
            "\n",
            "--2021-10-29 08:00:05--  https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/01366f96-df62-4941-8845-cd51b975811b/BGH2_Hayk.txt [following]\n",
            "--2021-10-29 08:00:05--  https://heibox.uni-heidelberg.de/seafhttp/files/01366f96-df62-4941-8845-cd51b975811b/BGH2_Hayk.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 411864 (402K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 402.21K   651KB/s    in 0.6s    \n",
            "\n",
            "2021-10-29 08:00:06 (651 KB/s) - ‘index.html?dl=1’ saved [411864/411864]\n",
            "\n",
            "--2021-10-29 08:00:06--  https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/f6d7e24c-11b4-4363-992d-08b30dcfbbfb/BGH2_Nino.txt [following]\n",
            "--2021-10-29 08:00:07--  https://heibox.uni-heidelberg.de/seafhttp/files/f6d7e24c-11b4-4363-992d-08b30dcfbbfb/BGH2_Nino.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 442399 (432K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 432.03K   753KB/s    in 0.6s    \n",
            "\n",
            "2021-10-29 08:00:07 (753 KB/s) - ‘index.html?dl=1’ saved [442399/442399]\n",
            "\n",
            "--2021-10-29 08:00:07--  https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/2a7f423e-5213-477d-a59f-ebfa33ff6017/BGH3_Tamar.txt [following]\n",
            "--2021-10-29 08:00:08--  https://heibox.uni-heidelberg.de/seafhttp/files/2a7f423e-5213-477d-a59f-ebfa33ff6017/BGH3_Tamar.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 501600 (490K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 489.84K   778KB/s    in 0.6s    \n",
            "\n",
            "2021-10-29 08:00:09 (778 KB/s) - ‘index.html?dl=1’ saved [501600/501600]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LdbtxRwwauy"
      },
      "source": [
        "# one more will be added: Frau Khachatryan\n",
        "!cat BGH1_s00Astghik.txt BGH1_s00Maia.txt BGH2_s00Hayk.txt BGH2_s00Nino.txt BGH3_s00Tamar.txt >BGH0_s00GoldStandard.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65srZoeW-wwA"
      },
      "source": [
        "FInput = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "FOutput = open('BGH0_s01GoldS_Terms.txt', 'w')\n",
        "# for statistical purposes - separately single and multiword terms\n",
        "FOutputDict1w = open('BGH0_s01GoldS_D1w.txt', 'w') # 1-word terms\n",
        "FOutputDict2w = open('BGH0_s01GoldS_D2w.txt', 'w') # 2-word terminological expressions\n",
        "FOutputDict3w = open('BGH0_s01GoldS_D3w.txt', 'w') # 3-word terminological expressions\n",
        "FOutputDictMWE = open('BGH0_s01GoldS_DMWE.txt', 'w') # more than 3 words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB30Z1Fm6Ic0"
      },
      "source": [
        "# creating gold-standard dictionaries for evaluation tasks:\n",
        "# import re, os, sys\n",
        "LGSTerms = [] # gold standard terms\n",
        "DGS1w = {} # dictionary of single words\n",
        "DGS2w = {} # dictionary of 2-word expressions\n",
        "DGS3w = {} # dictionary of 3-word expressions\n",
        "DGSMWE = {} # dictionary of other mwes\n",
        "IGS1w = 0 # number of annotated tokens of single words\n",
        "IGS2w = 0\n",
        "IGS3w = 0\n",
        "IGSMWE = 0 # number of annotated tokens of multiwords\n",
        "for SLine in FInput:\n",
        "    LAnnotatedTermsInLine = re.findall('<<([^><]+)>>', SLine)\n",
        "    LGSTerms.extend(LAnnotatedTermsInLine)\n",
        "\n",
        "for GSTerm in LGSTerms:\n",
        "    GSTerm = GSTerm.strip()\n",
        "    GSTerm = GSTerm.strip('„“\"().')\n",
        "\n",
        "    # everything is converted to upper case for quick dictionary lookup\n",
        "    GSTerm = GSTerm.upper()\n",
        "    \n",
        "    GSTerm = re.sub(' +', ' ', GSTerm)\n",
        "    LGSTErms = re.split(' ', GSTerm)\n",
        "    if len(LGSTErms) > 3:\n",
        "        IGSMWE += 1\n",
        "        try: DGSMWE[GSTerm] += 1\n",
        "        except: DGSMWE[GSTerm] = 1\n",
        "    elif len(LGSTErms) > 2:\n",
        "        IGS3w += 1\n",
        "        try: DGS3w[GSTerm] += 1\n",
        "        except: DGS3w[GSTerm] = 1\n",
        "    elif len(LGSTErms) > 1:\n",
        "        IGS2w += 1\n",
        "        try: DGS2w[GSTerm] +=1\n",
        "        except: DGS2w[GSTerm] = 1\n",
        "    else:\n",
        "        IGS1w += 1\n",
        "        try: DGS1w[GSTerm] +=1\n",
        "        except: DGS1w[GSTerm] = 1\n",
        "\n",
        "    FOutput.write(GSTerm + '\\n')\n",
        "\n",
        "FOutputDictMWE.write('# Number of tokens: ' + str(IGSMWE) + '\\n')\n",
        "FOutputDict3w.write('# Number of tokens: ' + str(IGS3w) + '\\n')\n",
        "FOutputDict2w.write('# Number of tokens: ' + str(IGS2w) + '\\n')\n",
        "FOutputDict1w.write('# Number of tokens: ' + str(IGS1w) + '\\n')\n",
        "\n",
        "printDictionary(DGSMWE, FOutputDictMWE)\n",
        "printDictionary(DGS3w, FOutputDict3w)\n",
        "printDictionary(DGS2w, FOutputDict2w)\n",
        "printDictionary(DGS1w, FOutputDict1w)\n",
        "\n",
        "FOutputDictMWE.flush()\n",
        "FOutputDictMWE.close()\n",
        "FOutputDict3w.flush()\n",
        "FOutputDict3w.close()\n",
        "FOutputDict2w.flush()\n",
        "FOutputDict2w.close()\n",
        "FOutputDict1w.flush()\n",
        "FOutputDict1w.close()\n",
        "\n",
        "FOutput.flush()\n",
        "FOutput.close()\n",
        "\n",
        "FInput.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0gL2rD-6Py"
      },
      "source": [
        "## Preparing the 'keyness' dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iceETOGHG8ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a748ad-22dd-4201-bb8e-9d5e252e0f15"
      },
      "source": [
        "# Stage 2: preparing keyness dictionary\n",
        "!wget https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
        "!mv index.html?dl=1 TK_KW_Verif_V02.csv\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
        "!mv index.html?dl=1 KW_BGH_10000.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 20:14:19--  https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/b031ca77-aa49-4262-a734-d8156bfad85b/TK_KW_Verif_V02.csv [following]\n",
            "--2021-10-28 20:14:20--  https://heibox.uni-heidelberg.de/seafhttp/files/b031ca77-aa49-4262-a734-d8156bfad85b/TK_KW_Verif_V02.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38812 (38K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  37.90K   103KB/s    in 0.4s    \n",
            "\n",
            "2021-10-28 20:14:21 (103 KB/s) - ‘index.html?dl=1’ saved [38812/38812]\n",
            "\n",
            "--2021-10-28 20:14:21--  https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/a7b2c2c2-82f5-42fe-a613-134b58c5a829/KW_BGH_10000.csv [following]\n",
            "--2021-10-28 20:14:21--  https://heibox.uni-heidelberg.de/seafhttp/files/a7b2c2c2-82f5-42fe-a613-134b58c5a829/KW_BGH_10000.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216590 (212K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 211.51K   410KB/s    in 0.5s    \n",
            "\n",
            "2021-10-28 20:14:22 (410 KB/s) - ‘index.html?dl=1’ saved [216590/216590]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2okBQ92YwfKw"
      },
      "source": [
        "# Preparing a dictionnary of keyness weights, checking the 'approval' status\n",
        "FInputKW = open('TK_KW_Verif_V02.csv', 'r')\n",
        "FInputKWLarge = open('KW_BGH_10000.tsv', 'r') # for experiments with Precision / Recall\n",
        "FOutputKW = open('TK_KW_Verif_V02.txt', 'w')\n",
        "# FOutputGSKWS1w = open('BGH1_s01GoldSKW_D1w.txt', 'w')\n",
        "# FOutputGSKWS2w = open('BGH1_s01GoldSKW_D2w.txt', 'w')\n",
        "# FOutputGSKWS3w = open('BGH1_s01GoldSKW_D3w.txt', 'w')\n",
        "# FOutputGSKWSMWE = open('BGH1_s01GoldSKW_MWE.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJDmvPaOm1SD"
      },
      "source": [
        "del DScoresKW\n",
        "del DScoresNK\n",
        "del DStatKW\n",
        "del DScoresKWLarge\n",
        "del DScoresREKWquick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im1Syy-JwqWt"
      },
      "source": [
        "# \n",
        "DScoresKW = {} # keywords - scores\n",
        "DScoresNK = {} # non-keywords\n",
        "DStatKW = {} # status: key/non-key-word\n",
        "for Line in FInputKW:\n",
        "    LFieldsKW = re.split('\\t', Line) # add: strip()\n",
        "    SWord = LFieldsKW[1]\n",
        "    AKScore = float(LFieldsKW[2])\n",
        "    AKStat = float(LFieldsKW[3])\n",
        "    DStatKW[SWord] = AKStat\n",
        "    if AKStat >= 0: # change value to 0.5 if we need to restrict to 'sure' terms only (value 1)\n",
        "        DScoresKW[SWord] = AKScore\n",
        "    else:\n",
        "        DScoresNK[SWord] = AKScore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dg2sbpGB5ze"
      },
      "source": [
        "# reading the large keyword dictionary\n",
        "DScoresKWLarge = readDictionary(FInputKWLarge, Caps=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XowrNS8l0YQk"
      },
      "source": [
        "# we create a dictionary of keyness values with only upper case letters, which will be checked against also uppercased term candidates\n",
        "# the same dictionary as DScoresKW, but with ensured conversion in to upper case:\n",
        "DScoresREKWquick = {} # dictionary of RE\n",
        "for kw, val in DScoresKW.items():\n",
        "    SUpperC = kw.upper() # making sure our key words are in upper case\n",
        "    # these are the alternatives, which we do not consider in this stage...\n",
        "    # SLowerC = kw.lower()\n",
        "    # SSentenceC = kw.capitalize()\n",
        "    # RPatternKW = re.compile('^' + kw + '$', re.IGNORECASE)\n",
        "    DScoresREKWquick[SUpperC] = val # \n",
        "    # DScoresREKWquick[SLowerC] = val # \n",
        "    # DScoresREKWquick[SSentenceC] = val # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtUKUkDi5fdd"
      },
      "source": [
        "printDictionary(DScoresREKWquick, FOutputKW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf3a_32PIioD"
      },
      "source": [
        "## Preparing a dictionary of automatically extracted terms using PoS configurations vs. key-word based enhancements and re-orderings of this list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVXmJo4PHx5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070bee60-b7ff-411f-8ad2-73d4158c9cb9"
      },
      "source": [
        "# Stage 3: Reading a file with extracted terms; capitalizing everything...\n",
        "# Reading test data - Possible Terms (extracted automatically): reading the text files of single and multiword terms, recording ranks\n",
        "# single words candidates\n",
        "#\n",
        "# Warning: these files are 8 and 70 MB respectively (relatively large to view on-line)\n",
        "!wget https://heibox.uni-heidelberg.de/f/a9171080790f4932b7b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02term1w.txt\n",
        "\n",
        "# multiword candidates\n",
        "!wget https://heibox.uni-heidelberg.de/f/2488701205e34e4683b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02termMWE.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 22:21:12--  https://heibox.uni-heidelberg.de/f/a9171080790f4932b7b1/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/7e90df7c-7a4d-4183-bbd6-aa3913d36d59/BGH_term1w.txt [following]\n",
            "--2021-10-28 22:21:12--  https://heibox.uni-heidelberg.de/seafhttp/files/7e90df7c-7a4d-4183-bbd6-aa3913d36d59/BGH_term1w.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8954371 (8.5M) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]   8.54M  7.48MB/s    in 1.1s    \n",
            "\n",
            "2021-10-28 22:21:14 (7.48 MB/s) - ‘index.html?dl=1’ saved [8954371/8954371]\n",
            "\n",
            "--2021-10-28 22:21:14--  https://heibox.uni-heidelberg.de/f/2488701205e34e4683b1/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/8b57ff93-e13f-4079-9b60-98d9e8595dd6/BGH_termMWE.txt [following]\n",
            "--2021-10-28 22:21:14--  https://heibox.uni-heidelberg.de/seafhttp/files/8b57ff93-e13f-4079-9b60-98d9e8595dd6/BGH_termMWE.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78612600 (75M) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  74.97M  15.9MB/s    in 4.9s    \n",
            "\n",
            "2021-10-28 22:21:19 (15.4 MB/s) - ‘index.html?dl=1’ saved [78612600/78612600]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDa-Ti5JHn_h"
      },
      "source": [
        "FAutoTerms1w = open('BGH0_s02term1w.txt', 'r')\n",
        "FAutoTermsMWE = open('BGH0_s02termMWE.txt', 'r')\n",
        "\n",
        "FoutAutoTerms1w = open('BGH0_s02term1w_out.txt', 'w')\n",
        "FoutAutoTermsMWE = open('BGH0_s02termMWE_out.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no-I_PP8Y9So"
      },
      "source": [
        "# ... here we add functions for reading this dictionary (e.g., as ranked list, etc.)\n",
        "DAutoTerms1w = readDictionary(FAutoTerms1w, Caps=True)\n",
        "DAutoTermsMWE = readDictionary(FAutoTermsMWE, Caps=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45N7ox-OKpD6"
      },
      "source": [
        "# optional: convert values to ranks\n",
        "# DAutoTermsR1w = rankDict(DAutoTerms1w)\n",
        "# DAutoTermsRMWE = rankDict(DAutoTermsMWE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKKpyKSaW85a"
      },
      "source": [
        "printDictionary(DAutoTerms1w,FoutAutoTerms1w)\n",
        "printDictionary(DAutoTermsMWE,FoutAutoTermsMWE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1IwZfeeNtGW"
      },
      "source": [
        "# Stage 3.1 combine extracted words with keyness (e.g., filter by keyness, etc.)\n",
        "# here we will add / combine information about keyness...\n",
        "# to be implemented...\n",
        "# only allow those terms into the Auto dictionary, which have weights; replace frq by keyness weights (or sum)\n",
        "# function to be used on all dictionaries:\n",
        "\n",
        "def filterDictByKWDict(DAuto, DKeyness, Threshold = 0):\n",
        "    DAutoFiltered = {}\n",
        "    for SAutoTerm, Frq in DAuto.items():\n",
        "        AKeynessAll = 0\n",
        "        LAutoTermWs = re.split(' ', SAutoTerm)\n",
        "        for STerm in LAutoTermWs:\n",
        "            if STerm in DKeyness:\n",
        "                if DKeyness[STerm] > AKeynessAll:\n",
        "                    AKeynessAll = DKeyness[STerm] # we take the maximum keyness\n",
        "        if AKeynessAll > Threshold:\n",
        "            DAutoFiltered[SAutoTerm] = AKeynessAll\n",
        "\n",
        "    return DAutoFiltered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4XTFcK58xvB"
      },
      "source": [
        "# DAutoTerms01KW_H1w = filterDictByKWDict(DAutoTerms1w, DScoresREKWquick) # autoterms filtered by human annotated items\n",
        "# DAutoTerms01KW_HMWE = filterDictByKWDict(DAutoTermsMWE, DScoresREKWquick)\n",
        "\n",
        "DAutoTerms01KW_H1w = filterDictByKWDict(DAutoTerms1w, DScoresKWLarge, Threshold = 15000) # autoterms filtered by human annotated items\n",
        "DAutoTerms01KW_HMWE = filterDictByKWDict(DAutoTermsMWE, DScoresKWLarge, Threshold = 15000)\n",
        "\n",
        "# Threshold = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhPylbo-hyZh"
      },
      "source": [
        "FOutAutoTermsFiltered1w = open('BGH0_s03FilteredTerms1w.txt', 'w')\n",
        "FOutAutoTermsFilteredMWE = open('BGH0_s03FilteredTermsMWE.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_7d9siEiMNa"
      },
      "source": [
        "printDictionary(DAutoTerms01KW_H1w, FOutAutoTermsFiltered1w)\n",
        "printDictionary(DAutoTerms01KW_HMWE, FOutAutoTermsFilteredMWE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L78EDrAFBHny",
        "outputId": "c9f1496b-7d42-4ac6-9e56-fa3389712d82"
      },
      "source": [
        "# print statistics\n",
        "print(len(DAutoTerms1w))\n",
        "print(len(DAutoTermsMWE))\n",
        "\n",
        "print(len(DScoresREKWquick))\n",
        "\n",
        "print(len(DAutoTerms01KW_H1w))\n",
        "print(len(DAutoTerms01KW_HMWE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "466669\n",
            "2434043\n",
            "1202\n",
            "230\n",
            "400440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42jGIJDyEZDr"
      },
      "source": [
        "# now we update the dictionaries -- a different configuration (do not run in first experiment...)\n",
        "# DAutoTerms1w -- was used for creating DAutoTerms01KW_H1w\n",
        "# DAutoTermsMWE -- was used for creating DAutoTerms01KW_HMWE\n",
        "DAutoTerms1w_copy = DAutoTerms1w\n",
        "DAutoTermsMWE_copy = DAutoTermsMWE\n",
        "del DAutoTerms1w\n",
        "del DAutoTermsMWE\n",
        "DAutoTerms1w = DAutoTerms01KW_H1w\n",
        "DAutoTermsMWE = DAutoTerms01KW_HMWE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JdMo0qPWx6"
      },
      "source": [
        "We will check:\n",
        "\n",
        "- how terminology extraction works for Precison and Recall (intersecting the Gold Standard and extracted terms); \n",
        "\n",
        "- how high is the rank of the terms in the extracted list, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTMBRsW0JdXA"
      },
      "source": [
        "# Stage04: preparing data for calculating precision and recall on the space of all possible MWEs, 1, 2, 3 words; (overlapping)\n",
        "# keeping only 1 version of the text (2 annotators annotated the same text twice to measure interannotator agreement)\n",
        "!cat BGH1_s00Astghik.txt BGH2_s00Hayk.txt BGH3_s00Tamar.txt >BGH0_s03GoldStandard1Version.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx_y4aAx6bbv"
      },
      "source": [
        "This function will be used for measuring P and R:\n",
        "one-way comparison of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQVspvOj51o"
      },
      "source": [
        "FInputGS1V = open('BGH0_s03GoldStandard1Version.txt', 'r')\n",
        "# tokenizing gold standard\n",
        "'''\n",
        "The idea is to tokenise the gold standard (from Stage 0), and to generate all possible MWEs for each string / pargraph\n",
        "    then we can test what is the coverage (non-overlapping) or precision (overlapping)\n",
        "    or: we create a dictionary of potential single and MWE strings and check what has been identified ?\n",
        "    or: comparing with 'oracle': known annotations are run as a point of comparision on the space; and we establish relations, i.e., the amount of over-generation\n",
        "\n",
        "    tasks: \n",
        "        4a: create the \"all possible strings\" space from gold standard text\n",
        "        4b: intersect 4a results with corpus list of extracted MWEs >> generate \"extracted from gold standard\" dictionary\n",
        "        4c: intersect human annotation in gold standard with 4a >> generate \"correct in gold standard\" dictionary\n",
        "        4d: intersect 4b and 4c, >> correctly extracted\n",
        "        4e: calculate 4d/4b = precision\n",
        "            calculate 4d/4c = recall\n",
        "\n",
        "'''\n",
        "# 3a: processing gold standard: tokenizing\n",
        "import re, os, sys\n",
        "LLParTokens = [] # List of paragraphs, each represented as a list of tokens\n",
        "for SLine in FInputGS1V:\n",
        "    # print(SLine)\n",
        "    SLine = SLine.strip() # implement this change\n",
        "\n",
        "    # remove annotation\n",
        "\n",
        "\n",
        "    SLine = re.sub('[<>]+', ' ', SLine)\n",
        "    SLine = re.sub(' +', ' ', SLine)\n",
        "\n",
        "\n",
        "\n",
        "    SLine = SLine.upper() # capitalize all words\n",
        "    # print(SLine)\n",
        "\n",
        "    LLine = re.split(' ', SLine) \n",
        "\n",
        "    # separate punctuation\n",
        "    LLine = re.findall(r\"[\\w']+|[.,!?;()\\-„“\\\"]\", SLine)\n",
        "    # SLine = re.sub(r'(,\\.;:\\-\\!\\?\\(\\)\\[\\]\\“\\\")', r' \\1 ', SLine)\n",
        "\n",
        "    LLine = list(filter(None, LLine))\n",
        "    LLParTokens.append(LLine)\n",
        "\n",
        "FInputGS1V.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMpKbSTsjZ-a",
        "outputId": "97097f2c-105a-4efb-a4b9-f3c74a96b884"
      },
      "source": [
        "print(str(LLParTokens[9]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['UND', 'DIE', 'RICHTER', 'AM', 'BUNDESGERICHTSHOF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygnQ9IZkz3R"
      },
      "source": [
        "# Stage 4A\n",
        "# generating candidate MWEs for cheking if / when they have been identified as terms\n",
        "# algorithm from Terminologieextraktion3 notebook\n",
        "# 4a: creating space of all possible overlapping MWEs in gold standard\n",
        "\n",
        "def tokens2candNGrams(LWords, N): # working with specific N-gram size, to keep number of candidates under control\n",
        "    '''\n",
        "    convert a list of tokens into a list of all possible MWEs (works for each paragraph)\n",
        "    '''\n",
        "    LLCandidates = [] # lists - tokenised results\n",
        "    # LSCandidates = [] # strings - joint results\n",
        "\n",
        "    for i in range(N): # for up to the required N-gram length\n",
        "\n",
        "        for IPosition in range(len(LWords) - i): # unigrams -- no change; bigrams: up to penultimate, etc.\n",
        "            LCandidate = LWords[IPosition : IPosition + i + 1]\n",
        "            # SCandidate = ' '.join(LCandidate)\n",
        "            LLCandidates.append(LCandidate)\n",
        "        # LSCandidates.append(SCandidate)\n",
        "    \n",
        "    return LLCandidates\n",
        "\n",
        "\n",
        "# LLCandidates = tokens2candNGrams(['this', 'is', 'a', 'test', 'of', 'the', 'function'], 4)\n",
        "# for L in LLCandidates:\n",
        "#    print(str(L))\n",
        "\n",
        "#    ''' # full version; now abandoned...\n",
        "#    for klen in range(len(LWords)): # lengths of candidate lists\n",
        "#        klength = klen+1 # true length: for 0 it is le = 1\n",
        "#        # print(f'klen:{klength};')\n",
        "#        for i in range(len(LWords) - klen): # positions where candidates start\n",
        "#            # print(f'i:{i};')\n",
        "#            LCandidate = LWords[i:i+klength]\n",
        "#            SCandidate = ' '.join(LCandidate)\n",
        "#            LLCandidates.append(LCandidate)\n",
        "#            LSCandidates.append(SCandidate)\n",
        "#        \n",
        "#    return LLCandidates, LSCandidates\n",
        "#    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8NvSEc-dbE8"
      },
      "source": [
        "DA_1W = {} # dictionary of 1-word candidates from the gold standard text (to be tested)\n",
        "DA_2W = {} # dictionary of 2-word candidates from the gold standard text (to be tested)\n",
        "DA_3W = {} # dictionary of 3-word candidates from the gold standard text (to be tested)\n",
        "DA_MWE = {} # dictionary of MWE candidates from the gold standard text (to be tested)\n",
        "\n",
        "for LTokens in LLParTokens: # for each paragraph\n",
        "    LLCandidates = tokens2candNGrams(LTokens, 4)\n",
        "    # print(str(LLCandidates))\n",
        "    for LCandidate in LLCandidates:\n",
        "        SCandidate = ' '.join(LCandidate)\n",
        "        if len(LCandidate) > 3:\n",
        "            try:\n",
        "                DA_MWE[SCandidate] += 1\n",
        "            except:\n",
        "                DA_MWE[SCandidate] = 1  \n",
        "        elif len(LCandidate) > 2:\n",
        "            try:\n",
        "                DA_3W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_3W[SCandidate] = 1  \n",
        "        elif len(LCandidate) > 1:\n",
        "            try:\n",
        "                DA_2W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_2W[SCandidate] = 1  \n",
        "        else:\n",
        "            try:\n",
        "                DA_1W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_1W[SCandidate] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-3xaIslkrlm"
      },
      "source": [
        "FOutputA1w = open('BGH0_s04A_1w_res.txt', 'w')\n",
        "FOutputA2w = open('BGH0_s04A_2w_res.txt', 'w')\n",
        "FOutputA3w = open('BGH0_s04A_3w_res.txt', 'w')\n",
        "FOutputAMWE = open('BGH0_s04A_MWE_res.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnQeFPXGg_3j"
      },
      "source": [
        "printDictionary(DA_1W, FOutputA1w, Rev = True)\n",
        "printDictionary(DA_2W, FOutputA2w, Rev = True)\n",
        "printDictionary(DA_3W, FOutputA3w, Rev = True)\n",
        "printDictionary(DA_MWE, FOutputAMWE, Rev = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvK9d9MHUrh_"
      },
      "source": [
        "W1A = len(DA_1W)\n",
        "W2A = len(DA_2W)\n",
        "W3A = len(DA_3W)\n",
        "W4A = len(DA_MWE)\n",
        "# FOutResults1.write('NoKW\\t') # which run\n",
        "# FOutResults1.write(f'NoKW\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "# FOutResults2.write('Run\\tW1P\\tW1R\\tW2P\\tW2R\\tW3P\\tW3R\\tW4P\\tW4R\\n') # only precision and recall figures\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75ahKJ2Tr8Q"
      },
      "source": [
        "# Stage 4B \n",
        "# creating output files for B\n",
        "FOutputFOUND1wBinA = open('BGH0_s04BinA1w_resY.txt', 'w')\n",
        "FOutputMISSING1wBinA = open('BGH0_s04BinA1w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUND2wBinA = open('BGH0_s04BinA2w_resY.txt', 'w')\n",
        "FOutputMISSING2wBinA = open('BGH0_s04BinA2w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUND3wBinA = open('BGH0_s04BinA3w_resY.txt', 'w')\n",
        "FOutputMISSING3wBinA = open('BGH0_s04BinA3w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUNDMWEsBinA = open('BGH0_s04BinAMWE_resY.txt', 'w')\n",
        "FOutputMISSINGMWEsBinA = open('BGH0_s04BinAMWE_resN.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drezLRNp6VUi",
        "outputId": "35edb27c-9e40-4fc5-ddc8-d78f23794ba9"
      },
      "source": [
        "# Stage 4B preparing B-set for calculating performance\n",
        "AFound1wBinA, AAverageFoundRanks1wBinA, DB_1W = countIntersectDictionaries(DA_1W, DAutoTerms1w, FOutputFOUND1wBinA, FOutputMISSING1wBinA, SortBy = 0, Rev = False)\n",
        "AFound2wBinA, AAverageFoundRanks2wBinA, DB_2W = countIntersectDictionaries(DA_2W, DAutoTermsMWE, FOutputFOUND2wBinA, FOutputMISSING2wBinA, SortBy = 0, Rev = False)\n",
        "AFound3wBinA, AAverageFoundRanks3wBinA, DB_3W = countIntersectDictionaries(DA_3W, DAutoTermsMWE, FOutputFOUND3wBinA, FOutputMISSING3wBinA, SortBy = 0, Rev = False)\n",
        "AFoundMWEBinA, AAverageFoundRanksMWEBinA, DB_MWE = countIntersectDictionaries(DA_MWE, DAutoTermsMWE, FOutputFOUNDMWEsBinA, FOutputMISSINGMWEsBinA, SortBy = 0, Rev = False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total len of Gold Standard: 16555\n",
            "Found: 191\n",
            "Missing: 16364\n",
            "Found2LenGS: 0.011537299909392933\n",
            "Ave Found Ranks: 59317.04586387438 \n",
            "\n",
            "Total len of Gold Standard: 82886\n",
            "Found: 1245\n",
            "Missing: 81641\n",
            "Found2LenGS: 0.015020630745843689\n",
            "Ave Found Ranks: 112791.06759036117 \n",
            "\n",
            "Total len of Gold Standard: 138283\n",
            "Found: 515\n",
            "Missing: 137768\n",
            "Found2LenGS: 0.003724246653601672\n",
            "Ave Found Ranks: 88812.86860194182 \n",
            "\n",
            "Total len of Gold Standard: 162112\n",
            "Found: 106\n",
            "Missing: 162006\n",
            "Found2LenGS: 0.0006538689301223845\n",
            "Ave Found Ranks: 103863.23962264153 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PsIc7OFbnzF"
      },
      "source": [
        "# FOutResults1.write(f'NoKW\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "W1B = len(DB_1W)\n",
        "W2B = len(DB_2W)\n",
        "W3B = len(DB_3W)\n",
        "W4B = len(DB_MWE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3cHf49UgH6X"
      },
      "source": [
        "# 3c: creating output files for C (normally should be 100% overlap, this is just a check of the general approach)\n",
        "FOutputFOUND1wCinA = open('BGH0_s04CinA1w_resY.txt', 'w')\n",
        "FOutputMISSING1wCinA = open('BGH0_s04CinA1w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUND2wCinA = open('BGH0_s04CinA2w_resY.txt', 'w')\n",
        "FOutputMISSING2wCinA = open('BGH0_s04CinA2w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUND3wCinA = open('BGH0_s04CinA3w_resY.txt', 'w')\n",
        "FOutputMISSING3wCinA = open('BGH0_s04CinA3w_resN.txt', 'w')\n",
        "\n",
        "FOutputFOUNDMWEsCinA = open('BGH0_s04CinAMWE_resY.txt', 'w')\n",
        "FOutputMISSINGMWEsCinA = open('BGH0_s04CinAMWE_resN.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ikaZTXHhn-1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY4c2ZdTgo9J",
        "outputId": "ff2975f3-7749-4dbf-9421-c49b523cbe1f"
      },
      "source": [
        "# intersecting set A (all possible N-grams in gold standard) with the annotion from the gold standard\n",
        "AFound1wCinA, AAverageFoundRanks1wCinA, DC_1W = countIntersectDictionaries(DA_1W, DGS1w, FOutputFOUND1wCinA, FOutputMISSING1wCinA, SortBy = 0, Rev = False)\n",
        "AFound2wCinA, AAverageFoundRanks1wCinA, DC_2W = countIntersectDictionaries(DA_2W, DGS2w, FOutputFOUND2wCinA, FOutputMISSING2wCinA, SortBy = 0, Rev = False)\n",
        "AFound3wCinA, AAverageFoundRanks1wCinA, DC_3W = countIntersectDictionaries(DA_3W, DGS3w, FOutputFOUND3wCinA, FOutputMISSING3wCinA, SortBy = 0, Rev = False)\n",
        "AFoundMWECinA, AAverageFoundRanksMWECinA, DC_MWE = countIntersectDictionaries(DA_MWE, DGSMWE, FOutputFOUNDMWEsCinA, FOutputMISSINGMWEsCinA, SortBy = 0, Rev = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total len of Gold Standard: 16555\n",
            "Found: 2959\n",
            "Missing: 13596\n",
            "Found2LenGS: 0.1787375415282392\n",
            "Ave Found Ranks: 5.422440013518081 \n",
            "\n",
            "Total len of Gold Standard: 82886\n",
            "Found: 266\n",
            "Missing: 82620\n",
            "Found2LenGS: 0.003209227131240499\n",
            "Ave Found Ranks: 1.4548872180451127 \n",
            "\n",
            "Total len of Gold Standard: 138283\n",
            "Found: 133\n",
            "Missing: 138150\n",
            "Found2LenGS: 0.000961795737726257\n",
            "Ave Found Ranks: 2.691729323308271 \n",
            "\n",
            "Total len of Gold Standard: 162112\n",
            "Found: 16\n",
            "Missing: 162096\n",
            "Found2LenGS: 9.869719699960521e-05\n",
            "Ave Found Ranks: 1.125 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O50ZqyGlb7fz"
      },
      "source": [
        "# FOutResults1.write(f'NoKW\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "W1C = len(DC_1W)\n",
        "W2C = len(DC_2W)\n",
        "W3C = len(DC_3W)\n",
        "W4C = len(DC_MWE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHUq3Qr4qVnf"
      },
      "source": [
        "# Stage 5: precision and recall calculations\n",
        "# precision\n",
        "FOutputFOUND1wDinB_precision = open('BGH0_s05DinB1w_resY_precision.txt', 'w')\n",
        "FOutputMISSING1wDinB_precision = open('BGH0_s05DinB1w_resN_precision.txt', 'w')\n",
        "\n",
        "FOutputFOUND2wDinB_precision = open('BGH0_s05DinB2w_resY_precision.txt', 'w')\n",
        "FOutputMISSING2wDinB_precision = open('BGH0_s05DinB2w_resN_precision.txt', 'w')\n",
        "\n",
        "FOutputFOUND3wDinB_precision = open('BGH0_s05DinB3w_resY_precision.txt', 'w')\n",
        "FOutputMISSING3wDinB_precision = open('BGH0_s05DinB3w_resN_precision.txt', 'w')\n",
        "\n",
        "FOutputFOUNDMWEsDinB_precision = open('BGH0_s05DinBMWE_resY_precision.txt', 'w')\n",
        "FOutputMISSINGMWEsDinB_precision = open('BGH0_s05DinBMWE_resN_precision.txt', 'w')\n",
        "\n",
        "\n",
        "# recall\n",
        "FOutputFOUND1wDinC_recall = open('BGH0_s05DinC1w_resY_recall.txt', 'w')\n",
        "FOutputMISSING1wDinC_recall = open('BGH0_s05DinC1w_resN_recall.txt', 'w')\n",
        "\n",
        "FOutputFOUND2wDinC_recall = open('BGH0_s05DinC2w_resY_recall.txt', 'w')\n",
        "FOutputMISSING2wDinC_recall = open('BGH0_s05DinC2w_resN_recall.txt', 'w')\n",
        "\n",
        "FOutputFOUND3wDinC_recall = open('BGH0_s05DinC3w_resY_recall.txt', 'w')\n",
        "FOutputMISSING3wDinC_recall = open('BGH0_s05DinC3w_resN_recall.txt', 'w')\n",
        "\n",
        "FOutputFOUNDMWEsDinC_recall = open('BGH0_s05DinCMWE_resY_recall.txt', 'w')\n",
        "FOutputMISSINGMWEsDinC_recall = open('BGH0_s05DinCMWE_resN_recall.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4F7wd_psVeD",
        "outputId": "647fd4da-452b-400a-c94c-26359d82b99e"
      },
      "source": [
        "AFound1wDinB, AAverageFoundRanks1wDinB, DD_1Wp = countIntersectDictionaries(DB_1W, DC_1W, FOutputFOUND1wDinB_precision, FOutputMISSING1wDinB_precision, SortBy = 0, Rev = False)\n",
        "AFound2wDinB, AAverageFoundRanks2wDinB, DD_2Wp = countIntersectDictionaries(DB_2W, DC_2W, FOutputFOUND2wDinB_precision, FOutputMISSING2wDinB_precision, SortBy = 0, Rev = False)\n",
        "AFound3wDinB, AAverageFoundRanks3wDinB, DD_3Wp = countIntersectDictionaries(DB_3W, DC_3W, FOutputFOUND3wDinB_precision, FOutputMISSING3wDinB_precision, SortBy = 0, Rev = False)\n",
        "AFoundMWEDinB, AAverageFoundRanksMWEDinB, DD_MWEp = countIntersectDictionaries(DB_MWE, DC_MWE, FOutputFOUNDMWEsDinB_precision, FOutputMISSINGMWEsDinB_precision, SortBy = 0, Rev = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total len of Gold Standard: 191\n",
            "Found: 123\n",
            "Missing: 68\n",
            "Found2LenGS: 0.643979057591623\n",
            "Ave Found Ranks: 51.1219512195122 \n",
            "\n",
            "Total len of Gold Standard: 1245\n",
            "Found: 44\n",
            "Missing: 1201\n",
            "Found2LenGS: 0.035341365461847386\n",
            "Ave Found Ranks: 2.159090909090909 \n",
            "\n",
            "Total len of Gold Standard: 515\n",
            "Found: 2\n",
            "Missing: 513\n",
            "Found2LenGS: 0.003883495145631068\n",
            "Ave Found Ranks: 1.5 \n",
            "\n",
            "Total len of Gold Standard: 106\n",
            "Found: 0\n",
            "Missing: 106\n",
            "Found2LenGS: 0.0\n",
            "Ave Found Ranks: 0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1UTHxbltnxT",
        "outputId": "924879ba-4e61-47aa-82e5-c50dad669549"
      },
      "source": [
        "AFound1wDinC, AAverageFoundRanks1wDinC, DD_1Wr = countIntersectDictionaries(DC_1W, DB_1W, FOutputFOUND1wDinC_recall, FOutputMISSING1wDinC_recall, SortBy = 0, Rev = False)\n",
        "AFound2wDinC, AAverageFoundRanks2wDinC, DD_2Wr = countIntersectDictionaries(DC_2W, DB_2W, FOutputFOUND2wDinC_recall, FOutputMISSING2wDinC_recall, SortBy = 0, Rev = False)\n",
        "AFound3wDinC, AAverageFoundRanks3wDinC, DD_3Wr = countIntersectDictionaries(DC_3W, DB_3W, FOutputFOUND3wDinC_recall, FOutputMISSING3wDinC_recall, SortBy = 0, Rev = False)\n",
        "AFoundMWEDinC, AAverageFoundRanksMWEDinC, DD_MWEr = countIntersectDictionaries(DC_MWE, DB_MWE, FOutputFOUNDMWEsDinC_recall, FOutputMISSINGMWEsDinC_recall, SortBy = 0, Rev = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total len of Gold Standard: 2959\n",
            "Found: 123\n",
            "Missing: 2836\n",
            "Found2LenGS: 0.04156809733017911\n",
            "Ave Found Ranks: 67727.57097560978 \n",
            "\n",
            "Total len of Gold Standard: 266\n",
            "Found: 44\n",
            "Missing: 222\n",
            "Found2LenGS: 0.16541353383458646\n",
            "Ave Found Ranks: 122529.00090909093 \n",
            "\n",
            "Total len of Gold Standard: 133\n",
            "Found: 2\n",
            "Missing: 131\n",
            "Found2LenGS: 0.015037593984962405\n",
            "Ave Found Ranks: 19863.885 \n",
            "\n",
            "Total len of Gold Standard: 16\n",
            "Found: 0\n",
            "Missing: 16\n",
            "Found2LenGS: 0.0\n",
            "Ave Found Ranks: 0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gJxnq3cRIr"
      },
      "source": [
        "# FOutResults1.write(f'NoKW\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "W1D = len(DD_1Wr)\n",
        "W2D = len(DD_2Wr)\n",
        "W3D = len(DD_3Wr)\n",
        "W4D = len(DD_MWEr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q-YJQxPmqel"
      },
      "source": [
        "W1P = W1D / W1B\n",
        "W1R = W1D / W1C\n",
        "\n",
        "W2P = W2D / W2B\n",
        "W2R = W2D / W2C\n",
        "\n",
        "W3P = W3D / W3B\n",
        "W3R = W3D / W3C\n",
        "\n",
        "W4P = W4D / W4B\n",
        "W4R = W4D / W4C\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxsKhHYZcxni"
      },
      "source": [
        "Run = 'KW_T15000' # NoKW = default, only frequency dictionary\n",
        "# FOutResults1.write('NoKW\\t') # which run\n",
        "# FOutResults2.write('NoKW\\t') # which run\n",
        "FOutResults1.write(f'{Run}\\t{W1A}\\t{W1B}\\t{W1C}\\t{W1D}\\t{W1P}\\t{W1R}\\t{W2A}\\t{W2B}\\t{W2C}\\t{W2D}\\t{W2P}\\t{W2R}\\t{W3A}\\t{W3B}\\t{W3C}\\t{W3D}\\t{W3P}\\t{W3R}\\t{W4A}\\t{W4B}\\t{W4C}\\t{W4D}\\t{W4P}\\t{W4R}\\n')\n",
        "FOutResults2.write(f'{Run}\\t{W1P}\\t{W1R}\\t{W2P}\\t{W2R}\\t{W3P}\\t{W3R}\\t{W4P}\\t{W4R}\\n') # only precision and recall figures\n",
        "FOutResults1.flush()\n",
        "FOutResults2.flush()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}