{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion9EvaluationKeyWordsV03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYMUrOcptdcsbmwRsECPsS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion9EvaluationKeyWordsV03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Tp03sIjFRh"
      },
      "source": [
        "# Workflow\n",
        "\t1. gold standard terms \n",
        "\t\t> annotation\n",
        "\t\t> extracting annotation:\n",
        "\t\t> selecting lemma field;\n",
        "\t\t> filtering by specific patterns\n",
        "\n",
        "\n",
        "\n",
        "\t2. legal corpus > extraction of terms\n",
        "\t\t> sequences;\n",
        "\t\t> sub-sequences and patterns in sequences\n",
        "\t\t> filtering for what has been extracted >> the same way as from annotation\n",
        "\t\t> selecting the lemma field\n",
        "\n",
        "\n",
        "\t3. p/r measure -- baseline (max recall)\n",
        "\n",
        "\t4. filtering: \n",
        "\t\tkeyness;\n",
        "\t\tassociation measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj6O1sU2kqTE"
      },
      "source": [
        "## Part 1 - Terminology extraction\n",
        "\n",
        "This part of the workflow uses large files and may run for up to 20 minutes, needs to be run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-sZk6UMjtLo",
        "outputId": "5d0e593f-8cf6-4e8b-be9d-854ea3bc3718"
      },
      "source": [
        "# Stage 1: preparing terminology extraction workflow (~ 1 min, but may run longer)\n",
        "# 23.10.2021 part\n",
        "# German legal corpus, lemmatized in a zip archive (archive = 641 MB in zip archive)\n",
        "!wget https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
        "# renaming file ()\n",
        "!mv index.html?dl=1 BGH-utf8-lem.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 15:28:25--  https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/e749512e-206e-4fad-9bc8-ccdd7bd1691c/output-utf8-lem.zip [following]\n",
            "--2021-11-04 15:28:26--  https://heibox.uni-heidelberg.de/seafhttp/files/e749512e-206e-4fad-9bc8-ccdd7bd1691c/output-utf8-lem.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 673144348 (642M) [application/zip]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 641.96M  14.9MB/s    in 43s     \n",
            "\n",
            "2021-11-04 15:29:09 (14.9 MB/s) - ‘index.html?dl=1’ saved [673144348/673144348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYrGkFzlCT9",
        "outputId": "eca588f1-cf06-4d90-a966-ce97fe540442"
      },
      "source": [
        "# extraction ~ 1 min\n",
        "!unzip BGH-utf8-lem.zip\n",
        "!rm BGH-utf8-lem.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BGH-utf8-lem.zip\n",
            "  inflating: output-utf8-lem.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q-ueX-olCIA",
        "outputId": "5a3018b5-6a45-461c-cbd7-fd98025a78e1"
      },
      "source": [
        "!head --lines=10 output-utf8-lem.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<doc id=\"t1000001\">\n",
            "Nachschlagewerk\tNN\tNachschlagewerk\n",
            ":\t$.\t:\n",
            "ja\tADV\tja\n",
            "BGHSt\tVVFIN\tBGHSt\n",
            ":\t$.\t:\n",
            "nein\tPTKANT\tnein\n",
            "Veröffentlichung\tNN\tVeröffentlichung\n",
            ":\t$.\t:\n",
            "ja\tADV\tja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPk7DMolniZ"
      },
      "source": [
        "!mv output-utf8-lem.txt BGHlem.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJC2x26NlvNn"
      },
      "source": [
        "!head --lines=100000 BGHlem.txt >BGHlem1k.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t09pTHtl4jr",
        "outputId": "b4f48859-00a1-42b3-987e-bf7ead975a6d"
      },
      "source": [
        "# OPTIONAL -- just to know how big is our corpus, on a long corpus it can take a lot of time...\n",
        "# word counts: BGHlem.txt should be ~221 M lines long (vert lemmatized format; so one line is one word)\n",
        "!wc BGHlem1k.txt\n",
        "!wc BGHlem.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100000  299903 1673786 BGHlem1k.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x08Plvp9F_mA"
      },
      "source": [
        "# Stage 0: Some useful read/write and convert functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsmKBqMgGWLf"
      },
      "source": [
        "# import useful libraries, files\n",
        "import re, os, sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ9Hkx0OGfCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298882d5-64dc-48e2-9e76-8b825379428e"
      },
      "source": [
        "# file for recording results of different configurations\n",
        "# run this only once\n",
        "!rm AllTermExtractionResultsV01.txt\n",
        "!rm AllTermExtractionResultsV02.txt\n",
        "\n",
        "FOutResults1 = open('AllTermExtractionResultsV01.txt', 'a')\n",
        "FOutResults2 = open('AllTermExtractionResultsV02.txt', 'a')\n",
        "FOutResults1.write('Run\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "FOutResults2.write('Run\\tW1P\\tW1R\\tW2P\\tW2R\\tW3P\\tW3R\\tW4P\\tW4R\\n') # only precision and recall figures\n",
        "FOutResults1.flush()\n",
        "FOutResults2.flush()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'AllTermExtractionResultsV01.txt': No such file or directory\n",
            "rm: cannot remove 'AllTermExtractionResultsV02.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqvEq6IGGfv1"
      },
      "source": [
        "## to modify if necessary; however, we try to keep the code standard and parametrize as much as possible\n",
        "\n",
        "# useful functions\n",
        "# a useful function for recording / visualising current stage of dictionaries\n",
        "def printDictionary(DictionaryFrq, FOut, K = 1, Rev = True): # printing a dictionary: by values or alphabetically\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[K], reverse=Rev):\n",
        "        FOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    FOut.flush()\n",
        "    return\n",
        "\n",
        "# another useful function to just read and return a 2-field dictionary, eg., frequency or keyness\n",
        "def readDictionary(FIN, SkipComments = True, Caps=False):\n",
        "    DScoresLarge = {} # keywords - scores\n",
        "    for Line in FIN:\n",
        "        if SkipComments and re.match('#', Line): \n",
        "            continue\n",
        "        Line = Line.strip()\n",
        "        if Caps: \n",
        "            Line = Line.upper() # convert to upper case\n",
        "        LFieldsKW = re.split('\\t', Line)\n",
        "        SWord = LFieldsKW[0]\n",
        "        AKScore = float(LFieldsKW[1])\n",
        "        DScoresLarge[SWord] = AKScore   \n",
        "    return DScoresLarge\n",
        "\n",
        "# another possibly useful function: convert dictionary values to ranks (frequency, keyness weights, etc.)\n",
        "# for understanding how far down the list the item has been found...\n",
        "# currently not used ... \n",
        "def rankDict(DIN):\n",
        "    '''\n",
        "    reading a frequency dictionary from a file\n",
        "    '''\n",
        "    DTermRanks = {}\n",
        "    i = 0\n",
        "    IRank = 0\n",
        "    IPrevFrq = 0\n",
        "    SumRanks = 0\n",
        "    for SKey, Frq in DIN.items():\n",
        "        # if re.match('#', SKey): continue # skipping comments\n",
        "        i+=1\n",
        "        if IPrevFrq != Frq: IRank = i # rank is the number of the highest ranking element of the same frequency group\n",
        "        IPrevFrq = Frq\n",
        "        \n",
        "        DTermRanks[SKey] = IRank\n",
        "        SumRanks += IRank\n",
        "\n",
        "    AAveRank = SumRanks / i\n",
        "    print(f'MaxRank = {IRank}\\nAve Rank = {AAveRank}\\n')\n",
        "    return DTermRanks, AAveRank\n",
        "\n",
        "\n",
        "# Main evaluation function\n",
        "# One-directional comparision of dictionaries\n",
        "# one-directional comparison of two dictionaries; arguments: DGoldStandard (smaller) DTest (larger), file: GS items found in DTest; GS items missing from DText...\n",
        "# usually testing: smaller vs. bigger dictionaries\n",
        "def countIntersectDictionaries(DGS, DTest, FOutputPrecFOUND, FOutputPrecMISSING, SortBy = 0, Rev = False):\n",
        "    '''\n",
        "    general function: intersect dictionaries, return new intersection dictionaries, record \"in\" and \"out\" expressions\n",
        "    \n",
        "    3b: intersecting All possible MWEs in GS list with the \"Extracted\" list\n",
        "    DA (smaller and going over each element) with D1W / DMWE lists \n",
        "    '''\n",
        "\n",
        "    print('Total len of Gold Standard: ' + str(len(DGS.items())))\n",
        "    IFound = 0\n",
        "    IMissing = 0\n",
        "    SumFoundRanks = 0\n",
        "    DFound = {} # intersection dictionary\n",
        "\n",
        "    for Word, Frq in sorted(DGS.items(),  key=lambda x: x[SortBy], reverse=Rev):\n",
        "        if Word in DTest:\n",
        "            IFound += 1\n",
        "            try: # normally will not fire: if this word already exists with some rank, calculate the average of a new and old rank\n",
        "                r0 = DFound[Word]\n",
        "                r1 = DTest[Word]\n",
        "                r = (r0+r1)/2\n",
        "                DFound[Word] = r\n",
        "                print('r?')\n",
        "            except: # normal route: find the rank of the word in the dictionary\n",
        "                DFound[Word] = DTest[Word]\n",
        "\n",
        "            SumFoundRanks += DTest[Word] # add rank, to calculate average\n",
        "            try: FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + str(DFound[Word]) + '\\n') # record/calculate average rank, etc.\n",
        "            except: \n",
        "                FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "                print(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "        else:\n",
        "            IMissing += 1\n",
        "            FOutputPrecMISSING.write(Word + '\\t' + str(Frq) + '\\n') # record/calculate average rank, etc.\n",
        "\n",
        "    \n",
        "    # print(f'Found: {IFound}')\n",
        "    # print(f'Missing: {IMissing}')\n",
        "    try: ACoverage = IFound / len(DGS.items())\n",
        "    except: ACoverage = 0\n",
        "    # print(f'Found2LenGS: {ACoverage}')\n",
        "    try: AAverageFoundRanks = SumFoundRanks / IFound\n",
        "    except: AAverageFoundRanks = 0\n",
        "    # print(f'Ave Found Ranks: {AAverageFoundRanks} \\n')\n",
        "\n",
        "    print(f'Found: {IFound} ; Missing: {IMissing} ; AveRank: {AAverageFoundRanks} ; ACoverage: {ACoverage} ')\n",
        "    FOutputPrecFOUND.flush()\n",
        "    FOutputPrecMISSING.flush()\n",
        "\n",
        "    return ACoverage, AAverageFoundRanks, DFound\n",
        "\n",
        "\n",
        "\n",
        "# extracting annotated terms from the gold standard in xml format\n",
        "def vertCollectAnnotation(FInVert, SXmlTag, Caps = False):\n",
        "    L3AnnotatedSegs = []\n",
        "    L2Seg = [] # a list of the current segment -- eash string is added \n",
        "    BInTerm = False # boolean flag: inside / outside term\n",
        "    RTagOpen = re.compile('<' + SXmlTag + '>')\n",
        "    RTagClose = re.compile('</' + SXmlTag + '>')\n",
        "    for SLine in FInVert:\n",
        "        SLine = SLine.strip()\n",
        "        if Caps:\n",
        "            SLine = SLine.upper()\n",
        "        if re.match(RTagOpen, SLine):\n",
        "            BInTerm = True\n",
        "        elif re.match(RTagClose, SLine):\n",
        "            BInTerm = False\n",
        "            L3AnnotatedSegs.append(L2Seg)\n",
        "            L2Seg = []\n",
        "        else:\n",
        "            if BInTerm == True:\n",
        "                LFields = re.split('\\t', SLine)\n",
        "                L2Seg.append(LFields)\n",
        "\n",
        "    return L3AnnotatedSegs\n",
        "\n",
        "\n",
        "# converting in-text annotation (e.g., in the bracket form) into proper XML format\n",
        "def convertBrecket2Xml(FInAnnot, FOutAnnot, RInOpen, RInClose, SOutOpen, SOutClose):\n",
        "    RCOpen = re.compile(RInOpen)\n",
        "    RCClose = re.compile(RInClose)\n",
        "    for SLine in FInAnnot:\n",
        "        SLine.strip()\n",
        "        SLine = re.sub(RCOpen, SOutOpen, SLine)\n",
        "        SLine = re.sub(RCClose, SOutClose, SLine)\n",
        "\n",
        "        FOutAnnot.write(SLine + '\\n')\n",
        "    FOutAnnot.flush()\n",
        "    return\n",
        "\n",
        "# a service function for creating a dictionary of field values\n",
        "# is used for creating a dictionary of PoS patterns\n",
        "# can destructively change the list, if Normalize = 2 (changing PoS codes as specified inside the function)\n",
        "def createDictOfPatterns(L3AnnotatedSegs, IFieldN, Normalize = 0):\n",
        "    '''\n",
        "    take the list of annotated terms and return a dictionary of MWEs\n",
        "    Normalize = 0 : do not normalize;\n",
        "              = 1: normalize, but do not change the original list\n",
        "              = 2: normalize and change the original list\n",
        "    '''\n",
        "    DPatternsFrq = {} # returned dictionary of PoS patterns, etc.\n",
        "    for L2TermFlds in L3AnnotatedSegs:\n",
        "        LFlds = [] # here we will collect the values of the selected field\n",
        "        for LWordFlds in L2TermFlds:\n",
        "            if Normalize < 2:\n",
        "                LWordFlds0 = [] # making a copy of the list, not to modify original if Normalize is specified, or is it ok to normalize\n",
        "                LWordFlds0.extend(LWordFlds)\n",
        "            elif Normalize == 2:\n",
        "                LWordFlds0 = LWordFlds # just use a reference to the same list\n",
        "            \n",
        "            if Normalize > 0:\n",
        "                try: PoS = LWordFlds0[IFieldN]\n",
        "                except: \n",
        "                    print('PoS not found')\n",
        "                    PoS = ''\n",
        "                if re.match('N', PoS): LWordFlds0[IFieldN] = 'N'\n",
        "                if re.match('ADJ', PoS): LWordFlds0[IFieldN] = 'ADJ'\n",
        "                if re.match('V', PoS): LWordFlds0[IFieldN] = 'V'\n",
        "            try: LFlds.append(LWordFlds0[IFieldN])\n",
        "            except: print('index Error')\n",
        "        if len(LFlds) > 0: SFlds = ' '.join(LFlds)\n",
        "        try: DPatternsFrq[SFlds] += 1\n",
        "        except: DPatternsFrq[SFlds] = 1\n",
        "\n",
        "    return DPatternsFrq\n",
        "\n",
        "\n",
        "\n",
        "# change case for elements of the list\n",
        "def changeCaseL3(L3Segs, LFlds2Caps = [0, 2], Mode='upper', StripB = True):\n",
        "    '''\n",
        "    Mode = upper -- to all caps;\n",
        "        = lower -- to lowercase;\n",
        "        = capitalize -- to sentence case;\n",
        "    '''\n",
        "    for L2Seg in L3Segs: # for each multiword term in the list of terms\n",
        "        for LWordFlds in L2Seg: # for each word in the list of words in the multiword term\n",
        "            for IFld in LFlds2Caps: # for each field that needs to change case\n",
        "                SWord = LWordFlds[IFld]\n",
        "                if StripB: SWord = SWord.strip('<>')\n",
        "                if Mode == 'upper': \n",
        "                    SWord = SWord.upper()\n",
        "                if Mode == 'lower':\n",
        "                    SWord = SWord.lower()\n",
        "                if Mode == 'capitalize':\n",
        "                    SWord = SWord.capitalize()\n",
        "                LWordFlds[IFld] = SWord\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "def readDictKWAnnotations(FInputKW, AKStatThreshold = 1, Caps=True):\n",
        "    '''\n",
        "    reading a keyword file, returning a dictionary of keywords / not keywords\n",
        "    AKStatThreshold = 1 (only sure keywords)\n",
        "                    = 0.5 (unsure keywords)\n",
        "                    = 0 (all annotated keywords)\n",
        "    '''\n",
        "    DScoresKW = {} # keywords - scores\n",
        "    DScoresNK = {} # non-keywords\n",
        "    DStatKW = {} # status: key/non-key-word\n",
        "    for Line in FInputKW:\n",
        "        LFieldsKW = re.split('\\t', Line) # add: strip()\n",
        "        SWord = LFieldsKW[1]\n",
        "        if Caps: SWord = SWord.upper()\n",
        "        AKScore = float(LFieldsKW[2])\n",
        "        AKStat = float(LFieldsKW[3])\n",
        "        DStatKW[SWord] = AKStat\n",
        "        if AKStat >= AKStatThreshold: # change value to 0.5 if we need to restrict to 'sure' terms only (value 1)\n",
        "            DScoresKW[SWord] = AKScore\n",
        "        else:\n",
        "            DScoresNK[SWord] = AKScore\n",
        "    return DScoresKW, DScoresNK\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk7FkvbXvQHE"
      },
      "source": [
        "# main functions -- will be used also in extraction process\n",
        "# L3AnnotatedSegs[10]\n",
        "# FInput = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "# for statistical purposes - separately single and multiword terms\n",
        "\n",
        "# function(s) for selecting patterns in a list of candidates; \n",
        "# these can be either positive patterns, or, if postive are not specified, then negative pattenrs (start, edge or end restrictions on PoS codes)\n",
        "\n",
        "class ContinueI(Exception):\n",
        "    pass\n",
        "\n",
        "continue_i = ContinueI()\n",
        "\n",
        "def comparePattern(L2TermFlds, LPattern, IFldN):\n",
        "    '''\n",
        "    compares if a pattern is found in the term field\n",
        "    '''\n",
        "    for k in range(len(LPattern)):\n",
        "        if re.match(LPattern[k], L2TermFlds[k][IFldN]): continue\n",
        "        else: return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None,  SplitLen = False, IFldNumber = 0):\n",
        "    '''\n",
        "    function: 1. selects terms which match specified POS pattern; 2. divides them into dictionaries according to length\n",
        "    the function can also visualise terms with a specific pos pattern, specified in L2Patterns, e.g., L2Patterns = [['N', '\\$']]\n",
        "\n",
        "    '''\n",
        "    DGS = {}\n",
        "    DGS1w = {} # dictionary of single words\n",
        "    DGS2w = {} # dictionary of 2-word expressions\n",
        "    DGS3w = {} # dictionary of 3-word expressions\n",
        "    DGS4w = {} # dictionary of other mwes\n",
        "    IGS = 0\n",
        "    IGS1w = 0 # number of annotated tokens of single words\n",
        "    IGS2w = 0\n",
        "    IGS3w = 0\n",
        "    IGS4w = 0 # number of annotated tokens of multiwords\n",
        "\n",
        "    if L2Patterns: # positive filter\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs: # for each multiword term, where words are represented as fields\n",
        "            ILenTerm = len(L2AnnotatedSeg)\n",
        "            for LPattern in L2Patterns:\n",
        "                if len(LPattern) == ILenTerm and comparePattern(L2AnnotatedSeg, LPattern, 1):\n",
        "                    LTerm = []\n",
        "                    for LTerm2Fields in L2AnnotatedSeg:\n",
        "                        LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                    STerm = ' '.join(LTerm)\n",
        "                    try: DGS[STerm] += 1\n",
        "                    except: DGS[STerm] = 1\n",
        "\n",
        "    else: # negative filter checking\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs:\n",
        "            if not L2AnnotatedSeg: continue\n",
        "            try: SEnd = L2AnnotatedSeg[-1][1]\n",
        "            except: print('index: L2AnnotatedSeg - end' + str(L2AnnotatedSeg))\n",
        "\n",
        "            try: SStart = L2AnnotatedSeg[0][1]\n",
        "            except: print('index: L2AnnotatedSeg - start' + str(L2AnnotatedSeg))\n",
        "            \n",
        "            try:\n",
        "                if LNoEdge: # PoS which cannot apper at the edge\n",
        "                    for SPoS in LNoEdge:\n",
        "                        if re.match(SPoS, SEnd) or re.match(SPoS, SStart):\n",
        "                            # print('edge: ' + SPoS + ' ' + SStart  + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                if LNoStart:\n",
        "                    for SPoS in LNoStart:\n",
        "                        if re.match(SPoS, SStart): \n",
        "                            # print('start: ' + SPoS + ' ' + SStart)\n",
        "                            raise continue_i\n",
        "                if L2NoEnd:\n",
        "                    for SPoS in L2NoEnd:\n",
        "                        if re.match(SPoS, SEnd): \n",
        "                            # print('end: ' + SPoS + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                LTerm = []\n",
        "                for LTerm2Fields in L2AnnotatedSeg:\n",
        "                    LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                STerm = ' '.join(LTerm)\n",
        "                try: DGS[STerm] += 1\n",
        "                except: DGS[STerm] = 1\n",
        "\n",
        "            except ContinueI: \n",
        "                continue\n",
        "\n",
        "    if SplitLen:\n",
        "        for GSTerm, Frq in DGS.items():\n",
        "            LGSTErms = re.split(' ', GSTerm)\n",
        "\n",
        "            if len(LGSTErms) > 3:\n",
        "                IGS4w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS4w[GSTerm] += Frq\n",
        "                except: DGS4w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 2:\n",
        "                IGS3w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS3w[GSTerm] += Frq\n",
        "                except: DGS3w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 1:\n",
        "                IGS2w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS2w[GSTerm] += Frq\n",
        "                except: DGS2w[GSTerm] = Frq\n",
        "            else:\n",
        "                IGS1w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS1w[GSTerm] +=Frq\n",
        "                except: DGS1w[GSTerm] = Frq\n",
        "\n",
        "\n",
        "    print(IGS1w, IGS2w, IGS3w, IGS4w, IGS)\n",
        "    print(len(DGS1w), len(DGS2w), len(DGS3w), len(DGS4w), len(DGS))\n",
        "    return DGS, DGS1w, DGS2w, DGS3w, DGS4w\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvzLyPZYBUi"
      },
      "source": [
        "# filter functions -- e.g., by keyness\n",
        "# here we will add / combine information about keyness...\n",
        "# to be implemented...\n",
        "# only allow those terms into the Auto dictionary, which have weights; replace frq by keyness weights (or sum)\n",
        "# function to be used on all dictionaries:\n",
        "\n",
        "def filterDictByKWDict(DAuto, DKeyness, Threshold = 1, Mode='prod', Req = 2):\n",
        "    '''\n",
        "    Mode = max: we take maximum value of keyness\n",
        "         = prod -- we take the product of keyness, \n",
        "    Req = 1: we require at least one word in to be in the keyness dictionary\n",
        "        = 2...N: we require at least 2, N words to be in the keyness dictionary (if there are as many in the list)\n",
        "    '''\n",
        "    DAutoFiltered = {}\n",
        "    for SAutoTerm, Frq in DAuto.items():\n",
        "        if Mode == 'max': AKeynessAll = 0\n",
        "        elif Mode == 'prod': AKeynessAll = 1\n",
        "        LAutoTermWs = re.split(' ', SAutoTerm)\n",
        "        ICountFound = 0\n",
        "        for STerm in LAutoTermWs:\n",
        "            if STerm in DKeyness:\n",
        "                ICountFound += 1\n",
        "                if Mode == 'max' and DKeyness[STerm] > AKeynessAll:\n",
        "                    AKeynessAll = DKeyness[STerm] # we take the maximum keyness\n",
        "                elif Mode == 'prod':\n",
        "                    AKeynessAll = AKeynessAll * DKeyness[STerm]\n",
        "        if len(LAutoTermWs) < Req: Req0 = len(LAutoTermWs)\n",
        "        else: Req0 = Req\n",
        "        if ICountFound < Req0: continue\n",
        "        if AKeynessAll > Threshold:\n",
        "            DAutoFiltered[SAutoTerm] = AKeynessAll\n",
        "\n",
        "    return DAutoFiltered\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbsgvYg6YcO"
      },
      "source": [
        "# dowloading and creating necessary files\n",
        "# term extraction will be integrated here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3RKlelR6jx"
      },
      "source": [
        "## preparing for TreeTagger processing (do not have to re-run again once completed)\n",
        "## Annotated Gold Standard\n",
        "# Stage 1: Preparing Gold standard: Reading / extracting information from gold standard: creating a list of annotated terms\n",
        "# set 1 (same text annotated by two annotators)\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Astghik.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Maia.txt\n",
        "\n",
        "# set 2 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Hayk.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Nino.txt\n",
        "\n",
        "# set 3 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
        "!mv index.html?dl=1 BGH3_s00Tamar.txt\n",
        "# !wget \n",
        "# !mv index.html?dl=1\n",
        "\n",
        "# one more will be added: Frau Khachatryan\n",
        "!cat BGH1_s00Astghik.txt BGH1_s00Maia.txt BGH2_s00Hayk.txt BGH2_s00Nino.txt BGH3_s00Tamar.txt >BGH0_s00GoldStandard.txt\n",
        "\n",
        "FInBGH0_s00GoldStandard = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "FOutBGH0_s00GoldStandard = open('BGH0_s00GoldStandard_xml.txt', 'w')\n",
        "\n",
        "convertBrecket2Xml(FInBGH0_s00GoldStandard, FOutBGH0_s00GoldStandard, '<<+', '>>+', '<TERM>', '</TERM>')\n",
        "# this result is pos-tagged and uploaded in the next step\n",
        "# command:\n",
        "# tree-tagger-de.sh /Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_xml.txt >/Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_LEM.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5RfGopzJZr"
      },
      "source": [
        "# Stage04: preparing data for calculating precision and recall on the space of all possible MWEs, 1, 2, 3 words; (overlapping)\n",
        "# keeping only 1 version of the text (2 annotators annotated the same text twice to measure interannotator agreement)\n",
        "!cat BGH1_s00Astghik.txt BGH2_s00Hayk.txt BGH3_s00Tamar.txt >BGH0_s03GoldStandard1Version.txt\n",
        "FInputGS1V = open('BGH0_s03GoldStandard1Version.txt', 'r')\n",
        "FOutputGS1V = open('BGH0_s04GoldStandard1Version_text.txt', 'w')\n",
        "\n",
        "# gold standard - 1 version:\n",
        "# we clean up the document, with only 1 copy of the text; then we run it locally through TreeTagger and then process here\n",
        "\n",
        "'''\n",
        "The idea is to tokenise the gold standard (from Stage 0), and to generate all possible MWEs for each string / pargraph\n",
        "    then we can test what is the coverage (non-overlapping) or precision (overlapping)\n",
        "    or: we create a dictionary of potential single and MWE strings and check what has been identified ?\n",
        "    or: comparing with 'oracle': known annotations are run as a point of comparision on the space; and we establish relations, i.e., the amount of over-generation\n",
        "\n",
        "    tasks: \n",
        "        4a: create the \"all possible strings\" space from gold standard text\n",
        "        4b: intersect 4a results with corpus list of extracted MWEs >> generate \"extracted from gold standard\" dictionary\n",
        "        4c: intersect human annotation in gold standard with 4a >> generate \"correct in gold standard\" dictionary\n",
        "        4d: intersect 4b and 4c, >> correctly extracted\n",
        "        4e: calculate 4d/4b = precision\n",
        "            calculate 4d/4c = recall\n",
        "\n",
        "'''\n",
        "# 3a: processing gold standard: tokenizing\n",
        "\n",
        "LLParTokens = [] # List of paragraphs, each represented as a list of tokens\n",
        "for SLine in FInputGS1V:\n",
        "    # print(SLine)\n",
        "    SLine = SLine.strip() # implement this change\n",
        "\n",
        "    # remove annotation\n",
        "\n",
        "\n",
        "    SLine = re.sub('[<>]+', ' ', SLine)\n",
        "    SLine = re.sub(' +', ' ', SLine)\n",
        "\n",
        "    FOutputGS1V.write(SLine + '\\n')\n",
        "\n",
        "FInputGS1V.close()\n",
        "FOutputGS1V.flush()\n",
        "FOutputGS1V.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIf30Nt_djvd"
      },
      "source": [
        "!echo file1 BGH0_s03GoldStandard1Version.txt\n",
        "!head --lines=10 BGH0_s03GoldStandard1Version.txt\n",
        "!echo file2 BGH0_s04GoldStandard1Version_text.txt\n",
        "!head --lines=10 BGH0_s04GoldStandard1Version_text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htvZjKpX2Dku",
        "outputId": "aaf5c8ff-01c4-4806-d762-79874fd80bf5"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/d39b640f70504b4fb86a/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s04GoldStandard1Version_LEM.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 13:47:52--  https://heibox.uni-heidelberg.de/f/d39b640f70504b4fb86a/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/7e8bfc07-59f1-4728-9dec-fa1e29927f60/BGH0_s04GoldStandard1Version_LEM.txt [following]\n",
            "--2021-11-04 13:47:53--  https://heibox.uni-heidelberg.de/seafhttp/files/7e8bfc07-59f1-4728-9dec-fa1e29927f60/BGH0_s04GoldStandard1Version_LEM.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3297455 (3.1M) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]   3.14M  3.29MB/s    in 1.0s    \n",
            "\n",
            "2021-11-04 13:47:54 (3.29 MB/s) - ‘index.html?dl=1’ saved [3297455/3297455]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLmxjIO1HTD"
      },
      "source": [
        "here we pass the output file to TreeTagger and download the result, the statistics is:\n",
        "\n",
        "bash-3.2$ wc BGH0_s04GoldStandard1Version_LEM.txt \n",
        "\n",
        "BGH0_s04GoldStandard1Version_text.txt\n",
        "\n",
        "    198146  594438 3297455 BGH0_s04GoldStandard1Version_LEM.txt\n",
        "    3469  169054 1215715 BGH0_s04GoldStandard1Version_text.txt\n",
        "    201615  763492 4335398 total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaeRl0y_4nP9"
      },
      "source": [
        "!head --lines=10 BGH0_s04GoldStandard1Version_LEM.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHL3gREljDnQ"
      },
      "source": [
        "# downloading PoS-tagged files after TT processing\n",
        "# !wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# Vahram's file, manually checked (some breckets remaining...)\n",
        "!wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# alternative file created in this workflow\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d8f1bb53632d40538e0d/?dl=1\n",
        "\n",
        "!mv index.html?dl=1 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQvNF5V6wjB"
      },
      "source": [
        "!head --lines=10 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQNpab5JrOn"
      },
      "source": [
        "# read / generate all the necessary texts\n",
        "# first define functions, then download files and read them into dictionaries...\n",
        "\n",
        "# further versions of the Gold Standard: \n",
        "# Annotated and PoS-tagged Gold Standard -- for extraction of the correct evaluation set\n",
        "\n",
        "# printing words of different length\n",
        "# FOutput = open('BGH0_s01GoldS_Terms.txt', 'w')\n",
        "# FOutputDict1w = open('BGH0_s01GoldS_D1w.txt', 'w') # 1-word terms\n",
        "# FOutputDict2w = open('BGH0_s01GoldS_D2w.txt', 'w') # 2-word terminological expressions\n",
        "# FOutputDict3w = open('BGH0_s01GoldS_D3w.txt', 'w') # 3-word terminological expressions\n",
        "# FOutputDict4w = open('BGH0_s01GoldS_D4w.txt', 'w') # more than 3 words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA9UNO0NZbt0"
      },
      "source": [
        "# reading datasets\n",
        "# list of gold-standard annotated terms, with lemmatization and pos fileds\n",
        "FInBGH0_s00GS_LEM = open('BGH0_s00GS_LEM.txt', 'r')\n",
        "# FOutBGH0_s00GS_Terms = open('BGH0_s00GS_Terms.txt', 'w')\n",
        "L3AnnotatedSegs = vertCollectAnnotation(FInBGH0_s00GS_LEM, 'TERM', Caps = False)\n",
        "\n",
        "# testing the file read\n",
        "# for LSegment in L3AnnotatedSegs: FOutBGH0_s00GS_Terms.write(str(LSegment) + '\\n')\n",
        "# FOutBGH0_s00GS_Terms.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPE39kkEAOwZ",
        "outputId": "1da7211b-8e22-4ac7-f4d0-94697ec892a1"
      },
      "source": [
        "print(len(L3AnnotatedSegs))\n",
        "L3AnnotatedSegs[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16498\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['OBERSTAATSANWALT', 'NN', 'OBERSTAATSANWALT'],\n",
              " ['BEIM', 'APPRART', 'BEI'],\n",
              " ['BUNDESGERICHTSHOF', 'NN', 'BUNDESGERICHTSHOF']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6NsHtRHHOjp"
      },
      "source": [
        "changeCaseL3(L3AnnotatedSegs, LFlds2Caps = [0, 2], Mode='upper')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3_PKbspAYMD"
      },
      "source": [
        "# create a dictionary of PoS patterns in the gold standard annotation\n",
        "DPatternsFrq = createDictOfPatterns(L3AnnotatedSegs, 1, Normalize = 1)\n",
        "FOutTermPOS = open('BGH0_s00GoldStandard_pos.txt', 'w')\n",
        "\n",
        "printDictionary(DPatternsFrq, FOutTermPOS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX29G8r3Zo6-"
      },
      "source": [
        "try:\n",
        "    del DTermConfFrq\n",
        "    del DGS1w\n",
        "    del DGS2w\n",
        "    del DGS3w\n",
        "    del DGS4w\n",
        "except:\n",
        "    print('dictionaries not defined yet...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_kJbRh3TDa",
        "outputId": "b56017d4-d862-4842-8187-113c0497b9be"
      },
      "source": [
        "# selecting terms from the gold standard, which fit the description\n",
        "# DTermConfFrq = selectTerms(L3AnnotatedSegs, L2Patterns = [['N', '\\$']], SplitLen = False, IFldNumber = 0)\n",
        "# FOutExamples = open('BGH0_s00GoldStandard_examples_ADJ_N.txt', 'w')\n",
        "\n",
        "# DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "# DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ', 'N'], ['N', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['APPR', 'ART','N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 2)\n",
        "\n",
        "\n",
        "FOutExamples = open('BGH0_s00GoldStandard_examples.txt', 'w')\n",
        "FOutExamples1w = open('BGH0_s00GoldStandard1w_examples.txt', 'w')\n",
        "FOutExamples2w = open('BGH0_s00GoldStandard2w_examples.txt', 'w')\n",
        "FOutExamples3w = open('BGH0_s00GoldStandard3w_examples.txt', 'w')\n",
        "FOutExamples4w = open('BGH0_s00GoldStandard4w_examples.txt', 'w')\n",
        "\n",
        "printDictionary(DTermConfFrq, FOutExamples)\n",
        "printDictionary(DGS1w, FOutExamples1w)\n",
        "printDictionary(DGS2w, FOutExamples2w)\n",
        "printDictionary(DGS3w, FOutExamples3w)\n",
        "printDictionary(DGS4w, FOutExamples4w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14698 297 280 9 15284\n",
            "2285 177 105 9 2576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvy0lBNuQ7jI"
      },
      "source": [
        "# print(len(DTermConfFrq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTfxszq3xgjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e52677-a9bc-46f5-f9fc-2b3e36a46bd0"
      },
      "source": [
        "# Stage 2: preparing keyness dictionary\n",
        "!wget https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
        "!mv index.html?dl=1 TK_KW_Verif_V02.csv\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
        "!mv index.html?dl=1 KW_BGH_10000.tsv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 14:21:22--  https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/8e20ed8e-f61b-4a0f-9664-1b1c818070a4/TK_KW_Verif_V02.csv [following]\n",
            "--2021-11-04 14:21:23--  https://heibox.uni-heidelberg.de/seafhttp/files/8e20ed8e-f61b-4a0f-9664-1b1c818070a4/TK_KW_Verif_V02.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38812 (38K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  37.90K   145KB/s    in 0.3s    \n",
            "\n",
            "2021-11-04 14:21:23 (145 KB/s) - ‘index.html?dl=1’ saved [38812/38812]\n",
            "\n",
            "--2021-11-04 14:21:23--  https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/6aa416a5-c4d7-4a52-af60-0f492927c9ed/KW_BGH_10000.csv [following]\n",
            "--2021-11-04 14:21:24--  https://heibox.uni-heidelberg.de/seafhttp/files/6aa416a5-c4d7-4a52-af60-0f492927c9ed/KW_BGH_10000.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216590 (212K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 211.51K   400KB/s    in 0.5s    \n",
            "\n",
            "2021-11-04 14:21:24 (400 KB/s) - ‘index.html?dl=1’ saved [216590/216590]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWk89VE9xyct"
      },
      "source": [
        "# Preparing a dictionnary of keyness weights, checking the 'approval' status\n",
        "FInputKW = open('TK_KW_Verif_V02.csv', 'r')\n",
        "FInputKWLarge = open('KW_BGH_10000.tsv', 'r') # for experiments with Precision / Recall\n",
        "FOutputKW = open('TK_KW_Verif_V02.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3zBlO_Kx2xQ",
        "outputId": "81d02c81-7ae8-46ca-ca7a-1eeacb8f16af"
      },
      "source": [
        "try:\n",
        "    del DScoresKW\n",
        "    del DScoresNK\n",
        "    del DStatKW\n",
        "    del DScoresKWLarge\n",
        "    del DScoresREKWquick\n",
        "except:\n",
        "    print('dictionaries not defined')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionaries not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGkpLArq0m9h"
      },
      "source": [
        "# reading an annotated dictionary: keyword or not: 1, 0.5, 0 (yes, unsure, no)\n",
        "DScoresKW, DScoresNK = readDictKWAnnotations(FInputKW, AKStatThreshold = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9WdlvXV19UW",
        "outputId": "45dddfa4-ce75-49c3-a9c0-741d25102767"
      },
      "source": [
        "try:\n",
        "    print(DScoresKW['ANGEKLAGTE'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['VGL'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['NACHPRÜFUNG'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['RECHTLICH'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['JURISTISCH'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['PERSON'])\n",
        "\n",
        "    # 'JURISTISCHE PERSON'\n",
        "    # 'RECHTLICHER NACHPRÜFUNG'\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.5625\n",
            "345.6272888184\n",
            "3406.630859375\n",
            "keys not found\n",
            "keys not found\n",
            "keys not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQWGjirR174Z"
      },
      "source": [
        "# reading the large keyword dictionary\n",
        "DScoresKWLarge = readDictionary(FInputKWLarge, Caps=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE0Z1ePs754t",
        "outputId": "6116c3ea-36a6-41f2-8b6f-bf4da598b502"
      },
      "source": [
        "try:\n",
        "    print(DScoresKWLarge['ANGEKLAGTE'])\n",
        "    print(DScoresKWLarge['VGL'])\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.56\n",
            "258061.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "165U-v8z8HYg"
      },
      "source": [
        "# Stage 3: Reading a file with extracted terms; capitalizing everything...\n",
        "# Reading test data - Possible Terms (extracted automatically): reading the text files of single and multiword terms, recording ranks\n",
        "# single words candidates\n",
        "#\n",
        "# Warning: these files are 8 and 70 MB respectively (relatively large to view on-line)\n",
        "!wget https://heibox.uni-heidelberg.de/f/a9171080790f4932b7b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02term1w.txt\n",
        "\n",
        "# multiword candidates\n",
        "!wget https://heibox.uni-heidelberg.de/f/2488701205e34e4683b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02termMWE.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D316VSK9m9h",
        "outputId": "a3f2e6a1-123e-4ca6-c913-1925f1169d8b"
      },
      "source": [
        "!head --lines=10 BGH0_s02term1w.txt\n",
        "!head --lines=10 BGH0_s02termMWE.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "§\t1886129\n",
            "Rn\t584348\n",
            "vgl\t487398\n",
            "ZR\t486364\n",
            "Beklagten\t455616\n",
            "Urteil\t442960\n",
            "Satz\t434458\n",
            "Klägerin\t408278\n",
            "BGH\t392017\n",
            "Entscheidung\t376796\n",
            "BUNDESGERICHTSHOF BESCHLUSS\t94953\n",
            "Richter Dr\t72368\n",
            "VI ZR\t56532\n",
            "Vorsitzenden Richter\t56520\n",
            "XI ZR\t54848\n",
            "juris Rn\t54664\n",
            "VIII ZR\t54132\n",
            "V ZR\t41586\n",
            "aaO Rn\t41232\n",
            "III ZR\t39672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQk8MBlf92JN"
      },
      "source": [
        "FAutoTerms1w = open('BGH0_s02term1w.txt', 'r')\n",
        "FAutoTermsMWE = open('BGH0_s02termMWE.txt', 'r')\n",
        "\n",
        "# FoutAutoTerms1w = open('BGH0_s02term1w_out.txt', 'w')\n",
        "# FoutAutoTermsMWE = open('BGH0_s02termMWE_out.txt', 'w')\n",
        "# ... here we add functions for reading this dictionary (e.g., as ranked list, etc.)\n",
        "\n",
        "DAutoTerms1w = readDictionary(FAutoTerms1w, Caps=True)\n",
        "DAutoTermsMWE = readDictionary(FAutoTermsMWE, Caps=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wn2J7Ck-boW",
        "outputId": "ab410e18-16e8-4864-ff55-98afcfd1d0f5"
      },
      "source": [
        "try:\n",
        "    print(DAutoTerms1w['ANGEKLAGTE'])\n",
        "    print(DAutoTerms1w['VGL'])\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172346.0\n",
            "16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3pPwCbQ-8EN"
      },
      "source": [
        "# Stage 3.1 combine extracted words with keyness (e.g., filter by keyness, etc.)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhRHBnHEVnLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ee597d-c792-4d5e-b6bc-de35172dd46f"
      },
      "source": [
        "try:\n",
        "    del DAutoTerms01KW_H1w\n",
        "    del DAutoTerms01KW_HMWE\n",
        "except:\n",
        "    print('dictionaries not defined yet...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionaries not defined yet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvz6SKCUUNf9"
      },
      "source": [
        "DAutoTerms01KW_H1w = filterDictByKWDict(DAutoTerms1w, DScoresKWLarge, Threshold = 1, Req = 1) # autoterms filtered by human annotated items\n",
        "DAutoTerms01KW_HMWE = filterDictByKWDict(DAutoTermsMWE, DScoresKWLarge, Threshold = 1, Req = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2dc9MfZUnIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479c3789-4db0-4701-eec4-b7f81a102528"
      },
      "source": [
        "try:\n",
        "    print(DAutoTerms01KW_H1w['ANGEKLAGTE'])\n",
        "    print(DAutoTerms01KW_H1w['VGL'])\n",
        "\n",
        "    print(DAutoTerms01KW_HMWE['LANDGERICHT AUGSBURG'])\n",
        "    print(DAutoTerms01KW_HMWE['VORSITZENDER RICHTER'])\n",
        "    print(DAutoTerms01KW_HMWE['RECHTLICHER NACHPRÜFUNG'])\n",
        "    print(DAutoTerms01KW_HMWE['JURISTISCHER PERSON'])\n",
        "\n",
        "    # 'RECHTLICHER NACHPRÜFUNG'  \n",
        "except:\n",
        "    print('keys not found')\n",
        "\n",
        "# JURISTISCHE PERSON >> lemmas or text forms in keywords???\n",
        "\n",
        "# print statistics\n",
        "print(len(DAutoTerms1w))\n",
        "print(len(DAutoTermsMWE))\n",
        "\n",
        "print(len(DScoresKWLarge))\n",
        "\n",
        "print(len(DAutoTerms01KW_H1w))\n",
        "print(len(DAutoTerms01KW_HMWE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.56\n",
            "258061.06\n",
            "208549.09\n",
            "105619.59\n",
            "24163.04\n",
            "keys not found\n",
            "466669\n",
            "2434043\n",
            "9979\n",
            "8247\n",
            "1117210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to4SZdlvcC98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}