{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion6EvaluationKeyWordsV01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMse065qlmvtjlLZLz6ngh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion6EvaluationKeyWordsV01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_u_1JsuhpJV"
      },
      "source": [
        "# Terminology extraction with keywords, association measures, etc.\n",
        "\n",
        "Here we enrich extracted MWEs with keyness information (and other paremeters) and test Precision / Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI638SJm_FAJ"
      },
      "source": [
        "## Preparing gold standard annotation dictinaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEIsH60_hdEL"
      },
      "source": [
        "# Stage 1: Preparing Gold standard: Reading / extracting information from gold standard: creating a list of annotated terms\n",
        "# set 1 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Astghik.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Maia.txt\n",
        "\n",
        "# set 2 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Hayk.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Nino.txt\n",
        "\n",
        "# set 3 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
        "!mv index.html?dl=1 BGH3_s00Tamar.txt\n",
        "# !wget \n",
        "# !mv index.html?dl=1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LdbtxRwwauy"
      },
      "source": [
        "# one more will be added: Frau Khachatryan\n",
        "!cat BGH1_s00Astghik.txt BGH1_s00Maia.txt BGH2_s00Hayk.txt BGH2_s00Nino.txt BGH3_s00Tamar.txt >BGH0_s00GoldStandard.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNK9JjVN1_N7"
      },
      "source": [
        "# a useful function for recording / visualising current stage of dictionaries\n",
        "def printDictionary(DictionaryFrq, FOut, K = 1, Rev = True): # printing a dictionary: by values or alphabetically\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[K], reverse=Rev):\n",
        "        FOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    return"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65srZoeW-wwA"
      },
      "source": [
        "FInput = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "FOutput = open('BGH0_s01GoldS_Terms.txt', 'w')\n",
        "# for statistical purposes - separately single and multiword terms\n",
        "FOutputDict1w = open('BGH0_s01GoldS_D1w.txt', 'w') # 1-word terms\n",
        "FOutputDict2w = open('BGH0_s01GoldS_D2w.txt', 'w') # 2-word terminological expressions\n",
        "FOutputDict3w = open('BGH0_s01GoldS_D3w.txt', 'w') # 3-word terminological expressions\n",
        "FOutputDictMWE = open('BGH0_s01GoldS_DMWE.txt', 'w') # more than 3 words"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB30Z1Fm6Ic0"
      },
      "source": [
        "# creating gold-standard dictionaries for evaluation tasks:\n",
        "import re, os, sys\n",
        "LGSTerms = [] # gold standard terms\n",
        "DGS1w = {} # dictionary of single words\n",
        "DGS2w = {} # dictionary of 2-word expressions\n",
        "DGS3w = {} # dictionary of 3-word expressions\n",
        "DGSMWE = {} # dictionary of other mwes\n",
        "IGS1w = 0 # number of annotated tokens of single words\n",
        "IGS2w = 0\n",
        "IGS3w = 0\n",
        "IGSMWE = 0 # number of annotated tokens of multiwords\n",
        "for SLine in FInput:\n",
        "    LAnnotatedTermsInLine = re.findall('<<([^><]+)>>', SLine)\n",
        "    LGSTerms.extend(LAnnotatedTermsInLine)\n",
        "\n",
        "for GSTerm in LGSTerms:\n",
        "    GSTerm = GSTerm.strip()\n",
        "    GSTerm = GSTerm.strip('„“\"().')\n",
        "\n",
        "    # everything is converted to upper case for quick dictionary lookup\n",
        "    GSTerm = GSTerm.upper()\n",
        "    \n",
        "    GSTerm = re.sub(' +', ' ', GSTerm)\n",
        "    LGSTErms = re.split(' ', GSTerm)\n",
        "    if len(LGSTErms) > 3:\n",
        "        IGSMWE += 1\n",
        "        try: DGSMWE[GSTerm] += 1\n",
        "        except: DGSMWE[GSTerm] = 1\n",
        "    elif len(LGSTErms) > 2:\n",
        "        IGS3w += 1\n",
        "        try: DGS3w[GSTerm] += 1\n",
        "        except: DGS3w[GSTerm] = 1\n",
        "    elif len(LGSTErms) > 1:\n",
        "        IGS2w += 1\n",
        "        try: DGS2w[GSTerm] +=1\n",
        "        except: DGS2w[GSTerm] = 1\n",
        "    else:\n",
        "        IGS1w += 1\n",
        "        try: DGS1w[GSTerm] +=1\n",
        "        except: DGS1w[GSTerm] = 1\n",
        "\n",
        "    FOutput.write(GSTerm + '\\n')\n",
        "\n",
        "FOutputDictMWE.write('# Number of tokens: ' + str(IGSMWE) + '\\n')\n",
        "FOutputDict3w.write('# Number of tokens: ' + str(IGS3w) + '\\n')\n",
        "FOutputDict2w.write('# Number of tokens: ' + str(IGS2w) + '\\n')\n",
        "FOutputDict1w.write('# Number of tokens: ' + str(IGS1w) + '\\n')\n",
        "\n",
        "printDictionary(DGSMWE, FOutputDictMWE)\n",
        "printDictionary(DGS3w, FOutputDict3w)\n",
        "printDictionary(DGS2w, FOutputDict2w)\n",
        "printDictionary(DGS1w, FOutputDict1w)\n",
        "\n",
        "FOutputDictMWE.flush()\n",
        "FOutputDictMWE.close()\n",
        "FOutputDict3w.flush()\n",
        "FOutputDict3w.close()\n",
        "FOutputDict2w.flush()\n",
        "FOutputDict2w.close()\n",
        "FOutputDict1w.flush()\n",
        "FOutputDict1w.close()\n",
        "\n",
        "FOutput.flush()\n",
        "FOutput.close()\n",
        "\n",
        "FInput.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0gL2rD-6Py"
      },
      "source": [
        "## Preparing the 'keyness' dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iceETOGHG8ie"
      },
      "source": [
        "# Stage 2: preparing keyness dictionary\n",
        "!wget https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
        "!mv index.html?dl=1 TK_KW_Verif_V02.csv\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
        "!mv index.html?dl=1 KW_BGH_10000.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2okBQ92YwfKw"
      },
      "source": [
        "# Preparing a dictionnary of keyness weights, checking the 'approval' status\n",
        "FInputKW = open('TK_KW_Verif_V02.csv', 'r')\n",
        "FInputKWLarge = open('KW_BGH_10000.tsv', 'r') # for experiments with Precision / Recall\n",
        "FOutputKW = open('TK_KW_Verif_V02.txt', 'w')\n",
        "# FOutputGSKWS1w = open('BGH1_s01GoldSKW_D1w.txt', 'w')\n",
        "# FOutputGSKWS2w = open('BGH1_s01GoldSKW_D2w.txt', 'w')\n",
        "# FOutputGSKWS3w = open('BGH1_s01GoldSKW_D3w.txt', 'w')\n",
        "# FOutputGSKWSMWE = open('BGH1_s01GoldSKW_MWE.txt', 'w')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im1Syy-JwqWt"
      },
      "source": [
        "DScoresKW = {} # keywords - scores\n",
        "DScoresNK = {} # non-keywords\n",
        "DStatKW = {} # status: key/non-key-word\n",
        "for Line in FInputKW:\n",
        "    LFieldsKW = re.split('\\t', Line)\n",
        "    SWord = LFieldsKW[1]\n",
        "    AKScore = float(LFieldsKW[2])\n",
        "    AKStat = float(LFieldsKW[3])\n",
        "    DStatKW[SWord] = AKStat\n",
        "    if AKStat > 0: # change value to 0.5 if we need to restrict to 'sure' terms only (value 1)\n",
        "        DScoresKW[SWord] = AKScore\n",
        "    else:\n",
        "        DScoresNK[SWord] = AKScore"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XowrNS8l0YQk"
      },
      "source": [
        "# we create a dictionary of keyness values with only upper case letters, which will be checked against also uppercased term candidates\n",
        "# the same dictionary as DScoresKW, but with ensured conversion in to upper case:\n",
        "DScoresREKWquick = {} # dictionary of RE\n",
        "for kw, val in DScoresKW.items():\n",
        "    SUpperC = kw.upper() # making sure our key words are in upper case\n",
        "    # these are the alternatives, which we do not consider in this stage...\n",
        "    # SLowerC = kw.lower()\n",
        "    # SSentenceC = kw.capitalize()\n",
        "    # RPatternKW = re.compile('^' + kw + '$', re.IGNORECASE)\n",
        "    DScoresREKWquick[SUpperC] = val # \n",
        "    # DScoresREKWquick[SLowerC] = val # \n",
        "    # DScoresREKWquick[SSentenceC] = val # "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtUKUkDi5fdd"
      },
      "source": [
        "printDictionary(DScoresREKWquick, FOutputKW)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf3a_32PIioD"
      },
      "source": [
        "## Preparing a dictionary of automatically extracted terms using PoS configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVXmJo4PHx5-"
      },
      "source": [
        "# Stage 3: Reading a file with extracted terms; capitalizing everything...\n",
        "# Reading test data - Possible Terms (extracted automatically): reading the text files of single and multiword terms, recording ranks\n",
        "# single words candidates\n",
        "#\n",
        "# Warning: these files are 8 and 70 MB respectively (relatively large to view on-line)\n",
        "!wget https://heibox.uni-heidelberg.de/f/a9171080790f4932b7b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02term1w.txt\n",
        "\n",
        "# multiword candidates\n",
        "!wget https://heibox.uni-heidelberg.de/f/2488701205e34e4683b1/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s02termMWE.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no-I_PP8Y9So"
      },
      "source": [
        "# ... here we will add functions for reading this dictionary (e.g., as ranked list, etc.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JdMo0qPWx6"
      },
      "source": [
        "We will check:\n",
        "\n",
        "- how terminology extraction works for Precison and Recall (intersecting the Gold Standard and extracted terms); \n",
        "\n",
        "- how high is the rank of the terms in the extracted list, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTMBRsW0JdXA"
      },
      "source": [
        "# Stage04: preparing data for calculating precision and recall on the space of all possible MWEs, 1, 2, 3 words; (overlapping)\n",
        "# keeping only 1 version of the text (2 annotators annotated the same text twice to measure interannotator agreement)\n",
        "!cat BGH1_s00Astghik.txt BGH2_s00Hayk.txt BGH3_s00Tamar.txt >BGH0_s03GoldStandard1Version.txt\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQ8SJI5kvF2"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQVspvOj51o"
      },
      "source": [
        "FInputGS1V = open('BGH0_s03GoldStandard1Version.txt', 'r')\n",
        "# tokenizing gold standard\n",
        "'''\n",
        "The idea is to tokenise the gold standard (from Stage 0), and to generate all possible MWEs for each string / pargraph\n",
        "    then we can test what is the coverage (non-overlapping) or precision (overlapping)\n",
        "    or: we create a dictionary of potential single and MWE strings and check what has been identified ?\n",
        "    or: comparing with 'oracle': known annotations are run as a point of comparision on the space; and we establish relations, i.e., the amount of over-generation\n",
        "\n",
        "    tasks: \n",
        "        3a: create the \"all possible strings\" space from gold standard text\n",
        "        3b: intersect 3a results with corpus list of extracted MWEs >> generate \"extracted from gold standar\" dictionary\n",
        "        3c: intersect human annotation in gold standard with 3a >> generate \"correct in gold standard\" dictionary\n",
        "        3d: intersect 3b and 3c, >> correctly extracted\n",
        "        3e: calculate 3d/3b = precision\n",
        "            calculate 3d/3c = recall\n",
        "\n",
        "'''\n",
        "# 3a: processing gold standard: tokenizing\n",
        "import re, os, sys\n",
        "LLParTokens = [] # List of paragraphs, each represented as a list of tokens\n",
        "for SLine in FInputGS1V:\n",
        "    # print(SLine)\n",
        "    SLine.strip()\n",
        "\n",
        "    # separate punctuation\n",
        "    SLine = re.sub(r'(,\\.;:\\-\\!\\?\\(\\)\\[\\]\\“\\\")', r' \\1 ', SLine)\n",
        "    # remove annotation\n",
        "\n",
        "\n",
        "    SLine = re.sub('[<>]+', ' ', SLine)\n",
        "    SLine = re.sub(' +', ' ', SLine)\n",
        "    SLine = SLine.upper() # capitalize all words\n",
        "    # print(SLine)\n",
        "\n",
        "    LLine = re.split(' ', SLine) \n",
        "    LLParTokens.append(LLine)\n",
        "\n",
        "FInputGS1V.close()\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMpKbSTsjZ-a",
        "outputId": "35c380d7-e003-4cc9-85e4-fd7354ea4ab7"
      },
      "source": [
        "print(str(LLParTokens[9]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['UND', 'DIE', 'RICHTER', 'AM', 'BUNDESGERICHTSHOF', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygnQ9IZkz3R"
      },
      "source": [
        "# generating candidate MWEs for cheking if / when they have been identified as terms\n",
        "# algorithm from Terminologieextraktion3 notebook\n",
        "# 3a: creating space of all possible overlapping MWEs in gold standard\n",
        "import re, os, sys\n",
        "def tokens2candNGrams(LWords, N): # working with specific N-gram size, to keep number of candidates under control\n",
        "    '''\n",
        "    convert a list of tokens into a list of all possible MWEs (works for each paragraph)\n",
        "    '''\n",
        "    LLCandidates = [] # lists - tokenised results\n",
        "    # LSCandidates = [] # strings - joint results\n",
        "\n",
        "    for i in range(N): # for up to the required N-gram length\n",
        "\n",
        "        for IPosition in range(len(LWords) - i): # unigrams -- no change; bigrams: up to penultimate, etc.\n",
        "            LCandidate = LWords[IPosition : IPosition + i + 1]\n",
        "            # SCandidate = ' '.join(LCandidate)\n",
        "            LLCandidates.append(LCandidate)\n",
        "        # LSCandidates.append(SCandidate)\n",
        "    \n",
        "    return LLCandidates\n",
        "\n",
        "\n",
        "    ''' # full version; now abandoned...\n",
        "    for klen in range(len(LWords)): # lengths of candidate lists\n",
        "        klength = klen+1 # true length: for 0 it is le = 1\n",
        "        # print(f'klen:{klength};')\n",
        "        for i in range(len(LWords) - klen): # positions where candidates start\n",
        "            # print(f'i:{i};')\n",
        "            LCandidate = LWords[i:i+klength]\n",
        "            SCandidate = ' '.join(LCandidate)\n",
        "            LLCandidates.append(LCandidate)\n",
        "            LSCandidates.append(SCandidate)\n",
        "        \n",
        "    return LLCandidates, LSCandidates\n",
        "    '''"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "U8NvSEc-dbE8",
        "outputId": "427791e9-7b29-4215-c1bd-357ba9de8d5c"
      },
      "source": [
        "DA_1W = {} # dictionary of 1-word candidates from the gold standard text (to be tested)\n",
        "DA_2W = {} # dictionary of 2-word candidates from the gold standard text (to be tested)\n",
        "DA_3W = {} # dictionary of 3-word candidates from the gold standard text (to be tested)\n",
        "DA_MWE = {} # dictionary of MWE candidates from the gold standard text (to be tested)\n",
        "\n",
        "for LTokens in LLParTokens: # for each paragraph\n",
        "    LLCandidates = tokens2candNGrams(LTokens, 4)\n",
        "    print(str(LLCandidates))\n",
        "    for LCandidate in LLCandidates:\n",
        "        SCandidate = ' '.join(LCandidate)\n",
        "        if len(LCandidate) > 3:\n",
        "            try:\n",
        "                DA_MWE[SCandidate] += 1\n",
        "            except:\n",
        "                DA_MWE[SCandidate] = 1  \n",
        "        if len(LCandidate) > 2:\n",
        "            try:\n",
        "                DA_3W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_3W[SCandidate] = 1  \n",
        "        if len(LCandidate) > 1:\n",
        "            try:\n",
        "                DA_2W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_2W[SCandidate] = 1  \n",
        "        else:\n",
        "            try:\n",
        "                DA_1W[SCandidate] += 1\n",
        "            except:\n",
        "                DA_1W[SCandidate] = 1\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-f864bc154ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLLCandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens2candNGrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLCandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mLCandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLLCandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mSCandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLCandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLCandidate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-3xaIslkrlm"
      },
      "source": [
        "FOutputA1w = open('BGH0_s04A_1w_res.txt', 'w')\n",
        "FOutputA1w = open('BGH0_s04A_2w_res.txt', 'w')\n",
        "FOutputA3w = open('BGH0_s04A_3w_res.txt', 'w')\n",
        "FOutputAMWE = open('BGH1_s04A_MWE_res.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnQeFPXGg_3j"
      },
      "source": [
        "printDictionary(DA_1W, FOutputA1w, Rev = True)\n",
        "printDictionary(DA_2W, FOutputA2w, Rev = True)\n",
        "printDictionary(DA_3W, FOutputA3w, Rev = True)\n",
        "printDictionary(DA_MWE, FOutputAMWE, Rev = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}