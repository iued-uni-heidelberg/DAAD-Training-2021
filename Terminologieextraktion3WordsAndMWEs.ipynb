{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion3WordsAndMWEs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion3WordsAndMWEs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir693xy5khPU"
      },
      "source": [
        "# Terminology extraction experiments\n",
        "\n",
        "## Improving and running terminology extraction script on different corpora\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtWZ1FuvlfLk"
      },
      "source": [
        "# notebook Terminologieextraktion3WordsAndMWEs.ipynb is a production stage version: all debugging printouts have been removed; optimization for speed"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXKKCk4ow8fQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bfd48d-91ce-4e80-d342-ee680746c058"
      },
      "source": [
        "# ArmenianWP\n",
        "# !wget https://heibox.uni-heidelberg.de/f/206d85a0270943e4b87b/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 WPhy_vert.txt\n",
        "\n",
        "# Constitution Republic of Armenia\n",
        "# !wget https://heibox.uni-heidelberg.de/f/bf54977b17604ec592cd/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 Const_RA_l.txt\n",
        "\n",
        "# Grundgesetz\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d6c5b31edc84422d9e14/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 GG_l.txt\n",
        "\n",
        "# Origin of the species\n",
        "# !wget https://heibox.uni-heidelberg.de/f/befc6dbe718b4a37ba74/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 OOOS_l.txt\n",
        "\n",
        "# Europarl DE\n",
        "# !wget https://heibox.uni-heidelberg.de/f/3ba6122e744e4b7f9c14/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 EP_DE_l.txt\n",
        "\n",
        "# 23.10.2021 part\n",
        "# German legal corpus, lemmatized in a zip archive\n",
        "!wget https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 BGH-utf8-lem.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-24 10:41:30--  https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/b4d03479-8c5c-43f5-bf87-9c68a6f111aa/output-utf8-lem.zip [following]\n",
            "--2021-10-24 10:41:31--  https://heibox.uni-heidelberg.de/seafhttp/files/b4d03479-8c5c-43f5-bf87-9c68a6f111aa/output-utf8-lem.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 673144348 (642M) [application/zip]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 641.96M  14.9MB/s    in 43s     \n",
            "\n",
            "2021-10-24 10:42:14 (14.9 MB/s) - ‘index.html?dl=1’ saved [673144348/673144348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAvDk1BRQtsZ",
        "outputId": "b27ee1e3-39cf-4877-e464-655fe06fef6f"
      },
      "source": [
        "!unzip BGH-utf8-lem.zip\n",
        "!rm BGH-utf8-lem.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BGH-utf8-lem.zip\n",
            "  inflating: output-utf8-lem.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_OKgqfNRmQO"
      },
      "source": [
        "!mv output-utf8-lem.txt BGHlem.txt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koQG-wAsR0rQ"
      },
      "source": [
        "!head --lines=100000 BGHlem.txt >BGHlem1k.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kufm20oy9NIa",
        "outputId": "0a8c9e19-5bc7-4e57-9ffb-7d83f3871689"
      },
      "source": [
        "# OPTIONAL -- just to know how big is our corpus, on a long corpus it can take a lot of time...\n",
        "# word counts: BGHlem.txt should be ~221 M lines long (vert lemmatized format; so one line is one word)\n",
        "!wc BGHlem1k.txt\n",
        "!wc BGHlem.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100000  299903 1673786 BGHlem1k.txt\n",
            " 220915764  662340855 3684207041 BGHlem.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMF8TyfHjmAy",
        "outputId": "34851de2-2d17-4974-e87c-fb47025c0f87"
      },
      "source": [
        "# OPTIONAL -- just to demonstrate the algorithm of term selection\n",
        "# debugging and explaining the algorithm for Stage 1:\n",
        "LLCandidates = []\n",
        "LWords = ['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n",
        "for klen in range(len(LWords)): # lengths of candidate lists\n",
        "    klength = klen+1 # true length: for 0 it is le = 1\n",
        "    print(f'klen:{klength};')\n",
        "    for i in range(len(LWords) - klen): # positions where candidates start\n",
        "        print(f'i:{i};')\n",
        "        LCandidate = LWords[i:i+klength]\n",
        "        LLCandidates.append(LCandidate)\n",
        "        print(LCandidate)\n",
        "\n",
        "        # print(LWords[i])\n",
        "\n",
        "for LEl in LLCandidates:\n",
        "    # for el in LEl:\n",
        "    print(LEl)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "klen:1;\n",
            "i:0;\n",
            "['Revisionsverfahren']\n",
            "i:1;\n",
            "['entstanden']\n",
            "i:2;\n",
            "['notwendig']\n",
            "i:3;\n",
            "['Auslage']\n",
            "klen:2;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden']\n",
            "i:1;\n",
            "['entstanden', 'notwendig']\n",
            "i:2;\n",
            "['notwendig', 'Auslage']\n",
            "klen:3;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig']\n",
            "i:1;\n",
            "['entstanden', 'notwendig', 'Auslage']\n",
            "klen:4;\n",
            "i:0;\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n",
            "['Revisionsverfahren']\n",
            "['entstanden']\n",
            "['notwendig']\n",
            "['Auslage']\n",
            "['Revisionsverfahren', 'entstanden']\n",
            "['entstanden', 'notwendig']\n",
            "['notwendig', 'Auslage']\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig']\n",
            "['entstanden', 'notwendig', 'Auslage']\n",
            "['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeamIEr15Lev"
      },
      "source": [
        "#Terminologieextraktion\n",
        "import os, re, sys\n",
        "class clProcCorpus(object):\n",
        "    ''' we will read a text file and return a dictionary\n",
        "    this will be done on the line by line basis\n",
        "    The dictionary can be sorted later...\n",
        "    '''\n",
        "    # this class is processing a corpus\n",
        "\n",
        "    def __init__(self, FileIN):\n",
        "        self.DictFrq = {}\n",
        "        self.processCorpus(FileIN)\n",
        "\n",
        "\n",
        "    def procCorpSelectField4FrqDic(self, IFieldNumber, L2PossibleTerm):\n",
        "        '''\n",
        "        this functions takes a 2D list (just one possible term at a time, with all fields) and returns a string which represents the term -- either as inflected words or lemmas\n",
        "        '''\n",
        "        LSelectedField = []\n",
        "        for LFields in L2PossibleTerm:\n",
        "            LSelectedField.append(LFields[IFieldNumber])\n",
        "        \n",
        "        SSelectedField = ' '.join(LSelectedField)\n",
        "        return SSelectedField\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def processCorpus(self, FileIN):\n",
        "        # here we consider a larger MWE, which has been collected, e.g., \n",
        "        # Wahl Schluckebier Nachschlagewerk; or:\n",
        "        # graphical user interface\n",
        "        # in the default version only the longest string is preserved;\n",
        "        # we try to split it into meaningful smaller units and preserve them as well:\n",
        "        # Wahl Schluckebier\n",
        "        # graphical user ; user interface ; interface\n",
        "        # PoS restrictions apply: \n",
        "        # - Adj cannot be at the end; function words (Prepositions, articles, etc.) cannot be on either edge.\n",
        "        # so we keep the list of tuples: and extract smaller MWEs from the larger strings using PoS restrictions\n",
        "\n",
        "        # Stage 0: collecting the longest string, with allowed PoS codes, e.g., the longest contiuous string of N, Adj, Prep, Gen. articles\n",
        "        LLTerm = [] # this is the List of Lists: list of words (with their fields), representing a sequence of allowed PoS codes\n",
        "        n = 0 # count lines processed (monitoring progress over the corpus)\n",
        "        for Line in FileIN:\n",
        "            n+=1\n",
        "            if n%1000000 == 0: print(n) # print every 1 Millonth line number\n",
        "            Line = Line.strip()\n",
        "            LLine = re.split('\\t', Line)\n",
        "            \n",
        "            try:\n",
        "                Word = LLine[0]\n",
        "                PoS = LLine[1]\n",
        "                Lemma = LLine[2]\n",
        "            except:\n",
        "                Word = \"\"\n",
        "                PoS = \"\"\n",
        "                Lemma = \"\"\n",
        "            \n",
        "            #Select the Tags for your langauge\n",
        "            # update: this is now done in two stages: \n",
        "            # Stage 1: a set of any PoS codes which may contain candidates\n",
        "            # Stage 2: restrictions on sequences (e.g., what is not allowed at the end, start...)\n",
        "            # \n",
        "            # CHANGE HERE FOR YOUR LANGUAGE: stage 1\n",
        "            #if re.match('N.*', PoS) or re.match('A.*', PoS): #Arm\n",
        "            #if re.match('N.*', PoS) or re.match('J.*', PoS): #EN\n",
        "            if re.match('N.*', PoS) or re.match('ADJ.*', PoS): #DE\n",
        "                LLTerm.append(LLine) # all the field as a list, to form the list of lists\n",
        "\n",
        "                # Terms as Words or Lemmas -- MOVED TO Stage 2\n",
        "                # to compare with the gold standard do we need words, or do we need to generate words from lemas?\n",
        "                # LTerm.append(Lemma)\n",
        "                # here we keep all the fields, because sub-sections of the longer MWE would also be analysed using PoS codes\n",
        "\n",
        "            else: # end of the 'candidate collection'\n",
        "                ### changed to an algorithm based on the data format: list of lists\n",
        "\n",
        "                # Stage 1: we generate candidate sub-n-grams using the longest allowed sequence, starting from single words to the longest one (the whole sequence)\n",
        "                L3Candidates = [] # this will be the list of all MWE candidates (sub-sequences)\n",
        "                IMaxMWE = len(LLTerm) # this is the longest MWE we can get from LLTerm: the length of the whole sequence\n",
        "                for klen in range(IMaxMWE): # lengths of candidate lists: starting from 0 till, but not including the max length\n",
        "                    klength = klen+1 # converting to true length: from length 1 till, and including the max length (we need this for slicing)\n",
        "                    # print(f'klen:{klength};')\n",
        "                    for i in range(IMaxMWE - klen): # positions where candidates start: position is from 0... up to (depending on length)\n",
        "                        # print(f'i:{i};')\n",
        "                        L2Candidate = LLTerm[i:i+klength] # L2 stands fro 'list of lists' (because we preserve 'word', 'lemma', 'pos' as a list for each item)\n",
        "                        L3Candidates.append(L2Candidate) # L3 is 'list of lists of lists' (here we collect all candidate sub-sequences from the longest L2 sequence)\n",
        "\n",
        "                # Stage 2: filtering by part-of-speech configurations: what is impossible, e.g., Adjectives can only appear in the beginning!\n",
        "                # CHANGE HERE FOR YOUR LANGUAGE: stage 2\n",
        "                L3PossibleTerms = [] # filtered lists of terms\n",
        "                for L2Candidate in L3Candidates: # for each candidate configuration\n",
        "                    # e.g., if Adj is the last element (at the end) -- do not add\n",
        "                    # ADD MORE CONFIGURATIONS, eg, Gen case articles, prepositions\n",
        "                    # FileDebug.write(str(L2Candidate) + '\\n') # TEMPORARY -  DEBUGGING\n",
        "                    # Word = L2Candidate[WNumber][0]; PoS = L2Candidate[WNumber][1]; Lemma = L2Candidate[WNumber][2]\n",
        "                    # CONF 1: remove adjectives at the end: ListOfFields4LastWord = L2Candidate[-1] \n",
        "                    if re.match('ADJ.*', L2Candidate[-1][1]): \n",
        "                        # FileDebug.write('\\t Removed:' + str(L2Candidate) + '\\n')\n",
        "                        continue\n",
        "                    # ADD MORE CONFIGURATIONS HERE, e.g, Gen. case articles...\n",
        "                    else: # when all filters passed -- add all fields; frq list of terms will be represented by the Word or Lemma field in the end\n",
        "                        L3PossibleTerms.append(L2Candidate)\n",
        "                # here we have L3PossibleTerms ready; con\n",
        "                # FileDebug.write('---\\n' + str(L3PossibleTerms) + '\\n---\\n')\n",
        "\n",
        "                # going over all elements in the list of kept PossibleTerms, converting to each to a string and adding each one to the frequency dictionary\n",
        "                \n",
        "                for L2PossibleTerm in L3PossibleTerms:\n",
        "                    # CHOOSE SECOND ARGUMENT TO COLLECT: Word = 0; PoS = 1; Lemma = 2\n",
        "                    SPossibleTerm = self.procCorpSelectField4FrqDic(0, L2PossibleTerm)\n",
        "\n",
        "                    try:\n",
        "                        # self.DictFrq[STerm] += 1\n",
        "                        self.DictFrq[SPossibleTerm] += 1\n",
        "                    except:\n",
        "                        # self.DictFrq[STerm] = 1\n",
        "                        self.DictFrq[SPossibleTerm] = 1  \n",
        "\n",
        "                LLTerm = [] # now we clear the list represening allowed PoS sequence, and start over again (we are in the else, we encountered end of allowed PoS codes)\n",
        "                     \n",
        "        return\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnxocDPBMHcL"
      },
      "source": [
        "# FileDebug = open('BGH_debug.txt', 'w')\n",
        "# TEST:\n",
        "# FileIN = open('BGHlem1k.txt', 'r')\n",
        "# PRODUCTION:\n",
        "FileIN = open('BGHlem.txt', 'r')\n",
        "\n",
        "\n",
        "FileOut1w = open('BGH_term1w.txt', 'w')\n",
        "FileOutMWE = open('BGH_termMWE.txt', 'w')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY0xMY9SMSEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c6f404-1d5e-4594-de49-2b205757c4a1"
      },
      "source": [
        "# save the frequency dictionary into file, by decreasing frequencies\n",
        "# FileOutput.write( str( DictionaryFrq ) + '\\n' )\n",
        "\n",
        "OCorpus = clProcCorpus(FileIN)\n",
        "DictionaryFrq = OCorpus.DictFrq\n",
        "\n",
        "\n",
        "for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[1], reverse=True):\n",
        "    if re.search(' ', Word):\n",
        "        FileOutMWE.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    else:\n",
        "        FileOut1w.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "\n",
        "FileOutMWE.flush()\n",
        "FileOutMWE.close()\n",
        "FileOut1w.flush()\n",
        "FileOut1w.close()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n",
            "2000000\n",
            "3000000\n",
            "4000000\n",
            "5000000\n",
            "6000000\n",
            "7000000\n",
            "8000000\n",
            "9000000\n",
            "10000000\n",
            "11000000\n",
            "12000000\n",
            "13000000\n",
            "14000000\n",
            "15000000\n",
            "16000000\n",
            "17000000\n",
            "18000000\n",
            "19000000\n",
            "20000000\n",
            "21000000\n",
            "22000000\n",
            "23000000\n",
            "24000000\n",
            "25000000\n",
            "26000000\n",
            "27000000\n",
            "28000000\n",
            "29000000\n",
            "30000000\n",
            "31000000\n",
            "32000000\n",
            "33000000\n",
            "34000000\n",
            "35000000\n",
            "36000000\n",
            "37000000\n",
            "38000000\n",
            "39000000\n",
            "40000000\n",
            "41000000\n",
            "42000000\n",
            "43000000\n",
            "44000000\n",
            "45000000\n",
            "46000000\n",
            "47000000\n",
            "48000000\n",
            "49000000\n",
            "50000000\n",
            "51000000\n",
            "52000000\n",
            "53000000\n",
            "54000000\n",
            "55000000\n",
            "56000000\n",
            "57000000\n",
            "58000000\n",
            "59000000\n",
            "60000000\n",
            "61000000\n",
            "62000000\n",
            "63000000\n",
            "64000000\n",
            "65000000\n",
            "66000000\n",
            "67000000\n",
            "68000000\n",
            "69000000\n",
            "70000000\n",
            "71000000\n",
            "72000000\n",
            "73000000\n",
            "74000000\n",
            "75000000\n",
            "76000000\n",
            "77000000\n",
            "78000000\n",
            "79000000\n",
            "80000000\n",
            "81000000\n",
            "82000000\n",
            "83000000\n",
            "84000000\n",
            "85000000\n",
            "86000000\n",
            "87000000\n",
            "88000000\n",
            "89000000\n",
            "90000000\n",
            "91000000\n",
            "92000000\n",
            "93000000\n",
            "94000000\n",
            "95000000\n",
            "96000000\n",
            "97000000\n",
            "98000000\n",
            "99000000\n",
            "100000000\n",
            "101000000\n",
            "102000000\n",
            "103000000\n",
            "104000000\n",
            "105000000\n",
            "106000000\n",
            "107000000\n",
            "108000000\n",
            "109000000\n",
            "110000000\n",
            "111000000\n",
            "112000000\n",
            "113000000\n",
            "114000000\n",
            "115000000\n",
            "116000000\n",
            "117000000\n",
            "118000000\n",
            "119000000\n",
            "120000000\n",
            "121000000\n",
            "122000000\n",
            "123000000\n",
            "124000000\n",
            "125000000\n",
            "126000000\n",
            "127000000\n",
            "128000000\n",
            "129000000\n",
            "130000000\n",
            "131000000\n",
            "132000000\n",
            "133000000\n",
            "134000000\n",
            "135000000\n",
            "136000000\n",
            "137000000\n",
            "138000000\n",
            "139000000\n",
            "140000000\n",
            "141000000\n",
            "142000000\n",
            "143000000\n",
            "144000000\n",
            "145000000\n",
            "146000000\n",
            "147000000\n",
            "148000000\n",
            "149000000\n",
            "150000000\n",
            "151000000\n",
            "152000000\n",
            "153000000\n",
            "154000000\n",
            "155000000\n",
            "156000000\n",
            "157000000\n",
            "158000000\n",
            "159000000\n",
            "160000000\n",
            "161000000\n",
            "162000000\n",
            "163000000\n",
            "164000000\n",
            "165000000\n",
            "166000000\n",
            "167000000\n",
            "168000000\n",
            "169000000\n",
            "170000000\n",
            "171000000\n",
            "172000000\n",
            "173000000\n",
            "174000000\n",
            "175000000\n",
            "176000000\n",
            "177000000\n",
            "178000000\n",
            "179000000\n",
            "180000000\n",
            "181000000\n",
            "182000000\n",
            "183000000\n",
            "184000000\n",
            "185000000\n",
            "186000000\n",
            "187000000\n",
            "188000000\n",
            "189000000\n",
            "190000000\n",
            "191000000\n",
            "192000000\n",
            "193000000\n",
            "194000000\n",
            "195000000\n",
            "196000000\n",
            "197000000\n",
            "198000000\n",
            "199000000\n",
            "200000000\n",
            "201000000\n",
            "202000000\n",
            "203000000\n",
            "204000000\n",
            "205000000\n",
            "206000000\n",
            "207000000\n",
            "208000000\n",
            "209000000\n",
            "210000000\n",
            "211000000\n",
            "212000000\n",
            "213000000\n",
            "214000000\n",
            "215000000\n",
            "216000000\n",
            "217000000\n",
            "218000000\n",
            "219000000\n",
            "220000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcbsMgJbC6hX",
        "outputId": "9c8fa16f-3489-44a9-ac57-91487b080431"
      },
      "source": [
        "!zip BGH_term1w.zip BGH_term1w.txt\n",
        "!zip BGH_termMWE.zip BGH_termMWE.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: BGH_term1w.txt (deflated 63%)\n",
            "  adding: BGH_termMWE.txt (deflated 74%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUPLnj8E5byZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576fa4e6-1111-47d7-9902-6c4e9ef6890b"
      },
      "source": [
        "# optional: how large are our output files for MWEs and single words?\n",
        "!wc BGH_term1w.txt\n",
        "!wc BGH_termMWE.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 476230  952039 8954371 BGH_term1w.txt\n",
            " 2448949  8875996 78612600 BGH_termMWE.txt\n"
          ]
        }
      ]
    }
  ]
}