{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-lexicons-v01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvgv5QBYNnfUVZDM2jXWcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/sentiment_analysis_lexicons_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis lexicons using Twitter feeds"
      ],
      "metadata": {
        "id": "N6RzyTvBB1Ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS9m1i1CBr5T"
      },
      "outputs": [],
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/da61b08e7ec945e9b2a5/?dl=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv index.html?dl=1 lexicon-overlap-score-examples-master.tar.gz\n",
        "!tar -xvzf lexicon-overlap-score-examples-master.tar.gz"
      ],
      "metadata": {
        "id": "sE2DF7LCCFx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Python script to open each file, read json input and copy to one text file for subsequent processing\n",
        "import os, re, sys\n",
        "\n",
        "class clLex2Dict(object):\n",
        "    '''\n",
        "    @author Bogdan Babych, IÃœD, Heidelberg University, 2021\n",
        "    @email bogdan [dot] babych [at] iued [dot] uni-heidelberg [dot] de\n",
        "    '''\n",
        "    def __init__(self, SDirName, output_file = 'lexicon-tweets-all.txt', filtr = 0): # initialising by openning the directories\n",
        "        self.SOutput_file = output_file\n",
        "        self.Filter = filtr\n",
        "        self.openDir(SDirName)\n",
        "        return\n",
        "\n",
        "    def openDir(self, path): # implementation of recursively openning directories from a given rule directory and reading each file recursively into a string\n",
        "        i = 0\n",
        "        DLexiconAll = {}\n",
        "        FOut = open(self.SOutput_file, 'w')\n",
        "\n",
        "        for root,d_names,f_names in os.walk(path):\n",
        "            for f in f_names: # for each file found in the directory\n",
        "                ## remove this if using on another corpus\n",
        "\n",
        "                fullpath = os.path.join(root, f)\n",
        "                i+=1\n",
        "                if i%1==0: \n",
        "                    print(str(i) + '. Processing: ' + f)\n",
        "                    print(fullpath)\n",
        "\n",
        "                DLexicon1File = self.procFile(fullpath, f, i) # returns converted string + tags\n",
        "                \n",
        "                for key, value in DLexicon1File.items():\n",
        "                    try:\n",
        "                        LValues = DLexiconAll[key]\n",
        "                    except:\n",
        "                        LValues = []\n",
        "                    '''\n",
        "                    '''\n",
        "                    LValues.append(value)\n",
        "                    DLexiconAll[key] = LValues\n",
        "                    del(LValues)\n",
        "        DLexiconMerged, DLexiconMerged2Conts = self.mergeLex(DLexiconAll)\n",
        "        self.printDict(DLexiconMerged, DLexiconMerged2Conts, FOut)\n",
        "\n",
        "        # if SText2Write: FOut.write(SText2Write) # if the string is not empty then write to file\n",
        "        # FIn.close()\n",
        "\n",
        "        '''\n",
        "        try:\n",
        "            pass\n",
        "        except:\n",
        "            print(f'file {f} cannot be read or processed')\n",
        "        finally:\n",
        "            pass\n",
        "        '''\n",
        "        \n",
        "        FOut.flush()\n",
        "        FOut.close()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def procFile(self, fullpath, SFNameIn, i): # sending each json string for extraction of text and attaching an correct tags to each output string output string\n",
        "        DLexicon1File = {}\n",
        "        FIn = open(fullpath, 'r')\n",
        "        for SLine in FIn:\n",
        "            SLine = SLine.strip()\n",
        "            try: \n",
        "                LLine = re.split('\\t', SLine)\n",
        "                SWord = LLine[0]\n",
        "                SSentiment = LLine[1]\n",
        "                ASentiment = float(SSentiment)\n",
        "                DLexicon1File[SWord] = ASentiment\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return DLexicon1File\n",
        "\n",
        "\n",
        "\n",
        "    def mergeLex(self, DLexiconAll):\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        DLexiconMerged = {}\n",
        "        DLexiconMerged2Conts = {}\n",
        "\n",
        "        \n",
        "        for key, value in DLexiconAll.items():\n",
        "            sum = 0\n",
        "            for el in value:\n",
        "                try:\n",
        "                    sum = sum + el\n",
        "                except:\n",
        "                    pass\n",
        "            average = sum / len(value)\n",
        "            DLexiconMerged[key] = average\n",
        "            DLexiconMerged2Conts[key] = len(value)\n",
        "\n",
        "\n",
        "        return DLexiconMerged, DLexiconMerged2Conts\n",
        "\n",
        "    def printDict(self, Dict, DictConts, FOut):\n",
        "        for key, value in sorted(Dict.items() , key=lambda x: x[1], reverse=False):\n",
        "            counts = DictConts[key]\n",
        "            if self.Filter:\n",
        "                if counts > self.Filter:\n",
        "                    FOut.write(key + '\\t' + str(value) + '\\t' + str(counts) + '\\n')\n",
        "            else:\n",
        "                FOut.write(key + '\\t' + str(value) + '\\t' + str(counts) + '\\n')\n",
        "        \n",
        "\n",
        "# calling the class\n",
        "OLexicon = clLex2Dict('/content/lexicon-overlap-score-examples-master/lexicons/', filtr = 1)"
      ],
      "metadata": {
        "id": "xlsZGbt0CYZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another positive-negative lexicon\n",
        "extracting from annotated file, putting into tsv"
      ],
      "metadata": {
        "id": "a1f2Onc1Pbrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/9072fdbed717466db460/?dl=1\n",
        "!mv index.html?dl=1 subjectivity_clues_hltemnlp05.zip\n",
        "!unzip subjectivity_clues_hltemnlp05.zip"
      ],
      "metadata": {
        "id": "nndQNUVKQ3v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global DTwitterLex\n",
        "def readTweeterLex(FInLex):\n",
        "    DTwitterLex = {}\n",
        "    for SLine in FInLex:\n",
        "        SLine = SLine.strip()\n",
        "        LLine = re.split('\\t', SLine)\n",
        "        try:\n",
        "            SWord = LLine[0]\n",
        "            SWeight = LLine[1]\n",
        "            DTwitterLex[SWord] = float(SWeight)\n",
        "        except:\n",
        "            print('.')\n",
        "    return DTwitterLex\n",
        "\n"
      ],
      "metadata": {
        "id": "OgaJLJVGTrLX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FInLex = open('/content/lexicon-tweets-all.txt', 'r')\n",
        "SFIn = '/content/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff'\n",
        "SFOut = 'lexicon-positive-negative.txt'\n",
        "PositiveC = 0\n",
        "PositiveS = 0\n",
        "\n",
        "NegativeC = 0\n",
        "NegativeS = 0\n",
        "\n",
        "NeutralC = 0\n",
        "NeutralS = 0\n",
        "\n",
        "NonFoundC = 0\n",
        "\n",
        "DTwitterLex = readTweeterLex(FInLex)\n",
        "print(len(DTwitterLex.items()))\n",
        "FIn = open(SFIn, 'r')\n",
        "FOut = open(SFOut, 'w')\n",
        "for SLine in FIn:\n",
        "    SLine = SLine.strip()\n",
        "    LLine = re.split('[ =]', SLine)\n",
        "    # try:\n",
        "    SWord = LLine[5]\n",
        "    SPoS = LLine[7]\n",
        "    SSentiment = LLine[11]\n",
        "\n",
        "\n",
        "    # Weight = 'No'\n",
        "    \n",
        "    try:\n",
        "        Weight = DTwitterLex[SWord]\n",
        "    except:\n",
        "        Weight = 'NF'\n",
        "\n",
        "    S2Print = '\\t'.join([SWord, SPoS, SSentiment, str(Weight)])\n",
        "    # except:\n",
        "    #     pass\n",
        "    if Weight == 'NF':\n",
        "        NonFoundC += 1\n",
        "    elif SSentiment == 'negative':\n",
        "        NegativeC +=1\n",
        "        NegativeS += Weight\n",
        "    elif SSentiment == 'positive':\n",
        "        PositiveC += 1\n",
        "        PositiveS += Weight\n",
        "\n",
        "    elif SSentiment == 'neutral':\n",
        "        NeutralC +=1\n",
        "        NeutralS += Weight\n",
        "       \n",
        "    FOut.write(S2Print + '\\n')\n",
        "FOut.flush()\n",
        "FOut.close()\n",
        "\n",
        "PositiveA = PositiveS/PositiveC\n",
        "NegativeA = NegativeS/NegativeC\n",
        "NeutralA = NeutralS/NeutralC\n",
        "\n",
        "print(PositiveC)\n",
        "print(PositiveS)\n",
        "print(PositiveA)\n",
        "\n",
        "print(NegativeC)\n",
        "print(NegativeS)\n",
        "print(NegativeA)\n",
        "\n",
        "print(NeutralC)\n",
        "print(NeutralS)\n",
        "print(NeutralA)\n",
        "\n",
        "print(NonFoundC)\n"
      ],
      "metadata": {
        "id": "uadUm06rRj-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging both lexicons"
      ],
      "metadata": {
        "id": "IOEj2_I9wYkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printDict(Dict, DictT, DictPN, FOut):\n",
        "    for key, value in sorted(Dict.items() , key=lambda x: x[1], reverse=False):\n",
        "        valueT = DictT[key]\n",
        "        valuePN = DictPN[key]\n",
        "        FOut.write(key + '\\t' + str(value) + '\\t' + str(valueT) + '\\t' + str(valuePN) + '\\n')\n",
        "    FOut.flush()\n",
        "    FOut.close()\n",
        "def readTweeterLex(FInLex):\n",
        "    DTwitterLex = {}\n",
        "    DTwitterLexCount = {}\n",
        "    for SLine in FInLex:\n",
        "        SLine = SLine.strip()\n",
        "        LLine = re.split('\\t', SLine)\n",
        "        try:\n",
        "            SWord = LLine[0]\n",
        "            SWeight = LLine[1]\n",
        "            SNofTopics = LLine[2]\n",
        "            DTwitterLex[SWord] = float(SWeight)\n",
        "            DTwitterLexCount[SWord] = SNofTopics\n",
        "        except:\n",
        "            print('.')\n",
        "    return DTwitterLex, DTwitterLexCount\n",
        "\n",
        "\n",
        "def readPositiveNegativeLex(SFIn):\n",
        "    DPositiveNegativeLex = {}\n",
        "    for SLine in FIn:\n",
        "        SLine = SLine.strip()\n",
        "        LLine = re.split('[ =]', SLine)\n",
        "        # try:\n",
        "        SWord = LLine[5]\n",
        "        SPoS = LLine[7]\n",
        "        SSentiment = LLine[11]\n",
        "        try:\n",
        "            DPositiveNegativeLex[SWord] = LValuesPosNeg\n",
        "        except:\n",
        "            LValuesPosNeg = []\n",
        "\n",
        "        LValuesPosNeg.append(SSentiment)\n",
        "        DPositiveNegativeLex[SWord] = LValuesPosNeg\n",
        "        del(LValuesPosNeg)\n",
        "\n",
        "\n",
        "    for key, val in DPositiveNegativeLex.items():\n",
        "        se = set(val)\n",
        "        if len(se) > 1:\n",
        "            print(key)\n",
        "            print(val)\n",
        "        DPositiveNegativeLex[key] = list(se)[0]\n",
        "    return DPositiveNegativeLex\n",
        "\n",
        "\n",
        "def mergeDicts(DTweet, DPosNeg):\n",
        "    '''\n",
        "    we trust manual annotation more, only use Tweet if no item found in manual\n",
        "    '''\n",
        "    DAll = {}\n",
        "    DAllEPN = {}\n",
        "    DAllETw = {}\n",
        "    for key, val in DTweet.items():\n",
        "        if key not in DPosNeg.keys():\n",
        "            DAll[key] = val\n",
        "            DAllEPN[key] = False\n",
        "            DAllETw[key] = val\n",
        "        else:\n",
        "            valR = None\n",
        "            valPosNeg = DPosNeg[key]\n",
        "            if valPosNeg == 'positive': valR = 1\n",
        "            elif valPosNeg == 'negative': valR = -1\n",
        "            elif valPosNeg == 'neutral': valR = 0\n",
        "            elif valPosNeg == 'both' or valPosNeg == 'priorpolarity': valR = 0 # then we will trust Twitter again?\n",
        "            DAllEPN[key] = valPosNeg\n",
        "            DAllETw[key] = val\n",
        "            DAll[key] = valR\n",
        "            if valPosNeg == 'both' or valPosNeg == 'priorpolarity': DAll[key] = val # we trust Twitter! (change the value from 0 neutral to twitter!)\n",
        "            if valR == None: print('Tw: ', key, val) # checks for errors\n",
        "\n",
        "    for key, val in DPosNeg.items():\n",
        "        valR = None\n",
        "        valPosNeg = DPosNeg[key]\n",
        "        if valPosNeg == 'positive': valR = 1\n",
        "        elif valPosNeg == 'negative': valR = -1\n",
        "        elif valPosNeg == 'neutral': valR = 0\n",
        "        elif valPosNeg == 'both' or valPosNeg == 'priorpolarity': valR = 0 # then we will trust Twitter again?\n",
        "\n",
        "\n",
        "\n",
        "        if key not in DTweet.keys():\n",
        "            DAllEPN[key] = valPosNeg\n",
        "            DAllETw[key] = False\n",
        "\n",
        "        else:\n",
        "            valT = DTweet[key]\n",
        "            DAllEPN[key] = valPosNeg\n",
        "            DAllETw[key] = valT            \n",
        "\n",
        "        DAll[key] = valR\n",
        "        if (valPosNeg == 'both' or valPosNeg == 'priorpolarity') and key in DTweet.keys(): DAll[key] = valT # we trust Twitter! (change the value from 0 neutral to twitter!)\n",
        "        if valR == None: print('PN: ', key, val) # checks for errors\n",
        "\n",
        "    return DAll, DAllETw, DAllEPN\n",
        "\n",
        "\n",
        "FInLex = open('/content/lexicon-tweets-all.txt', 'r')\n",
        "SFIn = '/content/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff'\n",
        "FIn = open(SFIn, 'r')\n",
        "SFOut = 'lexicon-sentiment-tw-n-pn.txt'\n",
        "FOut = open(SFOut, 'w')\n",
        "\n",
        "DTwitterLex, DTwitterLexCount = readTweeterLex(FInLex)\n",
        "print(len(DTwitterLex.items()))\n",
        "DPositiveNegativeLex = readPositiveNegativeLex(FIn)\n",
        "print(len(DPositiveNegativeLex.items()))\n",
        "\n",
        "DAll, DAllEPN, DAllETw = mergeDicts(DTwitterLex, DPositiveNegativeLex)\n",
        "\n",
        "print(len(DAll))\n",
        "print(len(DAllETw))\n",
        "print(len(DAllEPN))\n",
        "\n",
        "printDict(DAll, DAllETw, DAllEPN, FOut)\n",
        "\n"
      ],
      "metadata": {
        "id": "sTcta9zjwWIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for key, val in DPositiveNegativeLex.items():\n",
        "    i+=1\n",
        "    if i%100 == 0:\n",
        "        print(key)\n",
        "        print('\\t' + val)\n"
      ],
      "metadata": {
        "id": "UBvhImFF5d4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER sentiment repository"
      ],
      "metadata": {
        "id": "N_uZSWX9shxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cjhutto/vaderSentiment.git"
      ],
      "metadata": {
        "id": "XnY7A3SWrp9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SFInput = '/content/vaderSentiment/vaderSentiment/vader_lexicon.txt'\n",
        "FInput = open(SFInput, 'r')\n",
        "DVader = {}\n",
        "for SLine in FInput:\n",
        "    SLine = SLine.strip()\n",
        "    LLine = re.split('\\t', SLine)\n",
        "    SWord = LLine[0]\n",
        "    SScore = LLine[1]\n",
        "    DVader[SWord] = float(SScore)\n"
      ],
      "metadata": {
        "id": "A6Vw_R9asxxt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging all three lexicons: Vader and Tw-n-Pn\n",
        "SFInput = '/content/lexicon-sentiment-tw-n-pn.txt'\n",
        "FInput = open(SFInput, 'r')\n",
        "DTpn = {}\n",
        "DTpn2 = {}\n",
        "DTpn3 = {}\n",
        "\n",
        "for SLine in FInput:\n",
        "    SLine = SLine.strip()\n",
        "    LLine = re.split('\\t', SLine)\n",
        "    SWord = LLine[0]\n",
        "    SScore = LLine[1]\n",
        "    SF2 = LLine[2]\n",
        "    SF3  = LLine[3]\n",
        "    DTpn[SWord] = float(SScore)\n",
        "    DTpn2[SWord] = SF2\n",
        "    DTpn3[SWord] = SF3\n",
        "\n"
      ],
      "metadata": {
        "id": "fwVXyTkmtdmg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printDict2(FOut, DJointLex, DJointLexV, DJointLexT, DJointLexT2, DJointLexT3):\n",
        "    for key, value in sorted(DJointLex.items() , key=lambda x: x[1], reverse=False):\n",
        "\n",
        "        FOut.write(key + '\\t' + str(value) + '\\t' + str(DJointLexV[key]) + '\\t' + str(DJointLexT[key]) + '\\t' + str(DJointLexT2[key]) + '\\t' + str(DJointLexT3[key]) + '\\n')\n",
        "    FOut.flush()\n",
        "    FOut.close()\n",
        "\n",
        "\n",
        "def mergeDicts2(DVaderLex, DTpnLex):\n",
        "    '''\n",
        "    we trust VADER more, only use TPN if no item found in VADER\n",
        "    '''\n",
        "    DJointLex = {}\n",
        "    DJointLexV = {}\n",
        "    DJointLexT = {}\n",
        "    DJointLexT2 = {}\n",
        "    DJointLexT3 = {}\n",
        "\n",
        "    for key, val in DTpnLex.items():\n",
        "        if key in DVaderLex.keys():\n",
        "            # integrate information from both dictionaries\n",
        "            DJointLex[key] = DVaderLex[key] # we trust VADER more!\n",
        "            DJointLexV[key] =  DVaderLex[key]\n",
        "            DJointLexT[key] = DTpnLex[key]\n",
        "            DJointLexT2[key] = DTpn2[key]\n",
        "            DJointLexT3[key] = DTpn3[key]\n",
        "\n",
        "        else:\n",
        "            DJointLex[key] = DTpnLex[key] # we trust VADER more!\n",
        "            DJointLexV[key] =  False\n",
        "            DJointLexT[key] = DTpnLex[key]\n",
        "            DJointLexT2[key] = DTpn2[key]\n",
        "            DJointLexT3[key] = DTpn3[key]\n",
        "\n",
        "    for key, val in DVaderLex.items():\n",
        "        if key in DTpnLex.keys():\n",
        "            # integrate information from both dictionaries\n",
        "            DJointLex[key] = DVaderLex[key] # we trust VADER more!\n",
        "            DJointLexV[key] =  DVaderLex[key]\n",
        "            DJointLexT[key] = DTpnLex[key]\n",
        "            DJointLexT2[key] = DTpn2[key]\n",
        "            DJointLexT3[key] = DTpn3[key]\n",
        "        else:\n",
        "            DJointLex[key] = DVaderLex[key] # VADER is the only option!\n",
        "            DJointLexV[key] =  DVaderLex[key]\n",
        "            DJointLexT[key] = False\n",
        "            DJointLexT2[key] = False\n",
        "            DJointLexT3[key] = False\n",
        "    \n",
        "    return DJointLex, DJointLexV, DJointLexT, DJointLexT2, DJointLexT3\n",
        "\n",
        "\n",
        "SFOut = 'lexicon-sentiment-joint-vader-pn-tw.txt'\n",
        "FOut = open(SFOut, 'w')\n",
        "\n",
        "print(len(DVader.items()))\n",
        "print(len(DTpn.items()))\n",
        "\n",
        "DJointLex, DJointLexV, DJointLexT, DJointLexT2, DJointLexT3 = mergeDicts2(DVader, DTpn)\n",
        "\n",
        "print(len(DJointLex))\n",
        "print(len(DJointLexV))\n",
        "print(len(DJointLexT))\n",
        "print(len(DJointLexT2))\n",
        "print(len(DJointLexT3))\n",
        "\n",
        "printDict2(FOut, DJointLex, DJointLexV, DJointLexT, DJointLexT2, DJointLexT3)"
      ],
      "metadata": {
        "id": "pSCoxIk4u5_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering lexicon: \n",
        "only manually annotated items from Vader and Positive-Negative lexicons remain"
      ],
      "metadata": {
        "id": "sdLc9Su1b2cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SFIn = '/content/lexicon-sentiment-joint-vader-pn-tw.txt'\n",
        "SFOut = 'lexicon-sentiment-joint-vader-pn-manual.txt'\n",
        "\n",
        "FIn = open(SFIn, 'r')\n",
        "FOut = open(SFOut, 'w')\n",
        "\n",
        "for SLine in FIn:\n",
        "    SLine = SLine.strip()\n",
        "    LLine = re.split('\\t', SLine)\n",
        "    if LLine[2] == 'False' and LLine[4] == 'False':\n",
        "        continue\n",
        "    else:\n",
        "        FOut.write(SLine + '\\n')\n",
        "\n",
        "\n",
        "FOut.flush()\n",
        "FOut.close()"
      ],
      "metadata": {
        "id": "XiuNvOdPbzQa"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}