{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion9EvaluationKeyWordsV02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOn0jQ1kVK3vg2uZ5geZDa9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion9EvaluationKeyWordsV02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x08Plvp9F_mA"
      },
      "source": [
        "# Stage 0: Some useful read/write and convert functions"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsmKBqMgGWLf"
      },
      "source": [
        "# import useful libraries, files\n",
        "import re, os, sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ9Hkx0OGfCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe387ff-41ad-49f4-82d7-81bf5eac1507"
      },
      "source": [
        "# file for recording results of different configurations\n",
        "# run this only once\n",
        "!rm AllTermExtractionResultsV01.txt\n",
        "!rm AllTermExtractionResultsV02.txt\n",
        "\n",
        "FOutResults1 = open('AllTermExtractionResultsV01.txt', 'a')\n",
        "FOutResults2 = open('AllTermExtractionResultsV02.txt', 'a')\n",
        "FOutResults1.write('Run\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "FOutResults2.write('Run\\tW1P\\tW1R\\tW2P\\tW2R\\tW3P\\tW3R\\tW4P\\tW4R\\n') # only precision and recall figures\n",
        "FOutResults1.flush()\n",
        "FOutResults2.flush()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'AllTermExtractionResultsV01.txt': No such file or directory\n",
            "rm: cannot remove 'AllTermExtractionResultsV02.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqvEq6IGGfv1"
      },
      "source": [
        "## to modify if necessary; however, we try to keep the code standard and parametrize as much as possible\n",
        "\n",
        "# useful functions\n",
        "# a useful function for recording / visualising current stage of dictionaries\n",
        "def printDictionary(DictionaryFrq, FOut, K = 1, Rev = True): # printing a dictionary: by values or alphabetically\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[K], reverse=Rev):\n",
        "        FOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    FOut.flush()\n",
        "    return\n",
        "\n",
        "# another useful function to just read and return a 2-field dictionary, eg., frequency or keyness\n",
        "def readDictionary(FIN, SkipComments = True, Caps=False):\n",
        "    DScoresLarge = {} # keywords - scores\n",
        "    for Line in FIN:\n",
        "        if SkipComments and re.match('#', Line): \n",
        "            continue\n",
        "        Line = Line.strip()\n",
        "        if Caps: \n",
        "            Line = Line.upper() # convert to upper case\n",
        "        LFieldsKW = re.split('\\t', Line)\n",
        "        SWord = LFieldsKW[0]\n",
        "        AKScore = float(LFieldsKW[1])\n",
        "        DScoresLarge[SWord] = AKScore   \n",
        "    return DScoresLarge\n",
        "\n",
        "# another possibly useful function: convert dictionary values to ranks (frequency, keyness weights, etc.)\n",
        "# for understanding how far down the list the item has been found...\n",
        "# currently not used ... \n",
        "def rankDict(DIN):\n",
        "    '''\n",
        "    reading a frequency dictionary from a file\n",
        "    '''\n",
        "    DTermRanks = {}\n",
        "    i = 0\n",
        "    IRank = 0\n",
        "    IPrevFrq = 0\n",
        "    SumRanks = 0\n",
        "    for SKey, Frq in DIN.items():\n",
        "        # if re.match('#', SKey): continue # skipping comments\n",
        "        i+=1\n",
        "        if IPrevFrq != Frq: IRank = i # rank is the number of the highest ranking element of the same frequency group\n",
        "        IPrevFrq = Frq\n",
        "        \n",
        "        DTermRanks[SKey] = IRank\n",
        "        SumRanks += IRank\n",
        "\n",
        "    AAveRank = SumRanks / i\n",
        "    print(f'MaxRank = {IRank}\\nAve Rank = {AAveRank}\\n')\n",
        "    return DTermRanks, AAveRank\n",
        "\n",
        "\n",
        "# Main evaluation function\n",
        "# One-directional comparision of dictionaries\n",
        "# one-directional comparison of two dictionaries; arguments: DGoldStandard (smaller) DTest (larger), file: GS items found in DTest; GS items missing from DText...\n",
        "# usually testing: smaller vs. bigger dictionaries\n",
        "def countIntersectDictionaries(DGS, DTest, FOutputPrecFOUND, FOutputPrecMISSING, SortBy = 0, Rev = False):\n",
        "    '''\n",
        "    general function: intersect dictionaries, return new intersection dictionaries, record \"in\" and \"out\" expressions\n",
        "    \n",
        "    3b: intersecting All possible MWEs in GS list with the \"Extracted\" list\n",
        "    DA (smaller and going over each element) with D1W / DMWE lists \n",
        "    '''\n",
        "\n",
        "    print('Total len of Gold Standard: ' + str(len(DGS.items())))\n",
        "    IFound = 0\n",
        "    IMissing = 0\n",
        "    SumFoundRanks = 0\n",
        "    DFound = {} # intersection dictionary\n",
        "\n",
        "    for Word, Frq in sorted(DGS.items(),  key=lambda x: x[SortBy], reverse=Rev):\n",
        "        if Word in DTest:\n",
        "            IFound += 1\n",
        "            try: # normally will not fire: if this word already exists with some rank, calculate the average of a new and old rank\n",
        "                r0 = DFound[Word]\n",
        "                r1 = DTest[Word]\n",
        "                r = (r0+r1)/2\n",
        "                DFound[Word] = r\n",
        "                print('r?')\n",
        "            except: # normal route: find the rank of the word in the dictionary\n",
        "                DFound[Word] = DTest[Word]\n",
        "\n",
        "            SumFoundRanks += DTest[Word] # add rank, to calculate average\n",
        "            try: FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + str(DFound[Word]) + '\\n') # record/calculate average rank, etc.\n",
        "            except: \n",
        "                FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "                print(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "        else:\n",
        "            IMissing += 1\n",
        "            FOutputPrecMISSING.write(Word + '\\t' + str(Frq) + '\\n') # record/calculate average rank, etc.\n",
        "\n",
        "    \n",
        "    # print(f'Found: {IFound}')\n",
        "    # print(f'Missing: {IMissing}')\n",
        "    try: ACoverage = IFound / len(DGS.items())\n",
        "    except: ACoverage = 0\n",
        "    # print(f'Found2LenGS: {ACoverage}')\n",
        "    try: AAverageFoundRanks = SumFoundRanks / IFound\n",
        "    except: AAverageFoundRanks = 0\n",
        "    # print(f'Ave Found Ranks: {AAverageFoundRanks} \\n')\n",
        "\n",
        "    print(f'Found: {IFound} ; Missing: {IMissing} ; AveRank: {AAverageFoundRanks} ; ACoverage: {ACoverage} ')\n",
        "    FOutputPrecFOUND.flush()\n",
        "    FOutputPrecMISSING.flush()\n",
        "\n",
        "    return ACoverage, AAverageFoundRanks, DFound\n",
        "\n",
        "\n",
        "\n",
        "# extracting annotated terms from the gold standard in xml format\n",
        "def vertCollectAnnotation(FInVert, SXmlTag, Caps = False):\n",
        "    L3AnnotatedSegs = []\n",
        "    L2Seg = [] # a list of the current segment -- eash string is added \n",
        "    BInTerm = False # boolean flag: inside / outside term\n",
        "    RTagOpen = re.compile('<' + SXmlTag + '>')\n",
        "    RTagClose = re.compile('</' + SXmlTag + '>')\n",
        "    for SLine in FInVert:\n",
        "        SLine = SLine.strip()\n",
        "        if Caps:\n",
        "            SLine = SLine.upper()\n",
        "        if re.match(RTagOpen, SLine):\n",
        "            BInTerm = True\n",
        "        elif re.match(RTagClose, SLine):\n",
        "            BInTerm = False\n",
        "            L3AnnotatedSegs.append(L2Seg)\n",
        "            L2Seg = []\n",
        "        else:\n",
        "            if BInTerm == True:\n",
        "                LFields = re.split('\\t', SLine)\n",
        "                L2Seg.append(LFields)\n",
        "\n",
        "    return L3AnnotatedSegs\n",
        "\n",
        "\n",
        "# converting in-text annotation (e.g., in the bracket form) into proper XML format\n",
        "def convertBrecket2Xml(FInAnnot, FOutAnnot, RInOpen, RInClose, SOutOpen, SOutClose):\n",
        "    RCOpen = re.compile(RInOpen)\n",
        "    RCClose = re.compile(RInClose)\n",
        "    for SLine in FInAnnot:\n",
        "        SLine.strip()\n",
        "        SLine = re.sub(RCOpen, SOutOpen, SLine)\n",
        "        SLine = re.sub(RCClose, SOutClose, SLine)\n",
        "\n",
        "        FOutAnnot.write(SLine + '\\n')\n",
        "    FOutAnnot.flush()\n",
        "    return\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbsgvYg6YcO"
      },
      "source": [
        "# dowloading and creating necessary files\n",
        "# term extraction will be integrated here"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3RKlelR6jx"
      },
      "source": [
        "## preparing for TreeTagger processing (do not have to re-run again once completed)\n",
        "## Annotated Gold Standard\n",
        "# Stage 1: Preparing Gold standard: Reading / extracting information from gold standard: creating a list of annotated terms\n",
        "# set 1 (same text annotated by two annotators)\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Astghik.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Maia.txt\n",
        "\n",
        "# set 2 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Hayk.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Nino.txt\n",
        "\n",
        "# set 3 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
        "!mv index.html?dl=1 BGH3_s00Tamar.txt\n",
        "# !wget \n",
        "# !mv index.html?dl=1\n",
        "\n",
        "# one more will be added: Frau Khachatryan\n",
        "!cat BGH1_s00Astghik.txt BGH1_s00Maia.txt BGH2_s00Hayk.txt BGH2_s00Nino.txt BGH3_s00Tamar.txt >BGH0_s00GoldStandard.txt\n",
        "\n",
        "FInBGH0_s00GoldStandard = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "FOutBGH0_s00GoldStandard = open('BGH0_s00GoldStandard_xml.txt', 'w')\n",
        "\n",
        "convertBrecket2Xml(FInBGH0_s00GoldStandard, FOutBGH0_s00GoldStandard, '<<+', '>>+', '<TERM>', '</TERM>')\n",
        "# this result is pos-tagged and uploaded in the next step\n",
        "# command:\n",
        "# tree-tagger-de.sh /Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_xml.txt >/Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_LEM.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHL3gREljDnQ"
      },
      "source": [
        "# downloading PoS-tagged files after TT processing\n",
        "# !wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# Vahram's file, manually checked (some breckets remaining...)\n",
        "!wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# alternative file created in this workflow\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d8f1bb53632d40538e0d/?dl=1\n",
        "\n",
        "!mv index.html?dl=1 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZQvNF5V6wjB",
        "outputId": "f0da6445-c067-4ed2-a5c4-70c632422d9f"
      },
      "source": [
        "!head --lines=10 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TERM>\r\n",
            "BUNDESGERICHTSHOF\tNN\tBundesgerichtshof\r\n",
            "</TERM>\r\n",
            "IM\tNE\tIM\r\n",
            "NAMEN\tNN\tName\r\n",
            "DES\tART\tdie\r\n",
            "VOLKES\tNN\tVolk\r\n",
            "<TERM>\r\n",
            "URTEIL\tNN\tUrteil\r\n",
            "</TERM>\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQNpab5JrOn"
      },
      "source": [
        "# read / generate all the necessary texts\n",
        "# first define functions, then download files and read them into dictionaries...\n",
        "\n",
        "# further versions of the Gold Standard: \n",
        "# Annotated and PoS-tagged Gold Standard -- for extraction of the correct evaluation set\n",
        "\n",
        "# printing words of different length\n",
        "FOutput = open('BGH0_s01GoldS_Terms.txt', 'w')\n",
        "FOutputDict1w = open('BGH0_s01GoldS_D1w.txt', 'w') # 1-word terms\n",
        "FOutputDict2w = open('BGH0_s01GoldS_D2w.txt', 'w') # 2-word terminological expressions\n",
        "FOutputDict3w = open('BGH0_s01GoldS_D3w.txt', 'w') # 3-word terminological expressions\n",
        "FOutputDict4w = open('BGH0_s01GoldS_D4w.txt', 'w') # more than 3 words\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA9UNO0NZbt0"
      },
      "source": [
        "# reading datasets\n",
        "# list of gold-standard annotated terms, with lemmatization and pos fileds\n",
        "FInBGH0_s00GS_LEM = open('BGH0_s00GS_LEM.txt', 'r')\n",
        "# FOutBGH0_s00GS_Terms = open('BGH0_s00GS_Terms.txt', 'w')\n",
        "L3AnnotatedSegs = vertCollectAnnotation(FInBGH0_s00GS_LEM, 'TERM', Caps = True)\n",
        "\n",
        "# testing the file read\n",
        "# for LSegment in L3AnnotatedSegs: FOutBGH0_s00GS_Terms.write(str(LSegment) + '\\n')\n",
        "# FOutBGH0_s00GS_Terms.flush()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPE39kkEAOwZ",
        "outputId": "cd2992d1-cec6-4551-f23c-fe3d1bd1d5f0"
      },
      "source": [
        "L3AnnotatedSegs[10]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['OBERSTAATSANWALT', 'NN', 'OBERSTAATSANWALT'],\n",
              " ['BEIM', 'APPRART', 'BEI'],\n",
              " ['BUNDESGERICHTSHOF', 'NN', 'BUNDESGERICHTSHOF']]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3_PKbspAYMD"
      },
      "source": [
        "def createDictOfPatterns(L3AnnotatedSegs, IFieldN, Normalize = False):\n",
        "    '''\n",
        "    take the list of annotated terms and return a dictionary of MWEs\n",
        "    '''\n",
        "    DPatternsFrq = {} # returned dictionary of PoS patterns, etc.\n",
        "    for L2TermFlds in L3AnnotatedSegs:\n",
        "        LFlds = [] # here we will collect the values of the selected field\n",
        "        for LWordFlds in L2TermFlds:\n",
        "            if Normalize:\n",
        "                try: PoS = LWordFlds[IFieldN]\n",
        "                except: \n",
        "                    print('PoS not found')\n",
        "                    PoS = ''\n",
        "                if re.match('N', PoS): LWordFlds[IFieldN] = 'N'\n",
        "                if re.match('ADJ', PoS): LWordFlds[IFieldN] = 'ADJ'\n",
        "                if re.match('V', PoS): LWordFlds[IFieldN] = 'V'\n",
        "            try: LFlds.append(LWordFlds[IFieldN])\n",
        "            except: print('index Error')\n",
        "        if len(LFlds) > 0: SFlds = ' '.join(LFlds)\n",
        "        try: DPatternsFrq[SFlds] += 1\n",
        "        except: DPatternsFrq[SFlds] = 1\n",
        "\n",
        "    return DPatternsFrq\n",
        "\n",
        "DPatternsFrq = createDictOfPatterns(L3AnnotatedSegs, 1, Normalize = False)\n",
        "FOutTermPOS = open('BGH0_s00GoldStandard_pos.txt', 'w')\n",
        "\n",
        "printDictionary(DPatternsFrq, FOutTermPOS)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX29G8r3Zo6-"
      },
      "source": [
        "del DTermConfFrq"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_kJbRh3TDa",
        "outputId": "26b8bc41-b975-4e2a-fc0a-54880e1dda94"
      },
      "source": [
        "# L3AnnotatedSegs[10]\n",
        "# FInput = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "# for statistical purposes - separately single and multiword terms\n",
        "class ContinueI(Exception):\n",
        "    pass\n",
        "\n",
        "continue_i = ContinueI()\n",
        "\n",
        "def comparePattern(L2TermFlds, LPattern, IFldN):\n",
        "    '''\n",
        "    compares if a pattern is found in the term field\n",
        "    '''\n",
        "    for k in range(len(LPattern)):\n",
        "        if re.match(LPattern[k], L2TermFlds[k][IFldN]): continue\n",
        "        else: return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None,  SplitLen = False, Caps = True, IFldNumber = 0):\n",
        "    '''\n",
        "    function: 1. selects terms which match specified POS pattern; 2. divides them into dictionaries according to length\n",
        "    the function can also visualise terms with a specific pos pattern, specified in L2Patterns, e.g., L2Patterns = [['N', '\\$']]\n",
        "\n",
        "    '''\n",
        "    DGS = {}\n",
        "    DGS1w = {} # dictionary of single words\n",
        "    DGS2w = {} # dictionary of 2-word expressions\n",
        "    DGS3w = {} # dictionary of 3-word expressions\n",
        "    DGS4w = {} # dictionary of other mwes\n",
        "    IGS = 0\n",
        "    IGS1w = 0 # number of annotated tokens of single words\n",
        "    IGS2w = 0\n",
        "    IGS3w = 0\n",
        "    IGS4w = 0 # number of annotated tokens of multiwords\n",
        "\n",
        "    if L2Patterns: # positive filter\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs: # for each multiword term, where words are represented as fields\n",
        "            ILenTerm = len(L2AnnotatedSeg)\n",
        "            for LPattern in L2Patterns:\n",
        "                if len(LPattern) == ILenTerm and comparePattern(L2AnnotatedSeg, LPattern, 1):\n",
        "                    LTerm = []\n",
        "                    for LTerm2Fields in L2AnnotatedSeg:\n",
        "                        LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                    STerm = ' '.join(LTerm)\n",
        "                    try: DGS[STerm] += 1\n",
        "                    except: DGS[STerm] = 1\n",
        "\n",
        "    else: # negative filter checking\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs:\n",
        "            if not L2AnnotatedSeg: continue\n",
        "            try: SEnd = L2AnnotatedSeg[-1][1]\n",
        "            except: print('index: L2AnnotatedSeg - end' + str(L2AnnotatedSeg))\n",
        "\n",
        "            try: SStart = L2AnnotatedSeg[0][1]\n",
        "            except: print('index: L2AnnotatedSeg - start' + str(L2AnnotatedSeg))\n",
        "            \n",
        "            try:\n",
        "                if LNoEdge: # PoS which cannot apper at the edge\n",
        "                    for SPoS in LNoEdge:\n",
        "                        if re.match(SPoS, SEnd) or re.match(SPoS, SStart):\n",
        "                            # print('edge: ' + SPoS + ' ' + SStart  + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                if LNoStart:\n",
        "                    for SPoS in LNoStart:\n",
        "                        if re.match(SPoS, SStart): \n",
        "                            # print('start: ' + SPoS + ' ' + SStart)\n",
        "                            raise continue_i\n",
        "                if L2NoEnd:\n",
        "                    for SPoS in L2NoEnd:\n",
        "                        if re.match(SPoS, SEnd): \n",
        "                            # print('end: ' + SPoS + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                LTerm = []\n",
        "                for LTerm2Fields in L2AnnotatedSeg:\n",
        "                    LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                STerm = ' '.join(LTerm)\n",
        "                try: DGS[STerm] += 1\n",
        "                except: DGS[STerm] = 1\n",
        "\n",
        "            except ContinueI: \n",
        "                continue\n",
        "\n",
        "    if SplitLen:\n",
        "        for GSTerm, Frq in DGS.items():\n",
        "            LGSTErms = re.split(' ', GSTerm)\n",
        "\n",
        "            if len(LGSTErms) > 3:\n",
        "                IGS4w += Frq\n",
        "                try: DGS4w[GSTerm] += Frq\n",
        "                except: DGS4w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 2:\n",
        "                IGS3w += Frq\n",
        "                try: DGS3w[GSTerm] += Frq\n",
        "                except: DGS3w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 1:\n",
        "                IGS2w += Frq\n",
        "                try: DGS2w[GSTerm] += Frq\n",
        "                except: DGS2w[GSTerm] = Frq\n",
        "            else:\n",
        "                IGS1w += Frq\n",
        "                try: DGS1w[GSTerm] +=Frq\n",
        "                except: DGS1w[GSTerm] = Frq\n",
        "\n",
        "\n",
        "    print (IGS1w, IGS2w, IGS3w, IGS4w)\n",
        "    return DGS, DGS1w, DGS2w, DGS3w, DGS4w\n",
        "\n",
        "# DTermConfFrq = selectTerms(L3AnnotatedSegs, L2Patterns = [['N', '\\$']], SplitLen = False, IFldNumber = 0)\n",
        "# FOutExamples = open('BGH0_s00GoldStandard_examples_ADJ_N.txt', 'w')\n",
        "\n",
        "# DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ', 'N'], ['N', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "\n",
        "FOutExamples = open('BGH0_s00GoldStandard_examples.txt', 'w')\n",
        "FOutExamples1w = open('BGH0_s00GoldStandard1w_examples.txt', 'w')\n",
        "FOutExamples2w = open('BGH0_s00GoldStandard2w_examples.txt', 'w')\n",
        "FOutExamples3w = open('BGH0_s00GoldStandard3w_examples.txt', 'w')\n",
        "FOutExamples4w = open('BGH0_s00GoldStandard4w_examples.txt', 'w')\n",
        "\n",
        "printDictionary(DTermConfFrq, FOutExamples)\n",
        "printDictionary(DGS1w, FOutExamples1w)\n",
        "printDictionary(DGS2w, FOutExamples2w)\n",
        "printDictionary(DGS3w, FOutExamples3w)\n",
        "printDictionary(DGS4w, FOutExamples4w)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13760 297 0 0\n"
          ]
        }
      ]
    }
  ]
}