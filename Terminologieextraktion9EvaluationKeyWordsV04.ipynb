{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Terminologieextraktion9EvaluationKeyWordsV04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMX2LY9u7LtW6Rd1OylkJIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/Terminologieextraktion9EvaluationKeyWordsV04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Tp03sIjFRh"
      },
      "source": [
        "# Workflow\n",
        "1. legal corpus > extraction of terms\n",
        "- sequences;\n",
        "- sub-sequences and patterns in sequences\n",
        "- filtering for what has been extracted >> the same way as from annotation\n",
        "- selecting the lemma field\n",
        "\n",
        "2. Evaluation set (automatically annotating gold standard) (extracting candidates and automatically applying extraction based on Stage 1\n",
        "\n",
        "3. gold standard terms (manual annotation);\n",
        "- annotation\n",
        "- extracting annotation:\n",
        "- selecting lemma field;\n",
        "- filtering by specific patterns\n",
        "\n",
        "\n",
        "4. p/r measure -- baseline (max recall)\n",
        "\n",
        "5. filtering: \n",
        "- keyness;\n",
        "- association measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9V2CNrSnkmW"
      },
      "source": [
        "## Part 0: global declarations\n",
        "\n",
        "Libraries, functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L6lXh0UnjUN"
      },
      "source": [
        "import os, re, sys"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqvEq6IGGfv1"
      },
      "source": [
        "## to modify if necessary; however, we try to keep the code standard and parametrize as much as possible\n",
        "\n",
        "# useful functions\n",
        "# a useful function for recording / visualising current stage of dictionaries\n",
        "def printDictionary(DictionaryFrq, FOut, K = 1, Rev = True): # printing a dictionary: by values or alphabetically\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[K], reverse=Rev):\n",
        "        FOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    FOut.flush()\n",
        "    return\n",
        "\n",
        "# another useful function to just read and return a 2-field dictionary, eg., frequency or keyness\n",
        "def readDictionary(FIN, SkipComments = True, Caps=False):\n",
        "    DScoresLarge = {} # keywords - scores\n",
        "    for Line in FIN:\n",
        "        if SkipComments and re.match('#', Line): \n",
        "            continue\n",
        "        Line = Line.strip()\n",
        "        if Caps: \n",
        "            Line = Line.upper() # convert to upper case\n",
        "        LFieldsKW = re.split('\\t', Line)\n",
        "        SWord = LFieldsKW[0]\n",
        "        AKScore = float(LFieldsKW[1])\n",
        "        DScoresLarge[SWord] = AKScore   \n",
        "    return DScoresLarge\n",
        "\n",
        "# another possibly useful function: convert dictionary values to ranks (frequency, keyness weights, etc.)\n",
        "# for understanding how far down the list the item has been found...\n",
        "# currently not used ... \n",
        "def rankDict(DIN):\n",
        "    '''\n",
        "    reading a frequency dictionary from a file\n",
        "    '''\n",
        "    DTermRanks = {}\n",
        "    i = 0\n",
        "    IRank = 0\n",
        "    IPrevFrq = 0\n",
        "    SumRanks = 0\n",
        "    for SKey, Frq in DIN.items():\n",
        "        # if re.match('#', SKey): continue # skipping comments\n",
        "        i+=1\n",
        "        if IPrevFrq != Frq: IRank = i # rank is the number of the highest ranking element of the same frequency group\n",
        "        IPrevFrq = Frq\n",
        "        \n",
        "        DTermRanks[SKey] = IRank\n",
        "        SumRanks += IRank\n",
        "\n",
        "    AAveRank = SumRanks / i\n",
        "    print(f'MaxRank = {IRank}\\nAve Rank = {AAveRank}\\n')\n",
        "    return DTermRanks, AAveRank\n",
        "\n",
        "\n",
        "# Main evaluation function\n",
        "# One-directional comparision of dictionaries\n",
        "# one-directional comparison of two dictionaries; arguments: DGoldStandard (smaller) DTest (larger), file: GS items found in DTest; GS items missing from DText...\n",
        "# usually testing: smaller vs. bigger dictionaries\n",
        "def countIntersectDictionaries(DGS, DTest, FOutputPrecFOUND, FOutputPrecMISSING, SortBy = 0, Rev = False):\n",
        "    '''\n",
        "    general function: intersect dictionaries, return new intersection dictionaries, record \"in\" and \"out\" expressions\n",
        "    \n",
        "    3b: intersecting All possible MWEs in GS list with the \"Extracted\" list\n",
        "    DA (smaller and going over each element) with D1W / DMWE lists \n",
        "    '''\n",
        "\n",
        "    print('Total len of Gold Standard: ' + str(len(DGS.items())))\n",
        "    IFound = 0\n",
        "    IMissing = 0\n",
        "    SumFoundRanks = 0\n",
        "    DFound = {} # intersection dictionary\n",
        "\n",
        "    for Word, Frq in sorted(DGS.items(),  key=lambda x: x[SortBy], reverse=Rev):\n",
        "        if Word in DTest:\n",
        "            IFound += 1\n",
        "            try: # normally will not fire: if this word already exists with some rank, calculate the average of a new and old rank\n",
        "                r0 = DFound[Word]\n",
        "                r1 = DTest[Word]\n",
        "                r = (r0+r1)/2\n",
        "                DFound[Word] = r\n",
        "                print('r?')\n",
        "            except: # normal route: find the rank of the word in the dictionary\n",
        "                DFound[Word] = DTest[Word]\n",
        "\n",
        "            SumFoundRanks += DTest[Word] # add rank, to calculate average\n",
        "            try: FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + str(DFound[Word]) + '\\n') # record/calculate average rank, etc.\n",
        "            except: \n",
        "                FOutputPrecFOUND.write(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "                print(Word + '\\t' + str(Frq) + '\\t' + 'KEY ERROR' + '\\n')\n",
        "        else:\n",
        "            IMissing += 1\n",
        "            FOutputPrecMISSING.write(Word + '\\t' + str(Frq) + '\\n') # record/calculate average rank, etc.\n",
        "\n",
        "    \n",
        "    # print(f'Found: {IFound}')\n",
        "    # print(f'Missing: {IMissing}')\n",
        "    try: ACoverage = IFound / len(DGS.items())\n",
        "    except: ACoverage = 0\n",
        "    # print(f'Found2LenGS: {ACoverage}')\n",
        "    try: AAverageFoundRanks = SumFoundRanks / IFound\n",
        "    except: AAverageFoundRanks = 0\n",
        "    # print(f'Ave Found Ranks: {AAverageFoundRanks} \\n')\n",
        "\n",
        "    print(f'Found: {IFound} ; Missing: {IMissing} ; AveRank: {AAverageFoundRanks} ; ACoverage: {ACoverage} ')\n",
        "    FOutputPrecFOUND.flush()\n",
        "    FOutputPrecMISSING.flush()\n",
        "\n",
        "    return ACoverage, AAverageFoundRanks, DFound\n",
        "\n",
        "\n",
        "\n",
        "# extracting annotated terms from the gold standard in xml format\n",
        "def vertCollectAnnotation(FInVert, SXmlTag, Caps = False):\n",
        "    L3AnnotatedSegs = []\n",
        "    L2Seg = [] # a list of the current segment -- eash string is added \n",
        "    BInTerm = False # boolean flag: inside / outside term\n",
        "    RTagOpen = re.compile('<' + SXmlTag + '>')\n",
        "    RTagClose = re.compile('</' + SXmlTag + '>')\n",
        "    for SLine in FInVert:\n",
        "        SLine = SLine.strip()\n",
        "        if Caps:\n",
        "            SLine = SLine.upper()\n",
        "        if re.match(RTagOpen, SLine):\n",
        "            BInTerm = True\n",
        "        elif re.match(RTagClose, SLine):\n",
        "            BInTerm = False\n",
        "            L3AnnotatedSegs.append(L2Seg)\n",
        "            L2Seg = []\n",
        "        else:\n",
        "            if BInTerm == True:\n",
        "                LFields = re.split('\\t', SLine)\n",
        "                L2Seg.append(LFields)\n",
        "\n",
        "    return L3AnnotatedSegs\n",
        "\n",
        "\n",
        "\n",
        "# converting in-text annotation (e.g., in the bracket form) into proper XML format\n",
        "def convertBrecket2Xml(FInAnnot, FOutAnnot, RInOpen, RInClose, SOutOpen, SOutClose):\n",
        "    RCOpen = re.compile(RInOpen)\n",
        "    RCClose = re.compile(RInClose)\n",
        "    for SLine in FInAnnot:\n",
        "        SLine.strip()\n",
        "        SLine = re.sub(RCOpen, SOutOpen, SLine)\n",
        "        SLine = re.sub(RCClose, SOutClose, SLine)\n",
        "\n",
        "        FOutAnnot.write(SLine + '\\n')\n",
        "    FOutAnnot.flush()\n",
        "    return\n",
        "\n",
        "# a service function for creating a dictionary of field values\n",
        "# is used for creating a dictionary of PoS patterns\n",
        "# can destructively change the list, if Normalize = 2 (changing PoS codes as specified inside the function)\n",
        "def createDictOfPatterns(L3AnnotatedSegs, IFieldN, Normalize = 0):\n",
        "    '''\n",
        "    take the list of annotated terms and return a dictionary of MWEs\n",
        "    Normalize = 0 : do not normalize;\n",
        "              = 1: normalize, but do not change the original list\n",
        "              = 2: normalize and change the original list\n",
        "    '''\n",
        "    DPatternsFrq = {} # returned dictionary of PoS patterns, etc.\n",
        "    for L2TermFlds in L3AnnotatedSegs:\n",
        "        LFlds = [] # here we will collect the values of the selected field\n",
        "        for LWordFlds in L2TermFlds:\n",
        "            if Normalize < 2:\n",
        "                LWordFlds0 = [] # making a copy of the list, not to modify original if Normalize is specified, or is it ok to normalize\n",
        "                LWordFlds0.extend(LWordFlds)\n",
        "            elif Normalize == 2:\n",
        "                LWordFlds0 = LWordFlds # just use a reference to the same list\n",
        "            \n",
        "            if Normalize > 0:\n",
        "                try: PoS = LWordFlds0[IFieldN]\n",
        "                except: \n",
        "                    print('PoS not found')\n",
        "                    PoS = ''\n",
        "                if re.match('N', PoS): LWordFlds0[IFieldN] = 'N'\n",
        "                if re.match('ADJ', PoS): LWordFlds0[IFieldN] = 'ADJ'\n",
        "                if re.match('V', PoS): LWordFlds0[IFieldN] = 'V'\n",
        "            try: LFlds.append(LWordFlds0[IFieldN])\n",
        "            except: print('index Error')\n",
        "        if len(LFlds) > 0: SFlds = ' '.join(LFlds)\n",
        "        try: DPatternsFrq[SFlds] += 1\n",
        "        except: DPatternsFrq[SFlds] = 1\n",
        "\n",
        "    return DPatternsFrq\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# change case for elements of the list\n",
        "def changeCaseL3(L3Segs, LFlds2Caps = [0, 2], Mode='upper', StripB = True):\n",
        "    '''\n",
        "    Mode = upper -- to all caps;\n",
        "        = lower -- to lowercase;\n",
        "        = capitalize -- to sentence case;\n",
        "    '''\n",
        "    for L2Seg in L3Segs: # for each multiword term in the list of terms\n",
        "        for LWordFlds in L2Seg: # for each word in the list of words in the multiword term\n",
        "            for IFld in LFlds2Caps: # for each field that needs to change case\n",
        "                SWord = LWordFlds[IFld]\n",
        "                if StripB: SWord = SWord.strip('<>')\n",
        "                if Mode == 'upper': \n",
        "                    SWord = SWord.upper()\n",
        "                if Mode == 'lower':\n",
        "                    SWord = SWord.lower()\n",
        "                if Mode == 'capitalize':\n",
        "                    SWord = SWord.capitalize()\n",
        "                LWordFlds[IFld] = SWord\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "def readDictKWAnnotations(FInputKW, AKStatThreshold = 1, Caps=True):\n",
        "    '''\n",
        "    reading a keyword file, returning a dictionary of keywords / not keywords\n",
        "    AKStatThreshold = 1 (only sure keywords)\n",
        "                    = 0.5 (unsure keywords)\n",
        "                    = 0 (all annotated keywords)\n",
        "    '''\n",
        "    DScoresKW = {} # keywords - scores\n",
        "    DScoresNK = {} # non-keywords\n",
        "    DStatKW = {} # status: key/non-key-word\n",
        "    for Line in FInputKW:\n",
        "        LFieldsKW = re.split('\\t', Line) # add: strip()\n",
        "        SWord = LFieldsKW[1]\n",
        "        if Caps: SWord = SWord.upper()\n",
        "        AKScore = float(LFieldsKW[2])\n",
        "        AKStat = float(LFieldsKW[3])\n",
        "        DStatKW[SWord] = AKStat\n",
        "        if AKStat >= AKStatThreshold: # change value to 0.5 if we need to restrict to 'sure' terms only (value 1)\n",
        "            DScoresKW[SWord] = AKScore\n",
        "        else:\n",
        "            DScoresNK[SWord] = AKScore\n",
        "    return DScoresKW, DScoresNK\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk7FkvbXvQHE"
      },
      "source": [
        "# main functions -- will be used also in extraction process\n",
        "# L3AnnotatedSegs[10]\n",
        "# FInput = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "# for statistical purposes - separately single and multiword terms\n",
        "\n",
        "# function(s) for selecting patterns in a list of candidates; \n",
        "# these can be either positive patterns, or, if postive are not specified, then negative pattenrs (start, edge or end restrictions on PoS codes)\n",
        "\n",
        "class ContinueI(Exception):\n",
        "    pass\n",
        "\n",
        "continue_i = ContinueI()\n",
        "\n",
        "def comparePattern(L2TermFlds, LPattern, IFldN):\n",
        "    '''\n",
        "    compares if a pattern is found in the term field\n",
        "    '''\n",
        "    for k in range(len(LPattern)):\n",
        "        if re.match(LPattern[k], L2TermFlds[k][IFldN]): continue\n",
        "        else: return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None,  SplitLen = False, IFldNumber = 0):\n",
        "    '''\n",
        "    function: 1. selects terms which match specified POS pattern; 2. divides them into dictionaries according to length\n",
        "    the function can also visualise terms with a specific pos pattern, specified in L2Patterns, e.g., L2Patterns = [['N', '\\$']]\n",
        "\n",
        "    '''\n",
        "    DGS = {}\n",
        "    DGS1w = {} # dictionary of single words\n",
        "    DGS2w = {} # dictionary of 2-word expressions\n",
        "    DGS3w = {} # dictionary of 3-word expressions\n",
        "    DGS4w = {} # dictionary of other mwes\n",
        "    IGS = 0\n",
        "    IGS1w = 0 # number of annotated tokens of single words\n",
        "    IGS2w = 0\n",
        "    IGS3w = 0\n",
        "    IGS4w = 0 # number of annotated tokens of multiwords\n",
        "    ISelectedTermsCount = 0\n",
        "    L3SelectedTerms = [] # return not only dictionaries, but also a list of the selected terms, in the order how they apper\n",
        "\n",
        "    if L2Patterns: # positive filter\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs: # for each multiword term, where words are represented as fields\n",
        "            ILenTerm = len(L2AnnotatedSeg)\n",
        "            for LPattern in L2Patterns:\n",
        "                if len(LPattern) == ILenTerm and comparePattern(L2AnnotatedSeg, LPattern, 1):\n",
        "                    L3SelectedTerms.append(L2AnnotatedSeg) # last addition: adding the term to the list, if it matches the selected pattern\n",
        "                    ISelectedTermsCount += 1\n",
        "                    LTerm = []\n",
        "                    for LTerm2Fields in L2AnnotatedSeg:\n",
        "                        LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                    STerm = ' '.join(LTerm)\n",
        "                    try: DGS[STerm] += 1\n",
        "                    except: DGS[STerm] = 1\n",
        "\n",
        "    else: # negative filter checking\n",
        "        for L2AnnotatedSeg in L3AnnotatedSegs:\n",
        "            if not L2AnnotatedSeg: continue\n",
        "            try: SEnd = L2AnnotatedSeg[-1][1]\n",
        "            except: print('index: L2AnnotatedSeg - end' + str(L2AnnotatedSeg))\n",
        "\n",
        "            try: SStart = L2AnnotatedSeg[0][1]\n",
        "            except: print('index: L2AnnotatedSeg - start' + str(L2AnnotatedSeg))\n",
        "            \n",
        "            try:\n",
        "                if LNoEdge: # PoS which cannot apper at the edge\n",
        "                    for SPoS in LNoEdge:\n",
        "                        if re.match(SPoS, SEnd) or re.match(SPoS, SStart):\n",
        "                            # print('edge: ' + SPoS + ' ' + SStart  + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                if LNoStart:\n",
        "                    for SPoS in LNoStart:\n",
        "                        if re.match(SPoS, SStart): \n",
        "                            # print('start: ' + SPoS + ' ' + SStart)\n",
        "                            raise continue_i\n",
        "                if L2NoEnd:\n",
        "                    for SPoS in L2NoEnd:\n",
        "                        if re.match(SPoS, SEnd): \n",
        "                            # print('end: ' + SPoS + ' ' + SEnd)\n",
        "                            raise continue_i\n",
        "                L3SelectedTerms.append(L2AnnotatedSeg) # last addition: the term passed the negative filter, and is added to the list\n",
        "                ISelectedTermsCount += 1\n",
        "                LTerm = []\n",
        "                for LTerm2Fields in L2AnnotatedSeg:\n",
        "                    LTerm.append(LTerm2Fields[IFldNumber])\n",
        "                STerm = ' '.join(LTerm)\n",
        "                try: DGS[STerm] += 1\n",
        "                except: DGS[STerm] = 1\n",
        "\n",
        "            except ContinueI: \n",
        "                continue\n",
        "\n",
        "    if SplitLen:\n",
        "        for GSTerm, Frq in DGS.items():\n",
        "            LGSTErms = re.split(' ', GSTerm)\n",
        "\n",
        "            if len(LGSTErms) > 3:\n",
        "                IGS4w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS4w[GSTerm] += Frq\n",
        "                except: DGS4w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 2:\n",
        "                IGS3w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS3w[GSTerm] += Frq\n",
        "                except: DGS3w[GSTerm] = Frq\n",
        "            elif len(LGSTErms) > 1:\n",
        "                IGS2w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS2w[GSTerm] += Frq\n",
        "                except: DGS2w[GSTerm] = Frq\n",
        "            else:\n",
        "                IGS1w += Frq\n",
        "                IGS += Frq\n",
        "                try: DGS1w[GSTerm] +=Frq\n",
        "                except: DGS1w[GSTerm] = Frq\n",
        "\n",
        "\n",
        "    print(IGS1w, IGS2w, IGS3w, IGS4w, IGS, ISelectedTermsCount)\n",
        "    print(len(DGS1w), len(DGS2w), len(DGS3w), len(DGS4w), len(DGS), len(L3SelectedTerms))\n",
        "    return DGS, DGS1w, DGS2w, DGS3w, DGS4w, L3SelectedTerms\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x08Plvp9F_mA"
      },
      "source": [
        "# End: Stage 0: Some useful read/write and convert functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj6O1sU2kqTE"
      },
      "source": [
        "## Part 1 - Legal corpus: Terminology extraction\n",
        "\n",
        "This part of the workflow uses large files and may run for up to 20 minutes, needs to be run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-sZk6UMjtLo",
        "outputId": "90d0372a-ae62-450a-f7f1-11ff61fef473"
      },
      "source": [
        "# Stage 1: preparing terminology extraction workflow (~ 1 min, but may run longer)\n",
        "# 23.10.2021 part\n",
        "# German legal corpus, lemmatized in a zip archive (archive = 641 MB in zip archive)\n",
        "!wget https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
        "# renaming file ()\n",
        "!mv index.html?dl=1 BGH-utf8-lem.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 05:48:52--  https://heibox.uni-heidelberg.de/f/fd96c36723b741d4a972/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/d582f9b9-1698-479f-97ce-59ae7f79d825/output-utf8-lem.zip [following]\n",
            "--2021-11-05 05:48:52--  https://heibox.uni-heidelberg.de/seafhttp/files/d582f9b9-1698-479f-97ce-59ae7f79d825/output-utf8-lem.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 673144348 (642M) [application/zip]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 641.96M  13.4MB/s    in 43s     \n",
            "\n",
            "2021-11-05 05:49:36 (14.8 MB/s) - ‘index.html?dl=1’ saved [673144348/673144348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYrGkFzlCT9",
        "outputId": "f7748336-c190-440a-b23a-8c6e93cf35cc"
      },
      "source": [
        "# extraction ~ 1 min\n",
        "!unzip BGH-utf8-lem.zip\n",
        "!rm BGH-utf8-lem.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BGH-utf8-lem.zip\n",
            "  inflating: output-utf8-lem.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q-ueX-olCIA",
        "outputId": "ec0a3331-f0ae-4137-ea79-fd73b923eff8"
      },
      "source": [
        "!head --lines=10 output-utf8-lem.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<doc id=\"t1000001\">\n",
            "Nachschlagewerk\tNN\tNachschlagewerk\n",
            ":\t$.\t:\n",
            "ja\tADV\tja\n",
            "BGHSt\tVVFIN\tBGHSt\n",
            ":\t$.\t:\n",
            "nein\tPTKANT\tnein\n",
            "Veröffentlichung\tNN\tVeröffentlichung\n",
            ":\t$.\t:\n",
            "ja\tADV\tja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPk7DMolniZ"
      },
      "source": [
        "!mv output-utf8-lem.txt BGHlem.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJC2x26NlvNn"
      },
      "source": [
        "!head --lines=4000000 BGHlem.txt >BGHlem1k.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t09pTHtl4jr",
        "outputId": "3cf30f8d-0c57-47ec-c9db-fc1bc8d63912"
      },
      "source": [
        "# OPTIONAL -- just to know how big is our corpus, on a long corpus it can take a lot of time...\n",
        "# word counts: BGHlem.txt should be ~221 M lines long (vert lemmatized format; so one line is one word); runs ~ 1min\n",
        "!wc BGHlem1k.txt\n",
        "!wc BGHlem.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 4000000 11990777 66682067 BGHlem1k.txt\n",
            " 220915764  662340855 3684207041 BGHlem.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ6jfi_MmPP2"
      },
      "source": [
        "# OPTIONAL -- just to demonstrate the component of the algorithm of term selection\n",
        "# debugging and explaining the algorithm for Stage 1:\n",
        "# taking the maximal possilbe PoS string; \n",
        "# sub-dividing it into other reasonable strings --> term candidates;\n",
        "# filtering only possible / interesting patterns / terms matching these patterns\n",
        "LLCandidates = []\n",
        "LWords = ['Revisionsverfahren', 'entstanden', 'notwendig', 'Auslage']\n",
        "for klen in range(len(LWords)): # lengths of candidate lists\n",
        "    klength = klen+1 # true length: for 0 it is le = 1\n",
        "    print(f'klen:{klength};')\n",
        "    for i in range(len(LWords) - klen): # positions where candidates start\n",
        "        print(f'i:{i};')\n",
        "        LCandidate = LWords[i:i+klength]\n",
        "        LLCandidates.append(LCandidate)\n",
        "        print(LCandidate)\n",
        "\n",
        "        # print(LWords[i])\n",
        "\n",
        "for LEl in LLCandidates:\n",
        "    # for el in LEl:\n",
        "    print(LEl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2Gh3_Sm0Tu"
      },
      "source": [
        "# Terminologieextraktion\n",
        "# main automated pattern-based extraction\n",
        "# todo:\n",
        "# a. change -- lemmas: done\n",
        "# b. BACK TO NEGATIVE FILTER FOR EFFICIENCY \n",
        "# use positive patterns for extraction for candidates (same function as for gold standard)\n",
        "#     -- for this we need to move to collecting a 2D lists -- features\n",
        "#     \n",
        "\n",
        "class clProcCorpus(object):\n",
        "    ''' we will read a text file and return a dictionary\n",
        "    this will be done on the line by line basis\n",
        "    The dictionary can be sorted later...\n",
        "    '''\n",
        "    # this class is processing a corpus\n",
        "\n",
        "    def __init__(self, FileIN):\n",
        "        self.DictFrq = {}\n",
        "        self.processCorpus(FileIN)\n",
        "\n",
        "\n",
        "    def procCorpSelectField4FrqDic(self, IFieldNumber, L2PossibleTerm):\n",
        "        '''\n",
        "        this functions takes a 2D list (just one possible term at a time, with all fields) and returns a string which represents the term -- either as inflected words or lemmas\n",
        "        '''\n",
        "        LSelectedField = []\n",
        "        for LFields in L2PossibleTerm:\n",
        "            LSelectedField.append(LFields[IFieldNumber])\n",
        "        \n",
        "        SSelectedField = ' '.join(LSelectedField)\n",
        "        return SSelectedField\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def processCorpus(self, FileIN):\n",
        "        # here we consider a larger MWE, which has been collected, e.g., \n",
        "        # Wahl Schluckebier Nachschlagewerk; or:\n",
        "        # graphical user interface\n",
        "        # in the default version only the longest string is preserved;\n",
        "        # we try to split it into meaningful smaller units and preserve them as well:\n",
        "        # Wahl Schluckebier\n",
        "        # graphical user ; user interface ; interface\n",
        "        # PoS restrictions apply: \n",
        "        # - Adj cannot be at the end; function words (Prepositions, articles, etc.) cannot be on either edge.\n",
        "        # so we keep the list of tuples: and extract smaller MWEs from the larger strings using PoS restrictions\n",
        "\n",
        "        # Stage 0: collecting the longest string, with allowed PoS codes, e.g., the longest contiuous string of N, Adj, Prep, Gen. articles\n",
        "        LLTerm = [] # this is the List of Lists: list of words (with their fields), representing a sequence of allowed PoS codes\n",
        "        n = 0 # count lines processed (monitoring progress over the corpus)\n",
        "        for Line in FileIN:\n",
        "            n+=1\n",
        "            if n%1000000 == 0: print(n) # print every 1 Millonth line number\n",
        "            Line = Line.strip()\n",
        "            LLine = re.split('\\t', Line)\n",
        "            \n",
        "            try:\n",
        "                Word = LLine[0]\n",
        "                PoS = LLine[1]\n",
        "                Lemma = LLine[2]\n",
        "            except:\n",
        "                Word = \"\"\n",
        "                PoS = \"\"\n",
        "                Lemma = \"\"\n",
        "            \n",
        "            #Select the Tags for your langauge\n",
        "            # update: this is now done in two stages: \n",
        "            # Stage 1: a set of any PoS codes which may contain candidates\n",
        "            # Stage 2: restrictions on sequences (e.g., what is not allowed at the end, start...)\n",
        "            # \n",
        "            # CHANGE HERE FOR YOUR LANGUAGE: stage 1\n",
        "            # if re.match('N.*', PoS) or re.match('A.*', PoS): #Arm\n",
        "            # if re.match('N.*', PoS) or re.match('J.*', PoS): #EN\n",
        "            # if re.match('N.*', PoS) or re.match('ADJ.*', PoS): #DE\n",
        "            # L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['APPR', 'ART','N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']]\n",
        "            if re.match('N.*', PoS) or re.match('ADJ.*', PoS) or re.match('ART.*', PoS) or re.match('APPR.*', PoS): #DE -- extended\n",
        "                LLTerm.append(LLine) # all the field as a list, to form the list of lists\n",
        "\n",
        "                # Terms as Words or Lemmas -- MOVED TO Stage 2\n",
        "                # to compare with the gold standard do we need words, or do we need to generate words from lemas?\n",
        "                # LTerm.append(Lemma)\n",
        "                # here we keep all the fields, because sub-sections of the longer MWE would also be analysed using PoS codes\n",
        "\n",
        "            else: # end of the 'candidate collection'\n",
        "                ### changed to an algorithm based on the data format: list of lists\n",
        "\n",
        "                # Stage 1: we generate candidate sub-n-grams using the longest allowed sequence, starting from single words to the longest one (the whole sequence)\n",
        "                L3Candidates = [] # this will be the list of all MWE candidates (sub-sequences)\n",
        "                IMaxMWE = len(LLTerm) # this is the longest MWE we can get from LLTerm: the length of the whole sequence\n",
        "                for klen in range(IMaxMWE): # lengths of candidate lists: starting from 0 till, but not including the max length\n",
        "                    klength = klen+1 # converting to true length: from length 1 till, and including the max length (we need this for slicing)\n",
        "                    # print(f'klen:{klength};')\n",
        "                    for i in range(IMaxMWE - klen): # positions where candidates start: position is from 0... up to (depending on length)\n",
        "                        # print(f'i:{i};')\n",
        "                        L2Candidate = LLTerm[i:i+klength] # L2 stands fro 'list of lists' (because we preserve 'word', 'lemma', 'pos' as a list for each item)\n",
        "                        L3Candidates.append(L2Candidate) # L3 is 'list of lists of lists' (here we collect all candidate sub-sequences from the longest L2 sequence)\n",
        "\n",
        "                # Stage 2: filtering by part-of-speech configurations: what is impossible, e.g., Adjectives can only appear in the beginning!\n",
        "                # CHANGE HERE FOR YOUR LANGUAGE: stage 2\n",
        "                L3PossibleTerms = [] # filtered lists of terms\n",
        "                \n",
        "                # DTermConfFrqC, DGS1wC, DGS2wC, DGS3wC, DGS4wC, L3SelectedTerms = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['APPR', 'ART','N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 2)\n",
        "                # DTermConfFrqC, DGS1wC, DGS2wC, DGS3wC, DGS4wC, L3PossibleTerms = selectTerms(L3Candidates, L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['APPR', 'ART','N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 2)\n",
        "\n",
        "                ## selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['APPR', 'ART','N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 2)\n",
        "\n",
        "                # removing: replaced by a positive filter\n",
        "                # putting back\n",
        "                \n",
        "                for L2Candidate in L3Candidates: # for each candidate configuration\n",
        "                    # e.g., if Adj is the last element (at the end) -- do not add\n",
        "                    # ADD MORE CONFIGURATIONS, eg, Gen case articles, prepositions\n",
        "                    # FileDebug.write(str(L2Candidate) + '\\n') # TEMPORARY -  DEBUGGING\n",
        "                    # Word = L2Candidate[WNumber][0]; PoS = L2Candidate[WNumber][1]; Lemma = L2Candidate[WNumber][2]\n",
        "                    # CONF 1: remove adjectives at the end: ListOfFields4LastWord = L2Candidate[-1] \n",
        "                    if re.match('ADJ.*', L2Candidate[-1][1]): \n",
        "                        # FileDebug.write('\\t Removed:' + str(L2Candidate) + '\\n')\n",
        "                        continue\n",
        "                    elif re.match('ART.*', L2Candidate[-1][1]) or re.match('ART.*', L2Candidate[0][1]): \n",
        "                        # FileDebug.write('\\t Removed:' + str(L2Candidate) + '\\n')\n",
        "                        continue\n",
        "                    elif re.match('APPR.*', L2Candidate[-1][1]) or re.match('APPR.*', L2Candidate[0][1]): \n",
        "                        # FileDebug.write('\\t Removed:' + str(L2Candidate) + '\\n')\n",
        "                        continue                    \n",
        "                    # ADD MORE CONFIGURATIONS HERE, e.g, Gen. case articles...\n",
        "                    else: # when all filters passed -- add all fields; frq list of terms will be represented by the Word or Lemma field in the end\n",
        "                        L3PossibleTerms.append(L2Candidate)\n",
        "                # here we have L3PossibleTerms ready; con\n",
        "                # FileDebug.write('---\\n' + str(L3PossibleTerms) + '\\n---\\n')\n",
        "                \n",
        "\n",
        "                # end: removing : replaced by a positive filter\n",
        "                # putting back\n",
        "\n",
        "                # going over all elements in the list of kept PossibleTerms, converting to each to a string and adding each one to the frequency dictionary\n",
        "                \n",
        "                for L2PossibleTerm in L3PossibleTerms:\n",
        "                    # CHOOSE SECOND ARGUMENT TO COLLECT: Word = 0; PoS = 1; Lemma = 2\n",
        "                    # SPossibleTerm = self.procCorpSelectField4FrqDic(0, L2PossibleTerm)\n",
        "                    SPossibleTerm = self.procCorpSelectField4FrqDic(2, L2PossibleTerm)\n",
        "\n",
        "                    try:\n",
        "                        # self.DictFrq[STerm] += 1\n",
        "                        self.DictFrq[SPossibleTerm] += 1\n",
        "                    except:\n",
        "                        # self.DictFrq[STerm] = 1\n",
        "                        self.DictFrq[SPossibleTerm] = 1  \n",
        "\n",
        "                LLTerm = [] # now we clear the list represening allowed PoS sequence, and start over again (we are in the else, we encountered end of allowed PoS codes)\n",
        "                     \n",
        "        return\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTR0hEcpmGL3"
      },
      "source": [
        "# TEST:\n",
        "# FileIN = open('BGHlem1k.txt', 'r')\n",
        "# PRODUCTION:\n",
        "FileIN = open('BGHlem.txt', 'r')\n",
        "\n",
        "FileOut1w = open('BGH_term1w.txt', 'w')\n",
        "FileOutMWE = open('BGH_termMWE.txt', 'w')\n",
        "# these files don't need to be re-imported in the later stage -- merge this together...\n",
        "\n",
        "# save the frequency dictionary into file, by decreasing frequencies\n",
        "# FileOutput.write( str( DictionaryFrq ) + '\\n' )\n",
        "\n",
        "OCorpus = clProcCorpus(FileIN)\n",
        "DictionaryFrq = OCorpus.DictFrq\n",
        "\n",
        "\n",
        "for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[1], reverse=True):\n",
        "    if re.search(' ', Word):\n",
        "        FileOutMWE.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "    else:\n",
        "        FileOut1w.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "\n",
        "FileOutMWE.flush()\n",
        "FileOutMWE.close()\n",
        "FileOut1w.flush()\n",
        "FileOut1w.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6itsBOHOqKGX",
        "outputId": "30f5e749-9a31-4d24-c652-13dff4fe4407"
      },
      "source": [
        "!zip BGH_term1w.zip BGH_term1w.txt\n",
        "!zip BGH_termMWE.zip BGH_termMWE.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: BGH_term1w.txt (deflated 63%)\n",
            "  adding: BGH_termMWE.txt (deflated 84%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex0nhEOZoydT",
        "outputId": "b6771507-7554-4fa3-ccf7-fb35420a49f4"
      },
      "source": [
        "# optional: how large are our output files for MWEs and single words?\n",
        "!wc BGH_term1w.txt\n",
        "!wc BGH_termMWE.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 441279  882130 8352855 BGH_term1w.txt\n",
            " 10765229  67981861 520948169 BGH_termMWE.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhxlpBqmBYEk"
      },
      "source": [
        "!head --lines=10 BGH_term1w.txt\n",
        "!head --lines=10 BGH_termMWE.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfKRr-j5M9Nv"
      },
      "source": [
        "### Stage 1.1: manually: get onto heiBox\n",
        "\n",
        "The archives are downloaded and saved in heibox directory. Further we download these files, if the next stages are performed independently.\n",
        "\n",
        "\n",
        "### Stage 1.2: uploading the the file back to colab\n",
        "\n",
        "Download is done from HeiBox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "165U-v8z8HYg"
      },
      "source": [
        "# Stage 1.2\n",
        "# Stage 3: Reading a file with extracted terms; capitalizing everything...\n",
        "# Reading test data - Possible Terms (extracted automatically): reading the text files of single and multiword terms, recording ranks\n",
        "# single words candidates\n",
        "### replaced with new run\n",
        "### run is producing these files in Stage 1 (is ready, doesn't need to be run again)\n",
        "#\n",
        "# Warning: these files are 8 and 70 MB respectively (relatively large to view on-line)\n",
        "# !wget https://heibox.uni-heidelberg.de/f/a9171080790f4932b7b1/?dl=1\n",
        "# !mv index.html?dl=1 BGH0_s02term1w.txt\n",
        "\n",
        "# multiword candidates\n",
        "# !wget https://heibox.uni-heidelberg.de/f/2488701205e34e4683b1/?dl=1\n",
        "# !mv index.html?dl=1 BGH0_s02termMWE.txt\n",
        "\n",
        "# new run: with new pos patterns\n",
        "# multiword terms (zip archive)\n",
        "!wget https://heibox.uni-heidelberg.de/f/6e345e17e22d45bc8245/?dl=1\n",
        "!mv index.html?dl=1 BGH_termMWE02.zip\n",
        "!unzip BGH_termMWE02.zip\n",
        "# single words archive\n",
        "!wget https://heibox.uni-heidelberg.de/f/7a2fed4851a940958c43/?dl=1\n",
        "!mv index.html?dl=1 BGH_term1w02.zip\n",
        "!unzip BGH_term1w02.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6-BRAmPODgc"
      },
      "source": [
        "!mv BGH_termMWE.txt BGH0_s02termMWE.txt\n",
        "!mv BGH_term1w.txt BGH0_s02term1w.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APb7M33oORaN",
        "outputId": "28ec2ed5-1c1e-485b-d133-a4cc6c38592d"
      },
      "source": [
        "!wc BGH0_s02termMWE.txt\n",
        "!wc BGH0_s02term1w.txt\n",
        "!head --lines=10 BGH0_s02term1w.txt\n",
        "!head --lines=10 BGH0_s02termMWE.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10765229  67981861 520948169 BGH0_s02termMWE.txt\n",
            " 441279  882130 8352855 BGH0_s02term1w.txt\n",
            "§\t1886129\n",
            "Beklagte\t784204\n",
            "rn\t584370\n",
            "Urteil\t535254\n",
            "Angeklagte\t505748\n",
            "vgl\t487398\n",
            "ZR\t486364\n",
            "Satz\t438322\n",
            "Beschluss\t435239\n",
            "Berufungsgericht\t432318\n",
            "@card@ rn\t121146\n",
            "Bundesgerichtshof Beschluss\t94989\n",
            "§ @card@ rn\t85034\n",
            "@card@ BGB\t81532\n",
            "Zivilsenat die Bundesgerichtshof\t78478\n",
            "§ @card@ BGB\t78070\n",
            "Richter Dr\t72440\n",
            "Revision die Angeklagte\t61582\n",
            "Vorsitzende Richter\t60142\n",
            "VI ZR\t56532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQk8MBlf92JN"
      },
      "source": [
        "# reading files into a dictionary - exe ~ 41 seconds;\n",
        "FAutoTerms1w = open('BGH0_s02term1w.txt', 'r')\n",
        "FAutoTermsMWE = open('BGH0_s02termMWE.txt', 'r')\n",
        "\n",
        "# FoutAutoTerms1w = open('BGH0_s02term1w_out.txt', 'w')\n",
        "# FoutAutoTermsMWE = open('BGH0_s02termMWE_out.txt', 'w')\n",
        "# ... here we add functions for reading this dictionary (e.g., as ranked list, etc.)\n",
        "\n",
        "DAutoTerms1w = readDictionary(FAutoTerms1w, Caps=True)\n",
        "DAutoTermsMWE = readDictionary(FAutoTermsMWE, Caps=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wn2J7Ck-boW",
        "outputId": "a93fff58-7dee-4f1a-8b6d-56af22610f2b"
      },
      "source": [
        "try:\n",
        "    print(DAutoTerms1w['ANGEKLAGTE'])\n",
        "    print(DAutoTerms1w['VGL'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "\n",
        "try:\n",
        "    print(DAutoTermsMWE['ZIVILSENAT DIE BUNDESGERICHTSHOF'])\n",
        "    print(DAutoTermsMWE['VORSITZENDE RICHTER'])\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "505748.0\n",
            "16.0\n",
            "78478.0\n",
            "60142.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ow4TxiGUQEM"
      },
      "source": [
        "### End of Part 1: terminology extraction from legal corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn5C7CN4Sup9"
      },
      "source": [
        "## Part 2: The test set\n",
        "(Test set is also manually annotated and used as a gold standard)\n",
        "\n",
        "### 2.1. Concatenating the annotated test sets (5 out of 6 files)\n",
        "Then -- this will be used for extracting annotation in further stages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3RKlelR6jx"
      },
      "source": [
        "## preparing for TreeTagger processing (do not have to re-run again once completed)\n",
        "## Annotated Gold Standard\n",
        "# Stage 1: Preparing Gold standard: Reading / extracting information from gold standard: creating a list of annotated terms\n",
        "# set 1 (same text annotated by two annotators)\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/ae1110c4f9ad42b9a3d5/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Astghik.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/398e7a10fa3241519f26/?dl=1\n",
        "!mv index.html?dl=1 BGH1_s00Maia.txt\n",
        "\n",
        "# set 2 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/0c787f26123f49178639/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Hayk.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/356205b502fb4d759ad5/?dl=1\n",
        "!mv index.html?dl=1 BGH2_s00Nino.txt\n",
        "\n",
        "# set 3 (same text annotated by two annotators)\n",
        "!wget https://heibox.uni-heidelberg.de/f/ed0c7af9a9d04967b449/?dl=1\n",
        "!mv index.html?dl=1 BGH3_s00Tamar.txt\n",
        "# !wget \n",
        "# !mv index.html?dl=1\n",
        "\n",
        "# one more will be added: Frau Khachatryan\n",
        "!cat BGH1_s00Astghik.txt BGH1_s00Maia.txt BGH2_s00Hayk.txt BGH2_s00Nino.txt BGH3_s00Tamar.txt >BGH0_s00GoldStandard.txt\n",
        "\n",
        "FInBGH0_s00GoldStandard = open('BGH0_s00GoldStandard.txt', 'r')\n",
        "FOutBGH0_s00GoldStandard = open('BGH0_s00GoldStandard_xml.txt', 'w')\n",
        "\n",
        "convertBrecket2Xml(FInBGH0_s00GoldStandard, FOutBGH0_s00GoldStandard, '<<+', '>>+', '<TERM>', '</TERM>')\n",
        "# this result is pos-tagged and uploaded in the next step\n",
        "# command:\n",
        "# tree-tagger-de.sh /Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_xml.txt >/Users/bogdan/Seafile/research/corpus/DAAD-corpus/daad-experiments/BGH0_s00GoldStandard_LEM.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mci7JBJWekkn"
      },
      "source": [
        "### 2.2 Creating a version of 1 copy of the text and cleaning of the test set from human annotations \n",
        "\n",
        "(for automated processing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5RfGopzJZr"
      },
      "source": [
        "# Stage04: preparing data for calculating precision and recall on the space of all possible MWEs, 1, 2, 3 words; (overlapping)\n",
        "# keeping only 1 version of the text (2 annotators annotated the same text twice to measure interannotator agreement)\n",
        "!cat BGH1_s00Astghik.txt BGH2_s00Hayk.txt BGH3_s00Tamar.txt >BGH0_s03GoldStandard1Version.txt\n",
        "FInputGS1V = open('BGH0_s03GoldStandard1Version.txt', 'r')\n",
        "FOutputGS1V = open('BGH0_s04GoldStandard1Version_text.txt', 'w')\n",
        "\n",
        "# gold standard - 1 version:\n",
        "# we clean up the document, with only 1 copy of the text; then we run it locally through TreeTagger and then process here\n",
        "\n",
        "'''\n",
        "The idea is to tokenise the gold standard (from Stage 0), and to generate all possible MWEs for each string / pargraph\n",
        "    then we can test what is the coverage (non-overlapping) or precision (overlapping)\n",
        "    or: we create a dictionary of potential single and MWE strings and check what has been identified ?\n",
        "    or: comparing with 'oracle': known annotations are run as a point of comparision on the space; and we establish relations, i.e., the amount of over-generation\n",
        "\n",
        "    tasks: \n",
        "        4a: create the \"all possible strings\" space from gold standard text\n",
        "        4b: intersect 4a results with corpus list of extracted MWEs >> generate \"extracted from gold standard\" dictionary\n",
        "        4c: intersect human annotation in gold standard with 4a >> generate \"correct in gold standard\" dictionary\n",
        "        4d: intersect 4b and 4c, >> correctly extracted\n",
        "        4e: calculate 4d/4b = precision\n",
        "            calculate 4d/4c = recall\n",
        "\n",
        "'''\n",
        "# 3a: processing gold standard: tokenizing\n",
        "\n",
        "LLParTokens = [] # List of paragraphs, each represented as a list of tokens\n",
        "for SLine in FInputGS1V:\n",
        "    # print(SLine)\n",
        "    SLine = SLine.strip() # implement this change\n",
        "\n",
        "    # remove annotation\n",
        "\n",
        "\n",
        "    SLine = re.sub('[<>]+', ' ', SLine)\n",
        "    SLine = re.sub(' +', ' ', SLine)\n",
        "\n",
        "    FOutputGS1V.write(SLine + '\\n')\n",
        "\n",
        "FInputGS1V.close()\n",
        "FOutputGS1V.flush()\n",
        "FOutputGS1V.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIf30Nt_djvd"
      },
      "source": [
        "!echo file1 BGH0_s03GoldStandard1Version.txt\n",
        "!head --lines=10 BGH0_s03GoldStandard1Version.txt\n",
        "!echo file2 BGH0_s04GoldStandard1Version_text.txt\n",
        "!head --lines=10 BGH0_s04GoldStandard1Version_text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmUKEAwHiPKo"
      },
      "source": [
        "a) manual stage: the file is downloaded, lemmatized with tree tagger and uploaded onto colab back again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htvZjKpX2Dku"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/d39b640f70504b4fb86a/?dl=1\n",
        "!mv index.html?dl=1 BGH0_s04GoldStandard1Version_LEM.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaeRl0y_4nP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7fac89-3b34-41c1-fb71-63edd0c1b166"
      },
      "source": [
        "!head --lines=10 BGH0_s04GoldStandard1Version_LEM.txt\n",
        "!wc BGH0_s04GoldStandard1Version_LEM.txt\n",
        "!wc BGH0_s04GoldStandard1Version_text.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BUNDESGERICHTSHOF\tNN\tBundesgerichtshof\n",
            "IM\tNE\tIM\n",
            "NAMEN\tNN\tName\n",
            "DES\tART\tdie\n",
            "VOLKES\tNN\tVolk\n",
            "URTEIL\tVVIMP\turteilen\n",
            "1\tCARD\t1\n",
            "StR\tNN\tStR\n",
            "42/01\tCARD\t@card@\n",
            "vom\tAPPRART\tvon\n",
            " 198146  594342 3297455 BGH0_s04GoldStandard1Version_LEM.txt\n",
            "   3469  169020 1215715 BGH0_s04GoldStandard1Version_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLmxjIO1HTD"
      },
      "source": [
        "here we pass the output file to TreeTagger and download the result, the statistics is:\n",
        "\n",
        "bash-3.2$ wc BGH0_s04GoldStandard1Version_LEM.txt \n",
        "\n",
        "BGH0_s04GoldStandard1Version_text.txt\n",
        "\n",
        "    198146  594438 3297455 BGH0_s04GoldStandard1Version_LEM.txt\n",
        "    3469  169054 1215715 BGH0_s04GoldStandard1Version_text.txt\n",
        "    201615  763492 4335398 total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWmSj4zPf1fs"
      },
      "source": [
        "### 2.3. Automated extraction of MWEs from the test set\n",
        "(baseline; ~100 recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l72s3oCyoqlP"
      },
      "source": [
        "FInputGS1V = open('BGH0_s04GoldStandard1Version_LEM.txt', 'r')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru44lWdYotLI"
      },
      "source": [
        "# tokenizing the test set function\n",
        "# extracting annotated terms from the gold standard in xml format\n",
        "\n",
        "def collectField(FInputGS1V, Caps = False):\n",
        "\n",
        "    LLParTokens = [] # List of paragraphs, each represented as a list of tokens\n",
        "    for SLine in FInputGS1V:\n",
        "        # print(SLine)\n",
        "        SLine = SLine.strip() # implement this change\n",
        "\n",
        "        # remove annotation\n",
        "\n",
        "        # SLine = re.sub('[<>]+', ' ', SLine)\n",
        "        # SLine = re.sub(' +', ' ', SLine)\n",
        "        if Caps: SLine = SLine.upper() # capitalize all words\n",
        "        # print(SLine)\n",
        "\n",
        "        LLine = re.split(' ', SLine) \n",
        "\n",
        "        # separate punctuation\n",
        "        LLine = re.findall(r\"[\\w']+|[.,!?;()\\-„“\\\"]\", SLine)\n",
        "        # SLine = re.sub(r'(,\\.;:\\-\\!\\?\\(\\)\\[\\]\\“\\\")', r' \\1 ', SLine)\n",
        "\n",
        "        LLine = list(filter(None, LLine))\n",
        "        LLParTokens.append(LLine)\n",
        "        return LLParTokens\n",
        "\n",
        "'''\n",
        "def vertCollectField(FInVert, IFieldN, Caps = False):\n",
        "    L2Pars = []\n",
        "    LWords = []\n",
        "    for SLine in FInVert:\n",
        "        if re.match('<', SLine):\n",
        "            L2Pars.append(LWords)\n",
        "            LWords = []\n",
        "            continue\n",
        "        LFields = re.split('\\t', SLine)\n",
        "        LWords.append(LFields[IFieldN])\n",
        "    L2Pars.append(LWords)  \n",
        "    return L2Pars \n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSBlYExXpBvy"
      },
      "source": [
        "LLParTokens = vertCollectField(FInputGS1V, 2, Caps = True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gW1nja5p3pJ",
        "outputId": "937cd05a-1af5-480e-e2bb-ca295228664d"
      },
      "source": [
        "len(LLParTokens[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198146"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDrA6Ly4pWmz",
        "outputId": "2828729c-679e-4041-b7a2-e044c1860a4e"
      },
      "source": [
        "print(str(LLParTokens[0][9]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "von\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7SheSapAoh"
      },
      "source": [
        "FInputGS1V.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8I_PSNkbjt"
      },
      "source": [
        "# Stage 4A\n",
        "# generating candidate MWEs for cheking if / when they have been identified as terms\n",
        "# algorithm from Terminologieextraktion3 notebook\n",
        "# 4a: creating space of all possible overlapping MWEs in gold standard\n",
        "\n",
        "def tokens2candNGrams(LWords, N): # working with specific N-gram size, to keep number of candidates under control\n",
        "    '''\n",
        "    convert a list of tokens into a list of all possible MWEs (works for each paragraph)\n",
        "    '''\n",
        "    LLCandidates = [] # lists - tokenised results\n",
        "    # LSCandidates = [] # strings - joint results\n",
        "\n",
        "    for i in range(N): # for up to the required N-gram length\n",
        "\n",
        "        for IPosition in range(len(LWords) - i): # unigrams -- no change; bigrams: up to penultimate, etc.\n",
        "            LCandidate = LWords[IPosition : IPosition + i + 1]\n",
        "            # SCandidate = ' '.join(LCandidate)\n",
        "            LLCandidates.append(LCandidate)\n",
        "        # LSCandidates.append(SCandidate)\n",
        "    \n",
        "    return LLCandidates\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qunX9NxPnV_G"
      },
      "source": [
        "L2TestSetLemmas = vertCollectField()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAUo4zFUKFg"
      },
      "source": [
        "### functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsmKBqMgGWLf"
      },
      "source": [
        "# import useful libraries, files\n",
        "import re, os, sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ9Hkx0OGfCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc685fc2-6d95-4bef-edcc-760c7586fd12"
      },
      "source": [
        "# file for recording results of different configurations\n",
        "# run this only once\n",
        "!rm AllTermExtractionResultsV01.txt\n",
        "!rm AllTermExtractionResultsV02.txt\n",
        "\n",
        "FOutResults1 = open('AllTermExtractionResultsV01.txt', 'a')\n",
        "FOutResults2 = open('AllTermExtractionResultsV02.txt', 'a')\n",
        "FOutResults1.write('Run\\tW1A\\tW1B\\tW1C\\tW1D\\tW1P\\tW1R\\tW2A\\tW2B\\tW2C\\tW2D\\tW2P\\tW2R\\tW3A\\tW3B\\tW3C\\tW3D\\tW3P\\tW3R\\tW4A\\tW4B\\tW4C\\tW4D\\tW4P\\tW4R\\n')\n",
        "FOutResults2.write('Run\\tW1P\\tW1R\\tW2P\\tW2R\\tW3P\\tW3R\\tW4P\\tW4R\\n') # only precision and recall figures\n",
        "FOutResults1.flush()\n",
        "FOutResults2.flush()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'AllTermExtractionResultsV01.txt': No such file or directory\n",
            "rm: cannot remove 'AllTermExtractionResultsV02.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvzLyPZYBUi"
      },
      "source": [
        "# filter functions -- e.g., by keyness\n",
        "# here we will add / combine information about keyness...\n",
        "# to be implemented...\n",
        "# only allow those terms into the Auto dictionary, which have weights; replace frq by keyness weights (or sum)\n",
        "# function to be used on all dictionaries:\n",
        "# we can modify different parameters : both/one product or maximum...\n",
        "\n",
        "def filterDictByKWDict(DAuto, DKeyness, Threshold = 1, Mode='prod', Req = 2):\n",
        "    '''\n",
        "    Mode = max: we take maximum value of keyness\n",
        "         = prod -- we take the product of keyness, \n",
        "    Req = 1: we require at least one word in to be in the keyness dictionary\n",
        "        = 2...N: we require at least 2, N words to be in the keyness dictionary (if there are as many in the list)\n",
        "    '''\n",
        "    DAutoFiltered = {}\n",
        "    for SAutoTerm, Frq in DAuto.items():\n",
        "        if Mode == 'max': AKeynessAll = 0\n",
        "        elif Mode == 'prod': AKeynessAll = 1\n",
        "        LAutoTermWs = re.split(' ', SAutoTerm)\n",
        "        ICountFound = 0\n",
        "        for STerm in LAutoTermWs:\n",
        "            if STerm in DKeyness:\n",
        "                ICountFound += 1\n",
        "                if Mode == 'max' and DKeyness[STerm] > AKeynessAll:\n",
        "                    AKeynessAll = DKeyness[STerm] # we take the maximum keyness\n",
        "                elif Mode == 'prod':\n",
        "                    AKeynessAll = AKeynessAll * DKeyness[STerm]\n",
        "        if len(LAutoTermWs) < Req: Req0 = len(LAutoTermWs)\n",
        "        else: Req0 = Req\n",
        "        if ICountFound < Req0: continue\n",
        "        if AKeynessAll > Threshold:\n",
        "            DAutoFiltered[SAutoTerm] = AKeynessAll\n",
        "\n",
        "    return DAutoFiltered\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbsgvYg6YcO"
      },
      "source": [
        "# dowloading and creating necessary files\n",
        "# term extraction will be integrated here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHL3gREljDnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f46dc72-5803-41a9-8530-270b1d97f180"
      },
      "source": [
        "# downloading PoS-tagged files after TT processing\n",
        "# !wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# Vahram's file, manually checked (some breckets remaining...)\n",
        "!wget https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
        "# alternative file created in this workflow\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d8f1bb53632d40538e0d/?dl=1\n",
        "\n",
        "!mv index.html?dl=1 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 09:10:00--  https://heibox.uni-heidelberg.de/f/4e719e0466a143c0b1b5/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/df5015c9-c1f5-4bbd-a09d-3fbd86626d16/BGH_GS_LEM.txt [following]\n",
            "--2021-11-05 09:10:01--  https://heibox.uni-heidelberg.de/seafhttp/files/df5015c9-c1f5-4bbd-a09d-3fbd86626d16/BGH_GS_LEM.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5936140 (5.7M) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]   5.66M  5.19MB/s    in 1.1s    \n",
            "\n",
            "2021-11-05 09:10:02 (5.19 MB/s) - ‘index.html?dl=1’ saved [5936140/5936140]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQvNF5V6wjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64458547-6a7e-4dbc-994a-d5b83cd6d9e0"
      },
      "source": [
        "!head --lines=10 BGH0_s00GS_LEM.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TERM>\r\n",
            "BUNDESGERICHTSHOF\tNN\tBundesgerichtshof\r\n",
            "</TERM>\r\n",
            "IM\tNE\tIM\r\n",
            "NAMEN\tNN\tName\r\n",
            "DES\tART\tdie\r\n",
            "VOLKES\tNN\tVolk\r\n",
            "<TERM>\r\n",
            "URTEIL\tNN\tUrteil\r\n",
            "</TERM>\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQNpab5JrOn"
      },
      "source": [
        "# read / generate all the necessary texts\n",
        "# first define functions, then download files and read them into dictionaries...\n",
        "\n",
        "# further versions of the Gold Standard: \n",
        "# Annotated and PoS-tagged Gold Standard -- for extraction of the correct evaluation set\n",
        "\n",
        "# printing words of different length\n",
        "# FOutput = open('BGH0_s01GoldS_Terms.txt', 'w')\n",
        "# FOutputDict1w = open('BGH0_s01GoldS_D1w.txt', 'w') # 1-word terms\n",
        "# FOutputDict2w = open('BGH0_s01GoldS_D2w.txt', 'w') # 2-word terminological expressions\n",
        "# FOutputDict3w = open('BGH0_s01GoldS_D3w.txt', 'w') # 3-word terminological expressions\n",
        "# FOutputDict4w = open('BGH0_s01GoldS_D4w.txt', 'w') # more than 3 words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA9UNO0NZbt0"
      },
      "source": [
        "# reading datasets\n",
        "# list of gold-standard annotated terms, with lemmatization and pos fileds\n",
        "FInBGH0_s00GS_LEM = open('BGH0_s00GS_LEM.txt', 'r')\n",
        "# FOutBGH0_s00GS_Terms = open('BGH0_s00GS_Terms.txt', 'w')\n",
        "L3AnnotatedSegs = vertCollectAnnotation(FInBGH0_s00GS_LEM, 'TERM', Caps = False)\n",
        "\n",
        "# testing the file read\n",
        "# for LSegment in L3AnnotatedSegs: FOutBGH0_s00GS_Terms.write(str(LSegment) + '\\n')\n",
        "# FOutBGH0_s00GS_Terms.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPE39kkEAOwZ",
        "outputId": "fe4a0bed-9c45-4fec-8551-c4dc419f5eac"
      },
      "source": [
        "print(len(L3AnnotatedSegs))\n",
        "L3AnnotatedSegs[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16498\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['OBERSTAATSANWALT', 'NN', 'OBERSTAATSANWALT'],\n",
              " ['BEIM', 'APPRART', 'BEI'],\n",
              " ['BUNDESGERICHTSHOF', 'NN', 'BUNDESGERICHTSHOF']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6NsHtRHHOjp"
      },
      "source": [
        "changeCaseL3(L3AnnotatedSegs, LFlds2Caps = [0, 2], Mode='upper')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3_PKbspAYMD"
      },
      "source": [
        "# create a dictionary of PoS patterns in the gold standard annotation\n",
        "DPatternsFrq = createDictOfPatterns(L3AnnotatedSegs, 1, Normalize = 1)\n",
        "FOutTermPOS = open('BGH0_s00GoldStandard_pos.txt', 'w')\n",
        "\n",
        "printDictionary(DPatternsFrq, FOutTermPOS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX29G8r3Zo6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e10d22-d19b-4b6e-aecd-596282796520"
      },
      "source": [
        "try:\n",
        "    del DTermConfFrq\n",
        "    del DGS1w\n",
        "    del DGS2w\n",
        "    del DGS3w\n",
        "    del DGS4w\n",
        "except:\n",
        "    print('dictionaries not defined yet...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionaries not defined yet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_kJbRh3TDa",
        "outputId": "b56017d4-d862-4842-8187-113c0497b9be"
      },
      "source": [
        "# selecting terms from the gold standard, which fit the description\n",
        "# DTermConfFrq = selectTerms(L3AnnotatedSegs, L2Patterns = [['N', '\\$']], SplitLen = False, IFldNumber = 0)\n",
        "# FOutExamples = open('BGH0_s00GoldStandard_examples_ADJ_N.txt', 'w')\n",
        "\n",
        "# DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = None, LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "# DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ', 'N'], ['N', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 0)\n",
        "# removing the pattern: ['APPR', 'ART','N'],\n",
        "# ['APPR', 'ART','N'], \n",
        "DTermConfFrq, DGS1w, DGS2w, DGS3w, DGS4w, L3SelectedTerms = selectTerms(L3AnnotatedSegs, L2Patterns = [['N'], ['ADJ'], ['ADJ', 'N'], ['N', 'N'], ['N', 'ART', 'N'], ['N', 'APPR', 'N'], ['ADJ', 'ADJ', 'N'], ['ADJ', 'N', 'N'], ['N', 'N', 'N'], ['N', 'ADJ', 'N'], ['N', 'APPR', 'ART', 'N'], ['N', 'ART', 'ADJ', 'N']], LNoEdge = None, LNoStart = None, L2NoEnd = None, SplitLen = True, IFldNumber = 2)\n",
        "\n",
        "\n",
        "FOutExamples = open('BGH0_s00GoldStandard_examples.txt', 'w')\n",
        "FOutExamples1w = open('BGH0_s00GoldStandard1w_examples.txt', 'w')\n",
        "FOutExamples2w = open('BGH0_s00GoldStandard2w_examples.txt', 'w')\n",
        "FOutExamples3w = open('BGH0_s00GoldStandard3w_examples.txt', 'w')\n",
        "FOutExamples4w = open('BGH0_s00GoldStandard4w_examples.txt', 'w')\n",
        "\n",
        "printDictionary(DTermConfFrq, FOutExamples)\n",
        "printDictionary(DGS1w, FOutExamples1w)\n",
        "printDictionary(DGS2w, FOutExamples2w)\n",
        "printDictionary(DGS3w, FOutExamples3w)\n",
        "printDictionary(DGS4w, FOutExamples4w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14698 297 280 9 15284\n",
            "2285 177 105 9 2576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvy0lBNuQ7jI"
      },
      "source": [
        "# print(len(DTermConfFrq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTfxszq3xgjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e52677-a9bc-46f5-f9fc-2b3e36a46bd0"
      },
      "source": [
        "# Stage 2: preparing keyness dictionary\n",
        "!wget https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
        "!mv index.html?dl=1 TK_KW_Verif_V02.csv\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
        "!mv index.html?dl=1 KW_BGH_10000.tsv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 14:21:22--  https://heibox.uni-heidelberg.de/f/aa4560e627bd4b1d8055/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/8e20ed8e-f61b-4a0f-9664-1b1c818070a4/TK_KW_Verif_V02.csv [following]\n",
            "--2021-11-04 14:21:23--  https://heibox.uni-heidelberg.de/seafhttp/files/8e20ed8e-f61b-4a0f-9664-1b1c818070a4/TK_KW_Verif_V02.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38812 (38K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  37.90K   145KB/s    in 0.3s    \n",
            "\n",
            "2021-11-04 14:21:23 (145 KB/s) - ‘index.html?dl=1’ saved [38812/38812]\n",
            "\n",
            "--2021-11-04 14:21:23--  https://heibox.uni-heidelberg.de/f/a83ba95576a244a59966/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/6aa416a5-c4d7-4a52-af60-0f492927c9ed/KW_BGH_10000.csv [following]\n",
            "--2021-11-04 14:21:24--  https://heibox.uni-heidelberg.de/seafhttp/files/6aa416a5-c4d7-4a52-af60-0f492927c9ed/KW_BGH_10000.csv\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216590 (212K) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>] 211.51K   400KB/s    in 0.5s    \n",
            "\n",
            "2021-11-04 14:21:24 (400 KB/s) - ‘index.html?dl=1’ saved [216590/216590]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWk89VE9xyct"
      },
      "source": [
        "# Preparing a dictionnary of keyness weights, checking the 'approval' status\n",
        "FInputKW = open('TK_KW_Verif_V02.csv', 'r')\n",
        "FInputKWLarge = open('KW_BGH_10000.tsv', 'r') # for experiments with Precision / Recall\n",
        "FOutputKW = open('TK_KW_Verif_V02.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3zBlO_Kx2xQ",
        "outputId": "81d02c81-7ae8-46ca-ca7a-1eeacb8f16af"
      },
      "source": [
        "try:\n",
        "    del DScoresKW\n",
        "    del DScoresNK\n",
        "    del DStatKW\n",
        "    del DScoresKWLarge\n",
        "    del DScoresREKWquick\n",
        "except:\n",
        "    print('dictionaries not defined')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionaries not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGkpLArq0m9h"
      },
      "source": [
        "# reading an annotated dictionary: keyword or not: 1, 0.5, 0 (yes, unsure, no)\n",
        "DScoresKW, DScoresNK = readDictKWAnnotations(FInputKW, AKStatThreshold = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9WdlvXV19UW",
        "outputId": "45dddfa4-ce75-49c3-a9c0-741d25102767"
      },
      "source": [
        "try:\n",
        "    print(DScoresKW['ANGEKLAGTE'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['VGL'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['NACHPRÜFUNG'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['RECHTLICH'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['JURISTISCH'])\n",
        "except:\n",
        "    print('keys not found')\n",
        "try:\n",
        "    print(DScoresNK['PERSON'])\n",
        "\n",
        "    # 'JURISTISCHE PERSON'\n",
        "    # 'RECHTLICHER NACHPRÜFUNG'\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.5625\n",
            "345.6272888184\n",
            "3406.630859375\n",
            "keys not found\n",
            "keys not found\n",
            "keys not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQWGjirR174Z"
      },
      "source": [
        "# reading the large keyword dictionary\n",
        "DScoresKWLarge = readDictionary(FInputKWLarge, Caps=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE0Z1ePs754t",
        "outputId": "6116c3ea-36a6-41f2-8b6f-bf4da598b502"
      },
      "source": [
        "try:\n",
        "    print(DScoresKWLarge['ANGEKLAGTE'])\n",
        "    print(DScoresKWLarge['VGL'])\n",
        "except:\n",
        "    print('keys not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.56\n",
            "258061.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3pPwCbQ-8EN"
      },
      "source": [
        "# Stage 3.1 combine extracted words with keyness (e.g., filter by keyness, etc.)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhRHBnHEVnLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ee597d-c792-4d5e-b6bc-de35172dd46f"
      },
      "source": [
        "try:\n",
        "    del DAutoTerms01KW_H1w\n",
        "    del DAutoTerms01KW_HMWE\n",
        "except:\n",
        "    print('dictionaries not defined yet...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionaries not defined yet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvz6SKCUUNf9"
      },
      "source": [
        "DAutoTerms01KW_H1w = filterDictByKWDict(DAutoTerms1w, DScoresKWLarge, Threshold = 1, Req = 1) # autoterms filtered by human annotated items\n",
        "DAutoTerms01KW_HMWE = filterDictByKWDict(DAutoTermsMWE, DScoresKWLarge, Threshold = 1, Req = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2dc9MfZUnIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479c3789-4db0-4701-eec4-b7f81a102528"
      },
      "source": [
        "try:\n",
        "    print(DAutoTerms01KW_H1w['ANGEKLAGTE'])\n",
        "    print(DAutoTerms01KW_H1w['VGL'])\n",
        "\n",
        "    print(DAutoTerms01KW_HMWE['LANDGERICHT AUGSBURG'])\n",
        "    print(DAutoTerms01KW_HMWE['VORSITZENDER RICHTER'])\n",
        "    print(DAutoTerms01KW_HMWE['RECHTLICHER NACHPRÜFUNG'])\n",
        "    print(DAutoTerms01KW_HMWE['JURISTISCHER PERSON'])\n",
        "\n",
        "    # 'RECHTLICHER NACHPRÜFUNG'  \n",
        "except:\n",
        "    print('keys not found')\n",
        "\n",
        "# JURISTISCHE PERSON >> lemmas or text forms in keywords???\n",
        "\n",
        "# print statistics\n",
        "print(len(DAutoTerms1w))\n",
        "print(len(DAutoTermsMWE))\n",
        "\n",
        "print(len(DScoresKWLarge))\n",
        "\n",
        "print(len(DAutoTerms01KW_H1w))\n",
        "print(len(DAutoTerms01KW_HMWE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317698.56\n",
            "258061.06\n",
            "208549.09\n",
            "105619.59\n",
            "24163.04\n",
            "keys not found\n",
            "466669\n",
            "2434043\n",
            "9979\n",
            "8247\n",
            "1117210\n"
          ]
        }
      ]
    }
  ]
}