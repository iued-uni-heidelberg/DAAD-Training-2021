{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TerminologieextraktionV02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/DAAD-Training-2021/blob/main/TerminologieextraktionV02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir693xy5khPU"
      },
      "source": [
        "# Terminology extraction excercise\n",
        "\n",
        "## Running terminology extraction script on different corpora\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLYo1agEEIdJ"
      },
      "source": [
        "Stage 1 Morphological analysis of Armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPlL9dcNIOp"
      },
      "source": [
        "# importing python libraries\n",
        "import os, re, sys"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtWZ1FuvlfLk"
      },
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3\n",
        "\n",
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ivpN2eGbdd"
      },
      "source": [
        "Downloading Armenian texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGu0kwjPGYF0"
      },
      "source": [
        "%%bash\n",
        "# larger corpus of human rights 2M words\n",
        "wget https://heibox.uni-heidelberg.de/f/8148c49c41b84155b75d/?dl=1\n",
        "mv index.html?dl=1 humanrights_hy_2m.txt\n",
        "\n",
        "# smaller corpus of human rights 25k words\n",
        "wget https://heibox.uni-heidelberg.de/f/c888756380ba4f42b210/?dl=1\n",
        "mv index.html?dl=1 humanrights_hy_25k.txt\n",
        "\n",
        "# George Orwell 1994 - Armenian\n",
        "wget https://heibox.uni-heidelberg.de/f/e0bfae444a5a4c76957b/?dl=1\n",
        "mv index.html?dl=1 hy1984.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OrjpVo-Fg1i"
      },
      "source": [
        "FInText = open('humanrights_hy_2m.txt','r')\n",
        "FOutText = open('humanrights_hy_2m_vert.txt','w')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UlPD2WHFzdo"
      },
      "source": [
        "FInText = open('humanrights_hy_25k.txt','r')\n",
        "FOutText = open('humanrights_hy_25k_vert.txt','w')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJmwltoWF1Gi"
      },
      "source": [
        "FInText = open('hy1984.txt','r')\n",
        "FOutText = open('hy1984_vert.txt','w')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yOEP9vHFzL6",
        "outputId": "cf6a1984-cae6-4383-e818-499e0b6ac46b"
      },
      "source": [
        "countLines = 0\n",
        "for SLine in FInText:\n",
        "    countLines += 1\n",
        "    if countLines % 1000 == 0: print(countLines)\n",
        "    SLine = SLine.strip()\n",
        "    ListOfWords = re.split('[ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+', SLine) # tokenize: split on white spaces and punctuation\n",
        "    # if len(ListOfWords) > 0: FOutText.write(str(ListOfWords) + '\\n')\n",
        "    analyses = a.analyze_words(ListOfWords, disambiguate=False)\n",
        "    FOutText.write('<p>\\n')\n",
        "    for ana in analyses:\n",
        "        # for wfo in ana:\n",
        "        # how to type all variants + disambiguate ?\n",
        "        wfo = ana[0]\n",
        "        FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "        #    FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "    FOutText.write('</p>\\n')\n",
        "FOutText.flush()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXKKCk4ow8fQ"
      },
      "source": [
        "# ArmenianWP\n",
        "!wget https://heibox.uni-heidelberg.de/f/206d85a0270943e4b87b/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 WPhy_vert.txt\n",
        "\n",
        "# Constitution Republic of Armenia\n",
        "!wget https://heibox.uni-heidelberg.de/f/bf54977b17604ec592cd/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 Const_RA_l.txt\n",
        "\n",
        "# Grundgesetz\n",
        "!wget https://heibox.uni-heidelberg.de/f/d6c5b31edc84422d9e14/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 GG_l.txt\n",
        "\n",
        "# Origin of the species\n",
        "!wget https://heibox.uni-heidelberg.de/f/befc6dbe718b4a37ba74/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 OOOS_l.txt\n",
        "\n",
        "# Europarl DE\n",
        "!wget https://heibox.uni-heidelberg.de/f/3ba6122e744e4b7f9c14/?dl=1\n",
        "# renaming file\n",
        "!mv index.html?dl=1 EP_DE_l.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeamIEr15Lev"
      },
      "source": [
        "#Terminologieextraktion\n",
        "import os, re, sys\n",
        "class clProcCorpus(object):\n",
        "    ''' we will read a text file and return a dictionary\n",
        "    this will be done on the line by line basis\n",
        "    The dictionary can be sorted later...\n",
        "    '''\n",
        "    # this is a class for processing a corpus\n",
        "\n",
        "    def __init__(self, FileIN):\n",
        "        self.DictFrq = {}\n",
        "        self.processCorpus(FileIN)\n",
        "\n",
        "    def processCorpus(self, FileIN):\n",
        "        LTerm = []\n",
        "        for Line in FileIN:\n",
        "            Line = Line.strip()\n",
        "            LLine = re.split('\\t', Line)\n",
        "            try:\n",
        "                Word = LLine[0]\n",
        "                PoS = LLine[1]\n",
        "                Lemma = LLine[2]\n",
        "            except:\n",
        "                Word = \"\"\n",
        "                PoS = \"\"\n",
        "                Lemma = \"\"\n",
        "            \n",
        "      #Select the Tags for your langauge\n",
        "            #if re.match('N.*', PoS) or re.match('A.*', PoS): #Arm\n",
        "            if re.match('N.*', PoS) or re.match('ADJ.*', PoS): #DE\n",
        "            #if re.match('N.*', PoS) or re.match('J.*', PoS): #EN\n",
        "\n",
        "      #Terms as Words or Lemmas\n",
        "                LTerm.append(Lemma)\n",
        "            else:\n",
        "                STerm = ' '.join(LTerm)\n",
        "                LTerm = []\n",
        "\n",
        "                try:\n",
        "                    self.DictFrq[STerm] += 1\n",
        "                except:\n",
        "                    self.DictFrq[STerm] = 1        \n",
        "\n",
        "        return\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnxocDPBMHcL"
      },
      "source": [
        "#FileIN = open('GG_l.txt', 'r')\n",
        "#FileOut = open('GG_term.txt', 'w')\n",
        "\n",
        "# FileIN = open('EP_DE_l.txt', 'r')\n",
        "# FileOut = open('EP_DE_term.txt', 'w')\n",
        "\n",
        "FileIN = open('OOOS_l.txt', 'r')\n",
        "FileOut = open('OOOS_term.txt', 'w')\n",
        "\n",
        "#FileIN = open('Const_RA_l.txt', 'r')\n",
        "#FileOut = open('Const_RA_term.txt', 'w')\n",
        "\n",
        "#FileIN = open('Const_RF_l.txt', 'r')\n",
        "#FileOut = open('Const_RF_term.txt', 'w')\n",
        "\n",
        "# Georgian Brown corpus (lemmatisiert)\n",
        "# !wget https://heibox.uni-heidelberg.de/f/d306be8b559849e79826/?dl=1\n",
        "# renaming file\n",
        "# !mv index.html?dl=1 ge_Brown_lem.txt\n",
        "\n",
        "\n",
        "\n",
        "#FileIN = open('WPhy_vert.txt', 'r')\n",
        "#FileOut = open('WPhy-terms.txt', 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY0xMY9SMSEE"
      },
      "source": [
        "# save the frequency dictionary into file, by decreasing frequencies\n",
        "# FileOutput.write( str( DictionaryFrq ) + '\\n' )\n",
        "\n",
        "OCorpus = clProcCorpus(FileIN)\n",
        "DictionaryFrq = OCorpus.DictFrq\n",
        "\n",
        "\n",
        "for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[1], reverse=True):\n",
        "    if re.search(' ', Word):\n",
        "        FileOut.write(Word + '\\t' + str(Frq) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUPLnj8E5byZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}